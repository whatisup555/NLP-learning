sememe--义原

word -> sense -> sememe

[TOC]



##1 **Representation Learning and NLP**

摘要：自然语言是一种典型的非结构化信息。传统的自然语言处理（NLP）严重依赖于特征工程，这需要仔细的设计和相当多的专业知识。表示学习旨在学习原始数据的表示，作为进一步分类或预测的有用信息。本章简要介绍了表示学习，包括其动机和基本思想，并回顾了它在机器学习和自然语言学习方面的历史和最新进展。

***Machine Learning = Representation + Objective + Optimization*.**

也就是说，为了构建一个有效的机器学习系统，我们首先将原始数据上的有用信息转换为内部表示，如特征向量。然后，通过设计适当的目标函数，我们可以利用优化算法来找到系统的最优参数设置。

**Distributed Representation**

编者注：这篇文章是基于 District Data Labs 的 NLP[研究实验室](http://www.districtdatalabs.com/research-lab)进行的研究的系列文章的一部分。

这篇文章是关于分布式表示的，这个概念不仅是理解机器学习中的数据处理的基础，也是理解大脑中信息处理和存储的基础。数据的分布式表示是许多最先进的深度学习技术的事实上的方法，特别是在自然语言处理领域，这将是本博客文章的重点。

如果您像我一样，您可能会对机器学习出版物的技术术语和内容感到不知所措；您阅读的每一篇论文或文章似乎都需要理解许多基本概念（“先决条件”）。分布式表示就是这些概念之一，这篇博文的主要目的是希望将这个先决条件从您的清单中删除，并朝着更强大的机器学习控制迈出一步。

地方主义代表

我们将从一个思想实验开始：假设我的大脑有 3 个神经元，或者 3 个信息处理单元。我的妻子告诉我，有时当她和我说话时，她感觉我的大脑几乎处于关闭状态，所以也许这个形象比我们想象的更贴切。我们还可以想象一下，我的大脑是一块完全空白的石板；换句话说，我的大脑中还没有学习/存储任何知识。我的妻子也相信我的大脑，但我们离题了。现在假设我凝视窗外，看到一辆红色小车驶过。“红色小车”这个概念对我来说是全新的，所以我的大脑决定将它的一个单位分配给这个概念的表示。这意味着，从现在开始，我可以通过从这个神经元中检索“小红车”的概念来回忆它。让我们用有点“数学”的术语来写这个，

| 概念             | 表示  |
| ---------------- | ----- |
| 小红车           | [1]   |
| 不是一辆红色小车 | [ 0 ] |

在此示例中，1 表示神经元“放电”，0 表示神经元未“放电”。作为一个侧面推论，我们可以想象一个空白的 [ ] 向量代表我的大脑一无所知。

现在，我仍然盯着窗外，看到第二辆车驶过，这次是一辆大型蓝色SUV。以类似的方式，我的大脑将这个新概念分配给第二个处理单元，我们最终得到以下更新的知识表示：

| 概念        | 表示    |
| ----------- | ------- |
| 小红车      | [ 1 0 ] |
| 大型蓝色SUV | [ 0 1 ] |

我们现在有两个处理单元（神经元），每个处理单元要么打开要么关闭（发射或不发射）。请注意，理论上可以有 [ 0 0 ]，代表“不是小型红色汽车也不是大型蓝色 SUV”的概念，但不可能有 [ 1 1 ]，因为那么小型红色红色也将是大型蓝色SUV和汽车广告会突然变得很混乱。

继续盯着车窗前行驶（这是缓慢的一天），又一辆汽车驶过，那是一辆大型红色SUV。总共三个概念，三个可用处理单元：完美。我们最终得到以下知识表示：

| 概念        | 表示      |
| ----------- | --------- |
| 小红车      | [ 1 0 0 ] |
| 大型蓝色SUV | [ 0 1 0 ] |
| 大型红色SUV | [ 0 0 1 ] |

通过这种类型的表示，一件事立即变得显而易见：3个处理单元只能让我存储3条信息。如果我一直盯着窗外，看到一辆红色大车驶过，我就没有空间接受这个新概念。我要么必须丢弃它，要么更换现有的。

这种表示的第二个不太引人注目的结果是每个处理单元只能对一个概念做出贡献。换句话说，单元 1 只为一辆红色小车“点火”，并且不参与代表任何其他东西。这种表示信息的方式称为局部表示。这是一种简单且易于理解的数据表示方式，从某种意义上说，你可以指着我大脑中的某个位置并说“那个神经元是存储这个概念的地方。” 类似地，如果使用局部方法来表示机器中的数据或状态，那么它将在识别数据（只需找到打开的一个硬件单元）和更改数据（关闭一个单元、关闭一个单元）方面提供一些优势。单位开）。

一次性编码 (OHE)

如果您在 NLP 背景下使用过机器学习，那么您有可能遇到过一种称为 One-Hot 编码的本地主义表示形式。例如：您有一个包含 n 个单词的词汇表，并且使用一个 n 位长的向量来表示每个单词，其中除了一位设置为 1 之外的所有位都为零。不同的单词会被“分配”不同的位。例如，让我们考虑一下安迪·威尔的《火星救援》中的这句话：

> “管道胶带适用于任何地方。胶带是有魔力的，应该受到崇拜。”

上面的句子由 12 个单词组成，其中 10 个是唯一的（“duct”和“tape”重复）。请注意，出于本教程的目的，我们目前忽略字母大小写。因此，我们的词汇由 10 个单词组成：“duct”、“tape”、“works”、“anywhere”、“is”、“magic”、“and”、“should”、“be”、“worshiped”。我们可以使用 10 位向量来表示这些单词中的任何一个，只需让第 1 位表示单词“duct”，第二位表示单词“tape”，依此类推。如下所示：

```python
“duct”      -->     [ 1 0 0 0 0 0 0 0 0 0 ]

“tape”      -->     [ 0 1 0 0 0 0 0 0 0 0 ]

…

“magic”     -->     [ 0 0 0 0 0 1 0 0 0 0 ]

…

“worshiped” -->     [ 0 0 0 0 0 0 0 0 0 1 ]
```



One-Hot 编码在自然语言处理中非常常见，因为它涉及一种简单的本地化表示，并具有所需的所有优点。然而，OHE 有其局限性：

- 典型的词汇表包含多达 100,000 个单词，因此这些向量通常很长。如果我们想要表示 10 个单词的序列，我们已经在查看超过 1,000,000 个单位的输入向量！（提示邪恶博士）
- 为了能够表示 n 个可能的单词，我需要 n 位。或者反过来，一个长度为n的向量最多可以用来表示n个单词。
- 最后，对于机器学习来说至关重要的是，所有单词彼此之间都同样相似（不同）。理想情况下，在谈论语义时，我们希望用彼此有些接近的向量来表示单词“dog”和“fox”，至少与表示“lazy”这样的单词的向量不同。对于 One-Hot 编码，我们实际上无法做到这一点。

下面是 Python 中执行 One-Hot 编码的示例函数。该函数的输入是词汇表和我们想要进行 One-Hot 编码的单词。该函数返回一个 OHE 向量。

```python
def one_hot_encode_my_word(my_word, the_vocabulary):
    one_hot_encoded_vector = []
    for word in the_vocabulary:
        if word == my_word:
            one_hot_encoded_vector.append(1)
        else:
            one_hot_encoded_vector.append(0)

v = ["duct", "tape", "works", "anywhere", "is", "magic", "and", "should", "be", "worshiped"]

one_hot_encode_my_word("tape", v)
```



```python
>>> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
```

## 分布式表示：

让我们回到最初的小型红色汽车和大型蓝色 SUV 的思想实验。让我们再添加一些更多的概念——这里的假设是，我睡着了，我的大脑长出了更多的神经元，然后我醒来并观看了一些 YouTube 视频，所以我的知识图现在如下所示：

| 概念        | 表示                |
| ----------- | ------------------- |
| 小红车      | [ 1 0 0 0 0 0 0 0 ] |
| 大型蓝色SUV | [ 0 1 0 0 0 0 0 0 ] |
| 大型红色SUV | [ 0 0 1 0 0 0 0 0 ] |
| 青苹果      | [ 0 0 0 1 0 0 0 0 ] |
| 大黄蜂      | [ 0 0 0 0 1 0 0 0 ] |
| 高层建筑    | [ 0 0 0 0 0 1 0 0 ] |
| 小鱼        | [ 0 0 0 0 0 0 1 0 ] |
| 香蕉        | [ 0 0 0 0 0 0 0 1 ] |

在这里，我们面临着我们在 One-Hot Encoding 讨论中提到的相同问题：高计算成本、高维度，并且概念同样相似。例如，要从任何概念转到另一个概念，您需要走相同的距离：一维为 -1，第二维为 +1。就这种表述而言，一辆大型蓝色 SUV 与一辆大型红色 SUV 的相似度就如同香蕉一样。不完全细致入微。但我们可以做得更好。

自 20 世纪 80 年代中期以来（参见 Rumelhart、McClelland 和 Hinton - 1986），联结主义（读作“反地方主义”）群体一直提倡所谓的“分布式表示”，它比地方主义方法具有一些优势，并帮助我们克服其局限性。“分布式表示”这个名称主要是因为任何单个概念的表示都分布在许多（如果不是全部）处理单元上。在许多情况下，向量中的单位值是连续值，而不仅仅是 1 和 0。

让我们将这种表示形式应用到我的大脑知识上，看看是否能让我变得更聪明（我的妻子已经绝望了，所以现在全靠你了，没有压力）：

| 概念        | 表示                        |
| ----------- | --------------------------- |
| 小红车      | [ 0.555 0.761 0.243 0.812 ] |
| 大型蓝色SUV | [ 0.773 0.309 0.289 0.835 ] |
| 大型红色SUV | [ 0.766 0.780 0.294 0.834 ] |
| 青苹果      | [ 0.153 0.022 0.654 0.513 ] |
| 大黄蜂      | [ 0.045 0.219 0.488 0.647 ] |
| 高层建筑    | [ 0.955 0.085 0.900 0.773 ] |
| 小鱼        | [ 0.118 0.192 0.432 0.618 ] |
| 香蕉        | [ 0.184 0.232 0.671 0.589 ] |

我们有什么在这里？

- 连续值而不是离散的 1 和 0。
- 每个处理单元都有助于任何和所有概念。
- 这些表示是密集的（相对于稀疏的局部表示）。
- 概念不再局限于一个单元（因此称为“分布式”）。
- 我们能够仅用 4 个处理单元来表示大量概念（而不是被 n 个单元限制为 n 个概念）。
- 我们可以在不添加新单元的情况下学习新概念。我们所需要的只是一个新的价值观配置。
- 最重要的是，我们能够更好地表示相似性：大型红色 SUV [ 0.773 0.309 0.289 0.835 ] 和大型蓝色 SUV [ 0.766 0.780 0.294 0.834 ] 彼此之间的相似度比它们与小鱼 [ 0.118 0.192 0.432 0.618 ] 之间的相似度要高得多。

上面的最后一点至关重要，对于学习我们以前从未见过的新概念具有奇妙的意义。如果我看到一些我不认识的新对象，比如说一条河鳟鱼，它具有以下表示形式 [ 0.144 0.187 0.439 0.606 ]，我可以快速找到它与哪个现有概念的向量最相似（小鱼向量） ）并且我可以猜测它一定是一个像鱼一样的物体，没有任何关于它的附加信息！

此外——这就是为什么这种类型的表示如此强大——我们能够*概括*。我不需要向量中的所有单位都具有值，我可以仅通过部分表示来“猜测”它们可能属于什么概念类别。例如，[ 0.158 0.030 -- -- ] 与青苹果具有相似的特征。它可能是一个绿色的梨子，或者一个黄色的柠檬，但它不太可能是一座高层建筑。

词嵌入

我们将 One-Hot 编码视为一种本地主义表示类型，现在让我们看看词嵌入，它是 NLP 深度学习中使用的分布式表示的一个示例。

词嵌入的想法是获取文本语料库，并找出单词的分布式向量表示，这些单词之间保留一定程度的语义相似性。当这种情况发生时，“duct”和“tape”这两个词彼此“更接近”，而不是“magic”（我们无法使用 One-Hot Encoding 做到这一点）。这可以应用于任何语料库，无论是小说集，还是某个科学学科内的文档，或者许多其他应用程序。从这里开始，这些词嵌入可以用作其他模型（例如 SVM 或递归神经网络）的输入。该输入通常采用向量矩阵的形式，就像我们在上一节中看到的那样。

在本系列的以下文章中，我们将探索词嵌入模型的一种流行实现：word2vec。基于最近的一篇论文（参见 Mikolov '13），word2vec 将单热编码向量转换为嵌入向量，并在此过程中取得了出色的结果。它设法生成向量，使我们能够执行诸如 W(“King”) - W(“Man”) + W(“Woman”) = W(“Queen”) 之类的事情！它是如何做到的？请务必关注我们的下一篇文章！

### 1.5 Learning Approaches to Representation Learning for nlp

人们已经开发了各种有效有效的方法来学习NLP的语义表示。这里我们列出了一些典型的方法。

统计特征：如前所述，NLP在早期阶段的语义表示往往来自于统计数据，而不是来自于优化过程。例如，在n-gram或词袋模型中，表示中的元素通常是在大规模语料库中计算的相应条目出现的频率或次数。

手工特征：在某些自然语言处理任务中，句法和语义特征对解决问题是有用的。例如，单词和实体的类型、语义角色和解析树等。这些语言特征可以与任务一起提供，也可以通过特定的自然语言处理系统进行提取。在广泛使用分布式表示之前的很长一段时间里，研究人员投入大量精力来设计有用的特性，并将它们组合作为自然语言处理模型的输入。

监督学习：分布式表示产生于监督学习下的神经网络的优化过程。在神经网络的隐藏层中，神经元的不同激活模式代表着不同的实体或属性。有了训练目标（通常是目标任务的损失函数）和监督信号（通常是目标任务训练实例的金标准标签），网络可以通过优化（如梯度下降）来学习更好的参数。经过适当的训练，隐藏状态将成为信息丰富和广义的，作为自然语言的良好语义表示。

自我监督学习：在某些情况下，我们只是想为某些元素得到良好的表示，这样这些表示就可以转移到其他任务中。例如，在大多数神经NLP模型中，句子中的单词首先被映射到它们相应的单词嵌入中（可能来自word2vec或GloVe），然后再发送到网络中。然而，并没有由人类注释的“标签”来学习单词嵌入。为了获得神经网络所需的训练目标，我们需要从现有数据中从本质上生成“标签”。这被称为自监督学习（一种无监督学习的一种方式）。

##2 Word Representation

### 2.2 独热编码

本质上，独热表示将每个单词映射到词汇表的索引，这对于存储和计算非常有效。然而，它并不包含丰富的单词的语义和句法信息。因此，一热表示不能捕捉到单词之间的相关性。猫和狗的区别就像猫和床的一个热门词表示的区别一样。此外，一个热门的单词表示将每个单词嵌入到一个|V|维向量中，这只能适用于一个固定的词汇量。因此，在现实世界中处理新词是不灵活的。

### 2.3 Distributed Word Representation

#### 2.3.1 Brown Cluster

布朗聚类将单词分为几个具有相似语义意义的聚类。其中，Brown聚类从一个大规模的语料库中学习一个二叉树，其中树的叶子表示单词，树的内部节点表示单词的层次聚类。这是一种硬聚类方法，因为每个单词只属于一个组。

从一个大的语料库中估计P（wi|−1−k）很容易，但是模型具有|V|^k−1独立参数，这在20世纪90年代对计算机来说是一个巨大的数字。即使k是2，参数的数量也是相当大的。此外，对罕见的单词的估计是不准确的。为了解决这些问题，[9]提出将单词分组为集群，并训练集群级的n-gram语言模型，而不是单词级的模型。通过为每个单词分配一个簇，这个概率可以被写成

![image-20230801155601692](E:/typora/picture/刘志远文档/image-20230801155601692.png)

#### 2.3.2 Latent Semantic Analysis(LSA)

![image-20230801160037608](E:/typora/picture/刘志远文档/image-20230801160037608.png)

在这里，Σ中包含的奇异值k的数量是一个需要进行调整的超参数。通过使用合理数量的最大奇异值，LSA可以在单词-文档矩阵中捕获许多有用的信息，并提供平滑效果，防止较大的方差。

#### 2.3.3 word2vec2

谷歌的word2vec2工具包于2013年发布。它可以有效地从一个大型语料库中学习单词向量。该工具包有两种模型，包括连续的单词包（CBOW）和跳过克包。基于一个单词的含义可以从其上下文中学习的假设，CBOW优化了嵌入，以便它们能够预测给定其上下文单词的目标单词。相反，Skip-gram学习了可以预测给定目标单词的上下文单词的嵌入。在本节中，我们将详细介绍这两个模型。

##### 2.3.3.1 Continuous Bag-of-Words

![image-20230801161314013](E:/typora/picture/刘志远文档/image-20230801161314013.png)

![image-20230801161324063](E:/typora/picture/刘志远文档/image-20230801161324063.png)

##### 2.3.3.2  Skip-Gram

![image-20230801161604335](E:/typora/picture/刘志远文档/image-20230801161604335.png)

#####2.3.3.3 Hierarchical Softmax and Negative Sampling

层次化的softmax根据单词的频率生成层次化的类，即霍夫曼树。通过近似，它可以更快地计算每个单词的概率，而计算每个单词的概率的复杂性是负抽样更简单。为了计算一个单词的概率，O（log |V|）。

![image-20230801175909063](E:/typora/picture/刘志远文档/image-20230801175909063.png)

#### 2.3.4 GloVe

诸如Skip-gram和CBOW等方法都是基于浅窗口的方法。这些方法扫描了整个语料库的上下文窗口，但没有利用一些全局信息。相反，Word表示的全局向量（GloVe）可以直接捕获语料库统计量。

![image-20230801180227482](E:/typora/picture/刘志远文档/image-20230801180227482.png)

### 2.4  Contextualized Word Representation(双向lstm)

![image-20230801181743380](E:/typora/picture/刘志远文档/image-20230801181743380.png)

### 2.6 Evaluation

近年来，人们提出了各种将单词嵌入向量空间的方法。因此，评估不同的方法是很有必要的。对单词嵌入有两种一般的评价，包括单词相似性和单词类比。他们的目标都是检查分发这个词是否合理。这两种评价有时会产生不同的结果。例如，CBOW在单词相似度方面表现更好，而Skip-gram在单词类比方面表现优于CBOW。因此，选择哪种方法取决于高级应用程序。特定于任务的单词嵌入方法通常是为特定的高级任务而设计的.
#### 2.6.1 Word Similarity/Relatedness

单词的动态是非常复杂和微妙的。没有一个静态的、有限的关系集可以描述两个词之间的所有相互作用。对于下游任务来说，利用不同类型的单词关系也不是一件小事。一种更实际的方法是给一对单词分配一个分数，以表示它们之间的关联程度。这种测量方法被称为单词的相似度。当谈论这个术语单词的相似性时，确切的含义在不同的情况下可能会有很大的不同。在各种不同的文献中，可以提到几种相似之处。

#### 2.6.2 Word Analogy

除了单词相似性外，单词类比任务也是衡量表征捕获单词语义意义的另一种方法。这个任务给出三个单词w1、w2和w3，然后要求系统预测一个单词w4，使w1和w2之间的关系与w3和w4之间的关系相同。从[43,45]开始，这个任务就被用来利用单词之间的结构关系。在这里，词的关系可以分为语义关系和句法关系两类。这是一种相对新颖的单词表示评估方法，但由于数据集发布，很快成为一个标准的评估度量。与托福同义词测试不同的是，这个数据集中的大多数单词在各种语料库中都很常见，但是第四个单词是从整个词汇表中选择的，而不是四个选项。这个测试倾向于分布式单词表示，因为它强调了单词空间的结构。在[7,56,57,61]中可以找到不同模型之间的比较。

## 3 Compositional Semantics

NLP领域中的许多重要应用都依赖于理解更复杂的语言单元，如短语、句子和单词之外的文档。因此，组合语义仍然是自然语言处理的核心任务。在本章中，我们首先介绍了二进制语义组合的各种模型，包括加性模型和乘法模型。在此之后，我们提出了各种典型的n元语义组成模型，包括递归神经网络、递归神经网络和卷积神经网络。

![image-20230802162100128](E:/typora/picture/刘志远文档/image-20230802162100128.png)

语义信息由单词数量、种类、顺序、常识知识等构成。

### 3.2   Semantic Space

#### 3.2.1 Vector Space

一般来说，语义表示的中心任务是将单词从抽象语义空间投射到数学低维空间。如前几章所述，为了使转换合理，目的是在这个新的投影空间中保持单词的相似性。换句话说，这些单词越相似，它们的向量就应该越近。例如，我们希望单词向量w（book）和w（杂志）很接近，而单词向量w（apple）和w（计算机）距离很远。在本章中，我们将介绍几个广泛使用的典型语义向量空间，包括单热表示、分布式表示和分布表示。

#### 3.2.2  Matrix-Vector Space

尽管语义向量空间被广泛使用，但本文提出的另一种语义空间是一种更强大、更通用的组合语义框架。与传统的向量空间不同，矩阵向量语义空间利用矩阵来表示词的意义，而不是瘦向量。这背后的动机是，当在一个特定的上下文下建模语义意义时，人们不仅想知道每个单词的意思是什么，而且还想知道整个句子的整体意义。因此，我们关注每个句子中相邻单词之间的语义转换。然而，语义向量空间不能明确地描述一个词对另一个词的语义转换。在语义转换建模思想的驱动下，一些研究者提出使用矩阵来表示一个词对另一个词的转换操作。与那些向量空间模型不同，它可以包含一些结构信息，如词序和语法组成。

### 3.3 Binary Composition

其目标是为短语、句子、段落和文档构造向量表示。在不丧失一般性的情况下，我们假设一个短语（句子、段落或文档）的每个组成部分都嵌入到一个向量中，该向量随后将以某种方式组合，为短语（句子、段落或文档）生成一个表示向量。

近年来，二进制组合函数的建模是一个被充分研究但仍具有挑战性的问题。对于这个问题主要有两个观点，包括加性模型和乘法模型。

#### 3.3.1 Additive Model

加性模型有一个约束条件，它假设p、u和v位于同一语义空间中。这本质上意味着所有的语法类型都具有相同的维度。最简单的方法之一是直接使用和来表示联合表示：p=u+v。变种是p=au+bv,其中a,b为参数

#### 3.3.2 Multiplicative Model

其中，pi = ui·vi，表示输出的每个维数只依赖于两个输入向量的相应维数。然而，与最简单的加性模型相似，该模型也缺乏对词序建模的能力，以及缺乏背景句法或知识信息。

与将简单的乘法模型扩展到复杂的乘法模型不同，本文还提出了其他一种减少参数空间的方法。随着参数大小的减小，人们可以大大提高在基于张量的模型中的合成效率，而不是有一个O（n3）的时间复杂度。因此，可以在原张量模型中应用一些压缩技术。一个典型的例子是圆形卷积模型，它可以显示为

![image-20230805091758418](E:/typora/picture/刘志远文档/image-20230805091758418.png)

### 3.4 N-Ary Composition

在现实世界的NLP任务中，输入通常是多个单词的序列，而不仅仅是一对单词。因此，除了设计一个合适的二进制组合算子外，应用二进制运算的顺序也很重要。在本节中，我们将以语言建模为例，介绍n组合中的三种主流策略。

#### 3.4.1 Recurrent Neural Network

为了设计应用二进制组合函数的顺序，最直观的方法是利用序列性。即，序列顺序应该是sn=（sn−1，wn），其中sn−1是第一个n个−1个单词的顺序。基于这一想法，所使用的神经网络模型是递归神经网络（RNN）。

RNN依次应用组合函数，并推导出隐藏语义单元的表示。基于这些隐藏的语义单元，我们可以将它们用于一些特定的NLP任务，如情绪分析或文本分类。另外，请注意，基本的RNN只利用了一个句子/文档的从头到尾的顺序信息。为了提高其表示能力，可以通过考虑序列信息和反向序列信息，将RNN增强为双向RNN。

![image-20230805092650801](E:/typora/picture/刘志远文档/image-20230805092650801.png)

从RNN的定义来看，组合函数可以表述如下：

$\mathbf{h}_t=\tanh(\mathbf{W}_1\mathbf{h}_{t-1}+\mathbf{W}_2\mathbf{w}_t),$其中w1和w2是两个加权矩阵。

我们可以看到，这里我们使用一个矩阵加权求和来表示二进制语义组合：

$\mathbf{p}=\mathbf{W}_\alpha\mathbf{u}+\mathbf{W}_\beta\mathbf{v}.$

LSTM：由于原始的RNN只利用了简单的正切函数，因此很难获得一个长句子/文档的长期依赖关系。参考[5]重新创建了长短期记忆（LSTM）网络，以加强其能力建模了RNN中的长期语义依赖关系。详细地说，LSTM的组合功能允许来自以前层的信息直接流到它们下面的层。组合函数可以定义为

$\begin{aligned}
&\text{f} _{t}=\mathrm{Sigmoid}(\mathbf{W}_{f}^{h}\mathbf{h}_{t-1}+\mathbf{W}_{f}^{x}\mathbf{x}_{t}+\mathbf{b}_{f}),  \\
&\mathbf{i}_{t}=\mathrm{Sigmoid}(\mathbf{W}_{i}^{h}\mathbf{h}_{t-1}+\mathbf{W}_{i}^{x}\mathbf{x}_{t}+\mathbf{b}_{i}), \\
&\mathbf{0}_{t} =\mathrm{Sigmoid}(\mathbf{W}_o^h\mathbf{h}_{t-1}+\mathbf{W}_o^x\mathbf{x}_t+\mathbf{b}_o),  \\
&\hat{\mathbf{c}}_t =\tanh(\mathbf{W}_c^h\mathbf{h}_{t-1}+\mathbf{W}_c^x\mathbf{x}_t+\mathbf{b}_c),  \\
&c_t =\mathbf{f}_t\odot\mathbf{c}_{t-1}+\mathbf{i}_t\odot\mathbf{\hat{c}}_t,  \\
&\mathbf{h}_{t} =\mathbf{o}_t\odot\mathbf{c}_t. 
\end{aligned}$

LSTM的变体。为了简化LSTM并获得更有效的算法，[3]提出使用一个简单但可比较的RNN架构，名为门控循环单元（GRU）。与LSTM相比，GRU的参数更少，因此效率更高。组成函数显示为

$\begin{aligned}
&\mathbf{z}_{t} =\mathrm{Sigmoid}(\mathbf{W}_z^h\mathbf{h}_{t-1}+\mathbf{W}_z^x\mathbf{x}_t+\mathbf{b}_z),  \\
&r_{t} =\mathrm{Sigmoid}(\mathbf{W}_r^h\mathbf{h}_{t-1}+\mathbf{W}_r^x\mathbf{x}_t+\mathbf{b}_r),  \\
&\hat{\mathbf{h}}_{t} =\tanh(\mathbf{W}_{h}(\mathbf{r}_{t}\odot\mathbf{h}_{t-1})+\mathbf{W}_{h}^{x}\mathbf{x}_{t}+\mathbf{b}_{h}),  \\
&\mathbf{h}_{t} =(1-\mathbf{z}_t)\odot\mathbf{h}_{t-1}+\mathbf{z}_t\odot\hat{\mathbf{h}}_t. 
\end{aligned}$

#### 3.4.2 Recursive Neural Network

除了递归神经网络外，另一种应用二元组合函数的策略是遵循解析树，而不是顺序词序。基于这一理念，[15]提出了一种递归神经网络来建模不同层次的语义单元。在本小节中，我们将介绍一些具有不同二进制组合函数的递归解析树之后的算法。

由于所有的递归神经网络都是二叉树，我们需要考虑的基本问题是如何得到给定其两个子语义组件的树上的父分量的表示。参考[15]提出了一种递归矩阵向量模型（MV-RNN），该模型通过为每个组成部分分配一个矩阵向量表示来捕获组成部分的解析树结构信息。向量捕获成分本身的含义，矩阵表示它如何修改与之结合的单词的含义。假设我们有两个子分量a、b和它们的父分量p，其组成可以表述如下：

$\begin{aligned}\mathbf{p}&=f_{\nu ec}(a,b)=g\left(\mathbf{W}_1\begin{bmatrix}\mathbf{B}\mathbf{a}\\\mathbf{A}\mathbf{b}\end{bmatrix}\right),\\\mathbf{P}&=f_{matrix}(a,b)=\mathbf{W}_2\begin{bmatrix}\mathbf{A}\\\mathbf{B}\end{bmatrix},\end{aligned}$

a，b，p是每个组件的嵌入向量和a，b，p矩阵，W1是一个矩阵映射转换词到另一个语义空间，元素函数g是一个激活函数，和W2是一个矩阵，两个矩阵映射到一个组合矩阵P与相同的维度。整个过程如图3.2所示。然后MV-RNN在两个目标实体之间选择解析树中路径的最高节点来表示输入的句子。

![image-20230805094042681](E:/typora/picture/刘志远文档/image-20230805094042681.png)

#### 3.4. 3 Convolutional Neural Network

参考[6]提出使用卷积神经网络（CNN）嵌入输入句子，该网络通过卷积层提取局部特征，并通过最大池化操作结合所有局部特征，获得输入句子的固定大小的向量。

形式上，卷积运算定义为一组向量序列、一个卷积矩阵W和一个带有滑动窗口的偏置向量b之间的矩阵乘法。让我们将向量qi定义为第i个窗口中输入表示的子序列的连接

$\mathbf{h}_j=\max_i[f(\mathbf{Wq}_i+\mathbf{b})]_j,$

其中f表示一个非线性函数，如s型函数或切线函数，h表示句子的最终表示。

###3.5 Summary

在本章中，我们首先介绍了组合语义的语义空间。然后，我们以短语表示为例，介绍了二元语义组合的各种模型，包括加法模型和乘法模型。最后，我们介绍了典型的n元语义组成模型，包括递归神经网络、递归神经网络和卷积神经网络。组合语义允许语言从更简单的元素的组合及其二进制语义组合中构建复杂的含义而神经语义组合是多个NLP任务的基础，包括句子表示、文档表示、关系路径表示等。我们将在下面的章节中详细介绍这些场景。

## 4 Sentence Representation

句子是自然语言中的一个重要的语言单位。句子表示一直是自然语言处理的核心任务，因为在相关领域的许多重要应用在于理解句子，如摘要、机器翻译、情感分析和对话系统。句子表示的目的是将语义信息编码为实值表示向量，并将其用于进一步的句子分类或匹配任务。随着互联网上大规模的文本数据和深度神经网络的最新进展，研究人员倾向于使用神经网络（如卷积神经网络和递归神经网络）来学习低维句子表示，并在相关任务上取得很大的进展。

在本章中，我们首先介绍了句子的单热表示和n-gram句子表示（即概率语言模型）。然后，我们广泛地介绍了基于神经的模型来进行句子建模，包括前馈神经网络、卷积神经网络、递归神经网络、最新的变换模型和预先训练过的语言模型。最后，我们介绍了句子表示的几个典型应用。

### 4.1  Introduction

自然语言句子由单词或短语组成，遵循语法规则，传达完整的语义信息。与单词和短语相比，句子具有更复杂的结构，包括顺序结构和层次结构，这是理解句子的关键。在自然语言处理中，如何表示句子对于句子分类、情感分析、句子匹配等相关应用至关重要。

在深度学习开始之前，句子通常用单热向量或TF-IDF向量表示，遵循单词袋的假设。在这种情况下，一个句子被表示为一个词汇量大小的向量，其中每个元素表示一个特定的单词（术语频率或TF-IDF）对该句子的重要性。然而，这种方法面临着两个问题。首先，这种表示向量的维数通常高达数千个或数百万个。因此，它们通常面临稀疏性问题，并带来计算效率问题。其次，这样一个表示方法遵循词袋假设，忽略了顺序信息和结构信息，这对理解句子的语义意义至关重要。

受计算机视觉和语音中深度学习模型的最新进展的启发，研究人员提出用深度神经网络来建模句子，如卷积神经网络、递归神经网络等。与传统的基于词频的句子表示相比，深度神经网络可以通过卷积或循环操作来捕获句子的内部结构，如顺序信息和依赖信息。因此，基于神经网络的句子表示在句子建模和自然语言处理任务方面取得了巨大的成功。

### 4.2 One-Hot Sentence Representation

![image-20230805095739691](E:/typora/picture/刘志远文档/image-20230805095739691.png)

此外，研究人员通常会考虑到不同的单词的重要性，而不是平等地对待所有的单词。例如，诸如“a”、“an”和“the”等函数词通常出现在不同的句子中，并且很少保留什么含义。因此，使用逆文件频率（IDF）来衡量wi在V中的重要性如下：

$\mathrm{idf}_{w_i}=\log\frac{|D|}{\mathrm{df}_{w_i}},$

其中，|D|为语料库D中所有文档的数量，dfwi表示wi的文档频率（DF）。

### 4.3 Probabilistic Language Model

单热句表示通常忽略了句子中的结构信息。为了解决这个问题，研究人员提出了概率语言模型，该模型将n-gram而不是单词作为基本组成部分。n-gram表示长度为n的上下文窗口中单词的一个子序列，概率语言模型将句子s = [w1，w2，...，wl]的概率定义为

$P(s)=\prod\limits_{i=1}^lP(w_i|w_1^{i-1}).$

实际上，在等式中表示的模型（4.4）由于其巨大的参数空间，因此不可行。在实践中，我们简化了模型并设置了一个n大小的上下文窗口，假设单词wi的概率只依赖于[wi−n+1···wi−1]。更具体地说，一个n-gram语言模型基于它之前的n−1个单词来预测句子s中的单词wi。因此，一个句子的简化概率被形式化为:

$P(s)=\prod\limits_{i=1}^lP(w_i|w_{i-n+1}^{i-1}),$

其中，选择单词wi可以计算出n-gram模型频率计数的概率：

$P(w_i|w_{i-n+1}^{i-1})=\frac{P(w_{i-n+1}^i)}{P(w_{i-n+1}^{i-1})}.$

通常，n-gram语言模型中的条件概率不是直接从频率计数中计算出来的，因为当它遇到任何以前没有明确见过的n-克语言模型时，它会遇到严重的问题。因此，研究人员提出了几种类型的平滑方法，它们将一些总概率质量分配给看不见的单词或n个克，如“加一”平滑、好图灵贴现或后退模型。

n-gram模型是预测n-gram序列中下一个单词的典型概率语言模型，它遵循马尔可夫假设，即目标单词的概率只依赖于之前的n−1个单词。目前大多数的句子建模方法都采用了这一思想。我们使用n-gram语言模型作为真实的底层语言模型的近似值。这个假设是至关重要的，因为它大大简化了从数据中学习语言模型参数的问题。近年来关于单词表示学习[3,40,43]的研究主要是基于n-gram语言模型。

### 4.4 Neural Language Model

为了解决这个问题，研究人员提出神经语言模型使用连续表示或嵌入的单词和神经网络预测，嵌入在连续空间帮助缓解维度的诅咒语言建模，和神经网络避免这个问题表示单词以分布式的方式，非线性的权重组合神经网络[2]。另一种描述是，一个神经网络近似于语言函数。神经网络结构可能是前馈的或循环的，前者更简单，后者更常见。

与概率语言模型类似，神经语言模型被构建和训练为概率分类器，以学习预测概率分布：

$P(s)=\prod\limits_{i=1}^lP(w_i|\mathbf{w}_1^{i-1}),$

其中，选择词wi的条件概率可以通过前馈神经网络等各种神经网络、递归神经网络等来计算。在下面的章节中，我们将详细介绍这些神经语言模型。

#### 4.4.1 Feedforward Neural Network Language Model

神经网络语言模型的目标是估计条件概率P（wi|w1，...，wi−1）。然而，前馈神经网络（FNN）缺乏一种代表长期历史背景的有效方法。因此，它采用n-gram语言模型的想法近似条件概率，假设每个单词序列统计更依赖于这些单词接近它，和只有n−1上下文单词被用来计算条件概率，即P（||−1 1）≈P（|−1−+1）。FNN语言模型的总体架构由[3]提出。为了计算单词wi的条件概率，它首先将其n个与上下文相关的单词投射到它们的单词向量表示x=[wi−n+1，...，wi−1]，然后将它们输入FNN，通常可以表示为

$\mathbf{y}=\mathbf{M}f(\mathbf{W}\mathbf{x}+\mathbf{b})+\mathbf{d}$

其中W是将词向量转换为隐藏表示的加权矩阵，M是隐藏层和输出层之间连接的加权矩阵，b，d是偏置向量。

####4.4.2 Convolutional Neural Network Language Model

卷积神经网络（CNN）是一个神经网络模型的家族，其中有一种层被称为卷积层。这一层可以通过一个可学习的过滤器（或内核）在输入的不同位置提取特征。Pham等人[47]提出了CNN语言模型来增强FNN语言模型。所提出的CNN网络是通过在单词输入表示x=[wi−n，...，wi−1]之后注入一个卷积层而产生的。在形式上，卷积层包含一个以每个字向量为中心的输入向量的滑动窗口，通常可以表示为

$\mathbf{y}=\mathbf{M}(\max(\mathbf{W_cx}))$

其中，max（·）表示最大池化层。CNN的体系结构如图4.1所示。此外，[12]还引入了一种卷积神经网络用于具有新的门机制的语言建模。

![image-20230805104444091](E:/typora/picture/刘志远文档/image-20230805104444091.png)

#### 4.4.3 Recurrent Neural Network Language Model

为了解决FNN语言模型中缺乏建模长期依赖性的问题，[41]提出了一种应用RNN的递归神经网络（RNN）语言模型。rnn与fnn的本质不同在于，它们不仅作用于输入空间，还作用于内部状态空间，而内部状态空间能够表示顺序扩展的依赖关系。因此，RNN语言模型可以处理这些任意长度的句子。在每个时间步长中，它的输入是前一个单词的向量，而不是前n个单词的向量的连接，并且其他所有之前单词的信息可以通过它的内部状态来考虑。在形式上，RNN语言模型可以定义为

$\begin{aligned}\mathbf{h}_i&=f(\mathbf{W}_1\mathbf{h}_{i-1}+\mathbf{W}_2\mathbf{w}_i+\mathbf{b}),\\\mathbf{y}&=\mathbf{M}\mathbf{h}_{i-1}+\mathbf{d},\end{aligned}$

其中w1w2M是加权矩阵bd是偏置向量。在这里，RNN单元也可以通过LSTM或GRU来实现。RNN的体系结构如图4.2所示。

![image-20230805104655095](E:/typora/picture/刘志远文档/image-20230805104655095.png)

#### 4.4.4  Transformer Language Model

2018年，谷歌提出了一种名为BERT的预训练语言模型（PLM），该模型在各种NLP任务上取得了最先进的成果。当时，这是个大新闻。从那时起，所有的NLP研究人员都开始考虑plm如何使他们的研究任务受益。在本节中，我们将首先介绍transformer的体系结构，然后详细讨论BERT和其他plm。

##### 4.4.4.1 Transformer

Transformer是一种具有一系列基于注意力的块的非循环编码-解码器架构。该编码器共6层，每层由一个多头注意子层和一个位置级前馈子层组成。并且在子层之间有一个残留的连接。该变压器的体系结构如图4.3所示。

$\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V},$

其中，dk为查询矩阵的维数。

多头注意子层将输入的隐藏状态H多次线性投影到h头的查询矩阵、键矩阵和值矩阵中。查询、键和值向量的维度分别为dk、dk和dv。

![image-20230805105309873](E:/typora/picture/刘志远文档/image-20230805105309873.png)

多头注意子层可以表述为

$\text{Multihead}(H)=[head_1,head_2,\ldots,head_h]\mathbf{W}^O,$

![image-20230805110015974](E:/typora/picture/刘志远文档/image-20230805110015974.png)

$\mathrm{FFN}(x)=\mathbf{W}_2\max(0,\mathbf{W}_1x+\mathbf{b}_1)+\mathbf{b}_2.$

变压器在长期依赖性建模方面优于rnn，在注意操作过程中，所有的令牌都将被同等考虑。为了解决机器翻译的问题，提出了该变压器。由于变压器具有非常强大的建模顺序数据的能力，因此它成为NLP应用程序中最流行的骨干。

##### 4.4.4.2 Transformer-Based PLM

神经模型可以从语言建模中学习到大量的语言知识。由于语言知识涵盖了许多下游NLP任务的需求，并提供了强大的单词和句子的表征，一些研究人员发现，知识可以很容易地转移到其他NLP任务中。这些被转移的模型被称为预训练的语言模型（PLMs）。

语言建模是最基本和最重要的NLP任务。它包含了各种语言理解的知识，如语言知识和事实知识。例如，该模型需要决定它是否应该在一个名词之前添加一篇文章。这需要对文章的语言学知识。另一个例子是，在“特朗普是总统”之后的以下词是什么。答案是“美国”，这需要事实知识。由于语言建模非常复杂，模型可以从这个任务中学到很多东西。

另一方面，语言建模只需要纯文本，而不需要任何人工注释。有了这个特性，模型可以从一个非常大规模的语料库中学习复杂的自然语言处理能力。由于深度学习需要大量的数据，而语言建模可以充分利用世界上所有的文本，这大大有利于NLP研究的发展。

受到变形器成功的启发，GPT [50]和BERT [14]开始采用变形器作为预先训练好的语言模型的主干。GPT和BERT是最具代表性的基于变压器的预训练语言模型（PLMs）。由于它们在各种NLP任务上取得了最先进的性能，因此几乎所有排在它们之后的plm都是基于变压器的。在本小节中，我们将更详细地讨论GPT和BERT。

GPT是第一个对基于变压器的PLM进行预训练的工作。GPT [50]的训练过程包括两个经典阶段：生成式预训练和鉴别性微调。

在训练前阶段，模型的输入是一个大规模的未标记语料库，表示为U = {u1，u2，...，un}。训练前阶段旨在优化一个语言模型。在语料库上的学习目标是在一个固定大小的窗口中最大化条件似然：

$\mathscr{L}_1(\mathscr{U})=\sum_i\log P(u_i|u_{i-k},\ldots,u_{i-1};\Theta),$

其中k表示窗口的大小，条件似然P由一个参数为Θ的神经网络建模。

对于一个有监督的数据集χ，输入是一个单词序列s =（w1，w2，..，wl），输出是一个标签y。预训练阶段提供了一个有利的参数起点，可用于初始化后续的监督任务。在这种情况下，目标是一个鉴别任务，最大化条件可能性分布：

$\mathscr{L}_2(\chi)=\sum_{(s,y)}\log P(y|w_1,\ldots,w_l),$

其中P（y|w1，...，wl）由k层变压器建模。在输入的令牌经过预先训练好的GPT后，将产生最后一层hl K的隐藏向量。为了获得输出分布，我们添加了一个线性变换层，其大小与标签的数量相同：

$P(y|w_1,\ldots,w_m)=\text{Softmax}(\mathbf{W}_y\mathbf{h}_l^K).$

最后的训练目标与语言建模L1相结合，以更好地泛化：

$\mathscr{L}(\chi)=\mathscr{L}_2(\chi)+\lambda*\mathscr{L}_1(\chi),$

BERT [14]是PLM领域的一个里程碑式的工作。BERT在17种不同的NLP任务上取得了显著的实证结果，包括SQuAD（优于人类）、GLUE（7.7%绝对改进）、MultiNLI（4.6%绝对改进）等。与GPT相比，BERT采用双向深度变压器作为模型骨干。如图4.4所示，BERT包含了预训练和微调阶段。

在训练前阶段，设计了两个目标：蒙面语言模型（MLM）和下一个句子预测（NSP）。(1)对于MLM，标记是随机的用一个特殊的标记掩盖。训练的目标是根据上下文来预测掩蔽令牌。与只能在一个方向上训练的标准单向条件语言模型相比，MLM的目标是训练一个深度双向表示模型。这项任务的灵感来自于Cloze [64]。(2)NSP的目标是捕捉一些基于句子的下游任务中的句子之间的关系，如自然语言推理（NLI）和问题回答（QA）。在这个任务中，训练一个二元分类器来预测该句子是否是当前的下一个句子。该任务有效地捕捉了句子之间的深层关系，从不同的层次上探索语义信息。

![image-20230805142751150](E:/typora/picture/刘志远文档/image-20230805142751150.png)

经过预训练后，BERT可以捕获各种语言知识。通过修改输入和输出，BERT可以对任何NLP任务进行微调，这些任务包含具有单个文本或文本对输入的应用程序。输入由句子A和句子B组成，分别表示(1)释义中的句子对，(2)隐含中的假设-前提对，(3)QA中的问题-段落对，(4)文本分类任务或序列标记中的text-∅。对于输出，BERT可以为每个令牌生成标记级表示，用于对标记任务或问题回答进行排序。此外，将BERT中的特殊标记[CLS]输入分类层进行序列分类。

预先训练好的语言模型在BERT后得到了快速的发展。我们总结了plm的几个重要方向，并在图4.5中给出了一些具有代表性的模型及其关系。下面是对BERT后的plm的简要介绍。首先，为了更好的一般语言表示，有一些BERT的变体，如RoBERTa [38]和XLNet [70]。这些模型主要集中于训练前任务的改进。其次，一些人使用预先训练好的生成模型，如MASS [57]和UniLM [15]。这些模型在生成任务上取得了有希望的结果，而不是BERT使用的自然语言理解（NLU）任务。第三，BERT的句子对格式激发了跨语言和跨模态领域的工作。XLM [8]，ViLBERT [39]和VideoBERT [59]都是在这个方向上的重要作品。最后，由于一些低频知识不能有效地学习，有一些工作探索将外部知识纳入[46,81]中。

![image-20230805142952406](E:/typora/picture/刘志远文档/image-20230805142952406.png)

#### 4.4.5 Extensions

##### 4.4.5.1 Importance Sampling

受对比散度模型的启发，[4]提出采用重要抽样来加速神经语言模型的训练。他们首先使神经网络语言模型的输出，并将神经网络语言模型视为基于能量的概率模型的一种特例，如下：

$P(w_i|\mathbf{w}_{i-n}^{i-1})=\frac{\exp(-y_{w_i})}{\sum_j\exp(-y_j)}.$

重要性抽样的关键思想是通过对几个重要词进行抽样，而不是计算显式梯度，来逼近神经网络语言模型损失函数的对数似然梯度的平均值。在这里，神经网络语言模型的损失函数的对数似然梯度一般可以表示为

$\begin{aligned}
\frac{\partial P(w_{i}|\mathbf{w}_{i-n}^{i-1})}{\partial\theta}& =-\frac{\partial y_{w_{i}}}{\partial\theta}+\sum_{j=1}^{|V|}P(w_{j}|\mathbf{w}_{i-n}^{i-1})\frac{\partial y_{j}}{\partial\theta}  \\
&=-\frac{\partial y_{i}}{\partial\theta}+\mathbb{E}_{w_{k}\sim P}\left[\frac{\partial y_{k}}{\partial\theta}\right],
\end{aligned}$

其中，θ表示神经网络语言模型的所有参数。这里，损失函数的对数似然梯度包括两部分，包括目标单词wi的正梯度和所有单词wj的负梯度，即Ewi∼P[∂yj∂θ]。在这里，第二部分可以通过按照概率分布P抽样重要的单词来近似：

$\mathbb{E}_{w_k\sim P}\left[\frac{\partial y_k}{\partial\theta}\right]\approx\sum_{w_k\in V^{\prime}}\frac{1}{|V^{\prime}|}\frac{\partial y_k}{\partial\theta},$

###4.6 Summary

在本章中，我们将介绍句子表示法的学习。句子表示将句子的语义信息编码为实值表示向量，可用于进一步的句子分类或匹配任务。首先，我们介绍了句子和概率语言模型的单一热门表示。其次，我们广泛地介绍了几种神经语言模型，包括采用前馈神经网络、卷积神经网络、递归神经网络和语言模型的转换器。这些神经模型可以从语言建模中学习到丰富的语言和语义知识。受益于此，用大规模语料库训练的预制语言模型通过将学习到的语义知识从一般语料库转移到目标任务，在各种下游NLP任务上取得了最先进的性能。最后，我们介绍了句子表示的几种典型应用，包括文本分类和关系提取。

## 5 Document Representation

文件通常是自然语言的最高语言单位。文档表示旨在将整个文档的语义信息编码为实值表示向量，可进一步用于下游任务。近年来，文档表示已成为自然语言处理中的一项重要任务，并已广泛应用于许多文档级的现实世界应用中，如信息检索和问题回答。在本章中，我们首先介绍文档的一个热表示。接下来，我们将广泛地介绍学习单词和文档的主题分布的主题模型。此外，我们还介绍了分布式文档表示，包括段落向量和神经文档表示。最后，我们介绍了几个典型的文档表示的真实应用，包括信息检索和问题回答。

### 5.1 introduction

最近，神经网络模型在在机器翻译或解析等序列生成任务中生成有意义和具有语法的文档方面显示出了引人注目的结果。这部分归因于这些系统捕获局部组合的能力：相邻的单词通过语义和语法组合，形成它们想要表达的意义。基于神经网络模型，许多研究工作已经开发了多种方法来整合文档级的上下文信息。这些模型都是混合架构，因为它们在句子层次上是循环的，但是使用不同的结构来总结句子之外的上下文。此外，一些模型探索了在语言建模中结合局部和全局信息的多层循环架构。

在本章中，我们首先介绍文档的一个热表示。接下来，我们将广泛地介绍旨在学习单词和文档的潜在主题分布的主题模型。此外，我们还介绍了分布式文档表示，包括段落向量和神经文档表示。最后，我们介绍了几种典型的文档表示的真实应用，包括信息检索和问题回答。

### 5.2 One-Hot Document Representation

大多数机器学习算法都以一个固定长度的向量作为输入，因此需要将文档表示为向量。词袋模型是文档中最常见、最简单的表示方法。类似于一个热句表示，对于文档d = {w1，w2，...，wl}，可以使用一个包单词表示d来表示这个文档。具体来说，对于词汇表V = [w1，w2，...，w|V|]，单词w的一个热门表示是w = [0, 0,0,..., 1,..., 0].基于单热字表示和vocabV，它可以扩展为将文档表示为

实际上，词袋表示主要用作特征生成的工具，而从该方法计算出的最常见的特征类型是文档中出现的词频。这种方法简单而有效，有时可以在许多实际应用程序中达到优异的性能。但是，词袋表示仍然完全忽略了词序信息，这意味着只要使用相同的单词，不同的文档就可以有相同的表示。此外，词袋表示对单词的语义，或者更正式地说，单词之间的距离没有什么意义，这意味着这种方法不能利用隐藏在单词表示中的丰富信息。

### 5.3 Topic Model

随着我们的集体知识继续数字化，并以新闻、博客、网页、科学文章、书籍、图像、音频、视频和社交网络的形式存储，发现我们正在寻找的东西变得更加困难。我们需要新的计算工具来帮助组织、搜索和理解这些大量的信息。

现在，我们使用两种主要工具来处理在线信息——搜索和链接。我们在搜索引擎中输入关键字，然后找到一组与它们相关的文档。我们将查看该集合中的文档，可能会导航到其他链接的文档。这是一种与我们的在线档案互动的强大方式，但我们缺少了一些东西。

想象一下，基于贯穿文档的主题来搜索和探索文档。我们可以“放大”和“缩小”来找到特定的或更广泛的主题；我们可以看看这些主题是如何随着时间的推移而变化的，或者它们是如何联系在一起的。我们可能只通过关键字搜索来查找文档，而是首先找到我们感兴趣的主题，然后检查与该主题相关的文档。

例如，考虑使用主题来探索《纽约时报》的完整历史。在广泛的层面上，其中一些主题可能与报纸的章节相对应，如外交政策、国家事务和体育运动。我们可以放大一个令人感兴趣的主题，如外交政策，来揭示它的各个方面，如中国的外交政策、中东的冲突和美国与俄罗斯的关系。然后，我们可以通过时间来揭示这些具体的主题是如何变化的，例如，追踪过去50年里中东冲突的变化。并且，在所有这些探索中，我们将被指向与主题相关的原始文章。主题结构将是探索和消化文集的一种新的窗口。

但我们不会以这种方式与电子档案进行互动。虽然越来越多的文本可以在网上获得，但我们没有人力来阅读和研究它们来提供上述描述的浏览体验。为此，机器学习研究人员开发了概率主题建模，这是一套旨在发现并用主题信息注释大量文档档案的算法。主题建模算法是一种分析原始文本中的单词，以探索贯穿它们的主题，这些主题是如何连接的，以及它们是如何随时间变化的统计方法。主题建模算法不需要事先对文档进行任何注释或标记。这些主题来自于对原始文本的分析。主题建模使我们能够以一种人类注释不可能实现的规模来组织和总结电子档案。

#### 5.3.1 Latent Dirichlet Allocation

各种概率主题模型已被用来分析文档的内容和词语的意义。霍夫曼首先在他的概率潜在语义索引方法（pLSI）中引入了概率主题方法来进行文档建模。pLSI模型没有对混合物的权重是如何产生的做出任何假设，这使得测试模型对新文档的泛化能力变得困难。因此，通过在模型之前引入狄利克雷，扩展了潜在狄利克雷分配（LDA）。LDA被认为是一个简单但有效的主题模型。我们首先描述了LDA [6]的基本思想。

LDA背后的直觉是，文档显示了多个主题。LDA是一个文档集合的统计模型，它试图捕捉到这种直觉。它最容易描述的生成过程，即模型假设文档产生的想象随机过程。

我们正式地将一个主题定义为在一个固定的词汇表上的分布。我们假设这些主题是在生成任何数据之前指定的。现在，对于集合中的每个文档，我们在两个阶段的过程中生成单词。

1.随机选择一个分布的主题。

 2.对于文档中的每个单词，从步骤#1中的主题之上的分布中随机选择一个主题。•从相应的词汇表分布中随机选择一个词汇

我们强调，算法没有关于这些主题的信息，文章也没有标注主题或关键词。可解释的主题分布是通过计算可能生成所观察到的文档集合的隐藏结构而产生的

#####5.3.1.1  LDA and Probabilistic Models

LDA和其他主题模型是更广泛的概率建模领域的一部分。在生成概率建模中，我们将我们的数据视为来自于一个包含隐藏变量的生成过程。这个生成过程定义了一个关于观察到和隐藏随机变量的联合概率分布。给定观察到的变量，我们通过使用该联合分布来计算隐藏变量的条件分布来执行数据分析。这种条件分布也被称为后验分布。

LDA正好属于这个框架之中。观察到的变量是文档中的单词，隐藏的变量是主题结构，生成过程如上所述。从文档中推断隐藏主题结构的计算问题是计算文档中隐藏变量的后验分布和条件分布。

#####5.3.1.2 Posterior Computation for LDA

现在我们转向计算问题，计算给定观察文档的主题结构的条件分布。（正如我们上面提到的，这叫做后路。）用我们的符号，后验是

$P(\beta_{1:K},\theta_{1:D},z_{1:D}|\nu_{1:D})=\frac{P(\beta_{1:K},\theta_{1:D},z_{1:D},\nu_{1:D})}{P(\nu_{1:D})}.$

分子是所有随机变量的联合分布，它可以很容易地计算出隐藏变量的任何设置。分母是观察到的边际概率，即在任何主题模型下看到观察到的语料库的概率。在理论上，它可以通过对隐藏的主题结构的每一个可能的实例化上的联合分布求和来计算。

主题建模算法通过在潜在的主题结构上形成一个替代分布，以接近真实的后验，从而形成上述方程的近似。主题建模算法通常可分为两类：基于抽样的算法和变分算法。

基于抽样的算法试图通过用经验分布近似后验来收集样本。主题建模中最常用的抽样算法是吉布斯抽样，其中我们构造一个马尔可夫链，一个随机变量序列，每个变量依赖于前一个变量，其极限分布是后验的。在特定语料库的隐主题变量上定义了马尔可夫链，算法是长期运行该链，从极限分布中收集样本，然后与收集的样本近似分布。

变分方法是基于抽样的算法的一种确定性替代方法。变分方法不是用样本来近似后验，而是在隐藏结构上假设一个参数化的分布族，然后找到最接近后验的家族成员。因此，推理问题转化为优化问题。变分方法为优化方面的创新对概率建模的实际影响打开了大门。

### 5.4  Distributed Document Representation

为了解决单词袋文档表示的缺点，[31]提出了段落向量模型，包括使用分布式内存的版本（PV-DM）和使用分布式单词袋的版本（PV-DBOW）。此外，研究人员还提出了几种层次神经网络模型来表示文档。在本节中，我们将详细介绍这些模型。

#### 5.4.1 Paragraph Vector

如图5.2所示，段落向量将每个段落映射到一个唯一的向量，由矩阵P中的一列表示，将每个单词映射到一个唯一的向量，由单词嵌入矩阵e表示。段落向量和单词向量的平均或连接来预测上下文中的下一个词。更正式地说，与单词向量框架相比，这个模型中唯一的变化是在下面的方程中，他由E和P构造。

$y=\text{Softmax}(h(w_{t-k},\ldots,w_{t+k};\mathbf{E},\mathbf{P})),$

其中，h由从E和P中提取的词向量的连接或平均值构造。

![image-20230805170006642](E:/typora/picture/刘志远文档/image-20230805170006642.png)

该模型的另一部分是，给定一系列训练单词w1、w2、w3、...，wl，段落向量模型的目标是最大化平均对数概率：

$\mathscr{O}=\dfrac{1}{l}\sum_{i=k}^{l-k}\log P(w_i\mid w_{i-k},\dots,w_{i+k}).$

预测任务通常通过多类分类器来完成，如softmax。因此，概率方程为

$P(w_i\mid w_{i-k},\ldots,w_{i+k})=\frac{e^{y_{w_i}}}{\sum_je^{y_j}}.$

段落标记可以被认为是另一个词。它充当一个记忆，记住当前上下文或段落的主题中缺少的内容。因此，这个模型通常被称为段落向量的分布式记忆模型（PV-DM）。

上述方法考虑了段落向量与单词向量的连接，以预测文本窗口中的下一个单词。另一种方法是忽略输入中的上下文词，但强制模型预测从输出中的段落中随机抽样的单词。在现实中，这意味着在随机梯度下降的每次迭代中，我们采样一个文本窗口，然后从文本窗口中采样一个随机单词，并形成一个给定段落向量的分类任务。该技术如图5.3所示。这个版本被命名为段落向量（PV-DBOW），而不是前一节中的段落向量（PV-DM）。

![image-20230805170953998](E:/typora/picture/刘志远文档/image-20230805170953998.png)

除了概念上的简单之外，这个模型还需要存储更少的数据。只需要存储的数据是软max权值，而不是在之前的模型中同时存储软max权值和单词向量。该模型也类似于在词向量上的Skip-gram模型。

#### 5.4.2  Neural Document Representation

在这部分中，我们介绍了两种主要的文档表示神经网络，包括文档-上下文语言模型和分层文档自动编码器。

##### 5.4.2.1 Document-Context Language Model

循环体系结构可以用于在文档语言建模中结合本地信息和全局信息。最简单的模型是训练一个RNN，忽略上面提到的句子边界；前一个句子t−1的最后一个隐藏状态用于初始化句子t中的第一个隐藏状态。在这样的架构中，RNN的长度等于文档中标记的数量；在典型的类型如新闻文本，这意味着训练RNN序列的几百个标记，这引入了两个问题： (1)信息衰减在一个句子三十标记新闻文本（不罕见），从前面的句子必须通过循环动态传播三十次之前可以达到当前句子的最后一个令牌。有意义的文档级信息不太可能在如此长的管道中存活下来。(2)学习，训练涉及许多时间步骤的循环架构是众所周知的困难。在在整个文档上训练RNN的情况下，反向传播必须运行数百个步骤，从而带来严重的数值挑战。

为了解决这两个问题，[28]建议使用多级循环结构来表示文档，从而成功地在语言建模中有效地利用了文档级上下文。他们首先提出了上下文到上下文文档上下文语言模型（ccDCLM），该模型假设来自先前句子的上下文信息需要能够“短路”标准RNN，以便更直接地影响更长文本跨度的单词生成。正式来说，我们有

![image-20230805171430700](E:/typora/picture/刘志远文档/image-20230805171430700.png)

此外，他们还提出了上下文到输出的文档-上下文语言模型（coDCLM）。coDCLM模型没有将文档上下文合并到隐藏状态的循环定义中，而是将其直接推到输出中，如图5.5所示。设ht，n是句子t的常规RNNLM中的隐藏状态，

![image-20230805171450651](E:/typora/picture/刘志远文档/image-20230805171450651.png)

##### 5.4.2.2 Hierarchical Document Autoencoder

参考文献[33]还提出了分层的文档自动编码器来表示文档。该模型利用了一种直觉，即就像单词的并置创造了句子的联合意义一样，句子的并置也创造了段落或文档的联合意义。

他们首先通过将一层LSTM（记为LSTMwor d编码）放在句子级上获得表示向量：

$h_t^w(\text{enc})=\text{LSTM}_{encode}^{word}(\textbf{w}_t,h_{t-1}^\nu(\text{enc})).$

在结束时间步长时的向量输出用于将整个句子表示为

$\mathbf{s}=h_{end_s}^w.$

为了构建当前文档/段落的表示eD，在所有句子上放置另一层LSTM（记为LSTM句子编码），依次计算每个时间步的表示：

$h_t^s(\text{enc})=\text{LSTM}_{encode}^{sentence}(\textbf{s},h_{t-1}^s(\text{enc})).$

初始时间步长hs0(d)=(d)=，即编码过程hs t (d)的端到端输出作为LSTMwor d解码的原始输入，用于随后预测句子t + 1内的标记。LSTMwor d解码依次预测每个位置的标记，然后将其嵌入与之前的隐藏向量相结合，进行下一个时间步长预测，直到预测结束标记。该程序可总结如下：

$\begin{aligned}h_t^w(\text{dec})&=\text{LSTM}_{decode}^{sentence}(\textbf{w}_t,h_{t-1}^w(\text{dec})),\\\\P(w|\cdot)&=\text{Softmax}(\textbf{w},h_{t-1}^w(\text{dec})).\end{aligned}$

![image-20230805172117670](E:/typora/picture/刘志远文档/image-20230805172117670.png)

注意模型采用了一种回顾策略，即将当前的解码阶段与输入的句子联系起来，试图考虑输入的哪一部分对当前的解码状态最负责

![image-20230805172138058](E:/typora/picture/刘志远文档/image-20230805172138058.png)

### 5.5  Applications

在本节中，我们将介绍一些基于表示学习的文档级分析的应用程序。

#### 5.5.1 Neural Information Retrieval

信息检索旨在从大规模的信息资源收集中获取相关资源。如图5.8所示，以查询“SteveJobs”作为输入，搜索引擎（信息检索的典型应用程序）为用户提供了相关的网页。传统的信息检索数据由搜索查询和文档收集d组成，基本真相是通过显式的人类判断或隐含的用户行为数据，如点击率。

对于给定的查询q和文档d，传统的信息检索模型通过词汇匹配来估计它们的相关性。神经信息检索模型更注重从语义匹配中获取查询和文档的相关性。词汇匹配和语义匹配都是神经信息检索的关键。通过神经网络黑魔法的发展，帮助信息检索模型捕捉到更复杂的匹配特征，达到了信息检索任务[17]的先进水平。

目前的神经排序模型可以分为两组：基于表示的[23]和基于交互的[23]。早期的工作主要关注基于表示的模型。它们学习良好的表示，并在学习到的查询和文档的表示空间中匹配它们。另一方面，基于交互的方法根据它们的术语的交互对查询文档匹配进行建模。

##### 5.5.1.1 Representation-Based Neural Ranking Models

基于表示的方法通过分别学习两种分布式表示，直接将查询和文档进行匹配，然后根据它们之间的相似性计算匹配分数。近年来，一些基于暹罗架构的深度神经模型被探索出来，它们可以通过前馈层、卷积神经网络或递归神经网络来完成。

参考[26]提出了深度结构语义模型（DSSM），首先将单词散列到基于字母三字的表示。然后使用多层完全连接的神经网络将查询（或文档）编码为向量。查询和文档之间的相关性可以简单地用余弦相似度来计算。参考[26]通过最小化点击数据上的交叉熵损失来训练模型，其中每个训练样本由一个查询q、一个正文档d+和一个均匀采样的负文档集D组成

##### 5.5.1.2 Interaction-Based Neural Ranking Models

基于交互的神经排序模型从查询-文档对中学习单词级的交互模式，如图5.9所示。它们还提供了一个单独比较查询的不同部分与文档的不同部分的机会并汇总了相关性的部分证据。ARC-II [25]和匹配金字塔[45]利用卷积神经网络从单词级交互中捕获复杂的模式。深度相关性匹配模型（DRMM）使用金字塔池化（直方图）将单词级的相似性总结为排序模型[23]。也有一些工作建立了排序模型[27,46]的位置依赖的交互作用。

![image-20230805174257847](E:/typora/picture/刘志远文档/image-20230805174257847.png)

#### 5.5.2 Question Answering

问答（QA）是最重要的任务之一，NLP中的文档级应用程序也是如此。在QA方面已经投入了许多努力，特别是在机器阅读理解和开放领域QA方面。在本节中，我们将分别介绍这两个任务的进展

##### 5.5.2.1 Machine Reading Comprehension

一般来说，当前的机器阅读理解任务可以根据[10]的回答类型分为四类，即封闭风格、多重选择、跨度预测和自由形式的回答。像CNN/每日邮报[24]这样的结尾处风格的任务由填写空白的句子组成，其中问题包含一个要填写的占位符。答案a可以从预定义的候选集|A|中选择，或者从词汇表|V|中选择。RACE [30]和MCTest [50]等多项选择任务的目的是从一组答案选择中选择最佳答案。通常使用准确性来衡量这两个任务的表现：在整个示例集中正确回答问题的百分比，因为问题可以从给定的假设答案集中正确回答或不正确回答。

大多数机器阅读理解模型遵循相同的范式来定位答案跨度的起点和终点。如图5.12所示，该模型在对段落进行编码时，保留了序列的长度，并将问题编码为一个固定长度的隐藏表示q。然后，问题的隐藏向量被用作一个指针，扫描段落表示{pi}i n =1，并计算段落中每个位置的分数。在保持类似的架构的同时，大多数机器阅读理解模型在段落和问题之间的交互方法上都有所不同。下面，我们将介绍几个遵循此范例的经典阅读理解架构。

![image-20230807091106499](E:/typora/picture/刘志远文档/image-20230807091106499.png)

##### 5.5.2.2  Open-Domain Question Answering

开放域QA（OpenQA）最早由[21]提出。该任务旨在使用外部资源回答开放领域的问题，如文档集合[58]、网页[14,29]、结构化知识图[3,7]或自动提取的关系三元组[20]。近年来，随着机器阅读理解技术[11,16,19,55,63]的发展，研究人员试图通过对普通文本进行阅读理解来回答开放领域的问题。参考文献[12]提出使用基于神经的模型来回答开放领域的问题。如图5.14所示，基于神经的OpenQA系统通常从一个大规模的语料库中检索问题的相关文本，然后使用阅读理解模型从这些文本中提取答案。

DrQA系统由两个组成部分组成： (1)用于查找相关文章的文档检索器模块和(2)用于从给定的上下文中提取答案的文档读者模型。

文档检索器被用作第一次快速浏览，以缩小搜索空间，并关注可能相关的文档。检索器为文档和问题构建TF-IDF加权词袋向量，并计算相似度得分进行排序。为了进一步利用局部词序信息，检索器使用带有散列的双格符计数，同时保持了速度和内存效率。文档阅读器模型选取了由文档检索器生成的维基百科上的前5篇文章，并提取了这个问题的最终答案。对于每一篇文章，文档阅读器用一个信心分数来预测答案跨度。最终的预测是通过最大化整个文档中的预测分数的非标准化指数来实现的

### 5.6 Summary

在本章中，我们引入了文档表示学习，它将整个文档的语义信息编码为实值表示向量，为利用文档信息进行下游任务提供了一种有效的方法，并显著提高了这些任务的性能。首先，我们介绍了文档的一个热表示。接下来，我们广泛地介绍主题模型来使用潜在的主题分布来表示单词和文档。此外，我们还介绍了分布式文档表示，包括段落向量和神经文档表示。最后，我们介绍了几种典型的文档表示的真实应用，包括信息检索和问题回答。

## 6 Sememe Knowledge Representation

语言知识图（例如，WordNet和HowNet）描述了形式语言和结构语言中的语言知识，它们可以很容易地融入到现代自然语言处理系统中。在本章中，我们将重点关注关于HowNet的研究。我们首先简要介绍了HowNet和半音素的背景和基本概念。接下来，我们介绍半素表示学习的动机和现有的方法。在本章的最后，我们回顾了半音素表示的重要应用。

###6.1 Introduction

在自然语言处理（NLP）领域中，单词通常是最小的研究对象，因为它们被认为是人类语言中能够独立存在的最小的有意义的单元。然而，单词的意思可以进一步分为更小的部分。例如，人的意义可以被认为是人、男性和成人的意义的结合，而男孩的意义是由人、男性和儿童的意义组成的。在语言学中，意义的最小不可分割单位，即语义单位，被定义为语素[8]。一些语言学家认为，所有单词的意思都可以由一组有限的封闭语义组成。

然而，语素是隐式的，因此，很难直观地定义语素集，并确定一个单词可以有哪些语素。因此，一些研究者花了数十年的时间从各种词典和语言知识库（KBs）中筛选语素，并用这些选定的语素对单词进行注释，构建基于语素的语言知识库。WordNet和HowNet [17]是这类kb中最著名的两个。在本节中，我们将重点关注HowNet中语言知识的表示。

#### 6.1.1 Linguistic Knowledge Graphs

##### 6.1.1.1 WordNet

WordNet根据意义将英语名词、动词、形容词和副词分组为同义词（即认知同义词集），这代表了一个独特的概念。每个同步集都有一个简短的描述，在大多数情况下，甚至有一些简短的句子作为例子来说明在这个同步集中的单词的使用。概念-语义关系和词汇关系将同步集和单词联系起来。单词之间的主要关系是同义词，表示单词具有相似的含义，在某些情况下可以被其他词取代，而词集之间的主要关系是超名/下义词（即ISA关系），表示更一般的词集和更具体的词集之间的关系。动词词也有层次结构，反义是描述意思相反的形容词之间的关系。综上所述，所有WordNets的117,000个同步集都通过少量的概念关系相互连接。

##### 6.1.1.2 HowNet

HowNet还为半语素构建了一个分类法。HowNet的所有半音素都可以分为以下类型之一：事物、部分、属性、时间、空间、属性值和事件。此外，为了更精确地描述单词的语义，HowNet将被称为“动态角色”的语素之间的关系合并到单词的语素注释中。

考虑到多义词，HowNet区分了语素注释中每个词的不同含义。每一种感觉也可以用中文和中文来表达英语图6.1显示了一个单词的半音素注释的实例。从图中可以看出，苹果一词有四种意义，包括苹果（电脑）、苹果（电话）、苹果（水果）和苹果)（树），每个意义都是“半素树”的根节点，其中每一对父子半素节点都是多关系的。此外，HowNet还为每个感觉注释了POS标签，并添加了情绪类别和一些使用示例。

![image-20230807093831295](E:/typora/picture/刘志远文档/image-20230807093831295.png)

### 6.2 Sememe Knowledge Representation

这些研究表明，词义消除歧义对WRL至关重要，HowNet中词义的半素注释可以为这些任务[63]提供必要的语义正则化。为了探索其可行性，我们引入了半符号编码的Word表示学习（SE-WRL）模型，该模型可以同时检测词义和学习表示。更具体地说，该框架将每个词义视为其语义的组合，并根据其上下文迭代地执行词义消歧，并通过扩展Word2vec [43]中的跳格来学习词义、词义和单词的表示。在此框架下，提出了一种基于注意力的方法，根据上下文自动选择合适的词义。充分利用符号中，我们介绍了SE-WRL的三种不同的学习和注意策略SSA、SAC、SAT和SAT，将在下面的段落中描述。

#### 6.2.1  Simple Sememe Aggregation Model

简单半素聚合模型（SSA）是一种基于Skip-gram模型的简单思想。对于每个单词，SSA将该单词的所有意义上的所有信素考虑在一起，并使用其所有信素嵌入的平均值来表示目标单词。正式来说，我们有

$\mathbf{w}=\frac{1}{m}\sum_{s_i^{(w)}\in S^{(w)}x_j^{(s_i)}\in X_i^{(w)}}\mathbf{x}_j^{(s_i)},$

该模型遵循的假设是，一个词的语义意义是由语义单位，即语素组成的。与传统的跳过图模型相比，由于半素由多个词共享，该模型可以利用半素信息对词之间潜在的语义关联进行编码。在这种情况下，共享相同半音素的相似词可能最终会得到相似的表示。

#### 6.2.2  Sememe Attention over Context Model

SSA模型用聚合的半素嵌入代替目标词嵌入，将半素信息编码到单词表示学习中。然而，SSA模型中的每个单词在不同的上下文中仍然只有一个表示，这不能处理大多数单词的多义性。很直观，我们应该根据特定的上下文为目标词构建不同的嵌入，在HowNet中支持词义注释。

为了解决这个问题，提出了半上下文关注模型（SAC）。SAC利用注意方案，根据目标词自动为上下文词选择合适的感官。也就是说，SAC对上下文词进行词义消歧，以更好地学习目标词的表示。SAC模型的结构如图6.2所示。

![image-20230807095507413](E:/typora/picture/刘志远文档/image-20230807095507413.png)

更具体地说，SAC利用了目标词w的原始词嵌入，并使用半音素嵌入来表示上下文词wc，而不是原始的上下文词嵌入。假设一个词通常在一个句子中显示了一些特定的含义。这里采用目标词嵌入作为注意点来选择最合适的意义来组成上下文词的嵌入。上下文词嵌入wc可以形式化如下：

$\mathbf{w}_c=\sum_{j=1}^{|S^{(w_c)}|}\mathrm{Att}(s_j^{(w_c)})\mathbf{s}_j^{(w_c)},$

$\operatorname{Att}(s_j^{(w_c)})=\frac{\exp(\mathbf{w}\cdot\mathbf{\hat{s}}_j^{(w_c)})}{\sum_{k=1}^{|S^{(w_c)}|}\exp(\mathbf{w}\cdot\mathbf{\hat{s}}_k^{(w_c)})}.$

注意策略假设上下文词义嵌入与目标词w越相关，在构建上下文词嵌入时就越应该考虑这种意义。与有利于注意的方案，每个上下文词可以表示为其意义上的特定分布。这可以被看作是软的WSD，它有助于学习更好的单词表示。

####6.2.3 Sememe Attention over Target Model

上下文注意模型可以根据目标词灵活地为上下文词选择合适的意义和半素。该过程也可以通过以上下文词为注意词，为目标词选择合适的感官。因此，我们提出了半素注意超过目标模型（SAT），如图6.3所示。

![image-20230807101922156](E:/typora/picture/刘志远文档/image-20230807101922156.png)

与SAC模型不同，SAT学习上下文词的原始词嵌入和目标词的半素嵌入。然后SAT应用上下文词对目标词w的多种意义进行注意，构建w的嵌入，形式化如下：

$\mathbf{w}=\sum_{j=1}^{|S^{(w)}|}\mathbf{Att}(s_j^{(w)})\mathbf{s}_j^{(w)},$

而基于上下文的注意力的定义如下：

$\operatorname{Att}(s_j^{(w)})=\frac{\exp(\mathbf{w}_c^{\prime}\cdot\mathbf{\hat{s}}_j^{(w)})}{\sum_{k=1}^{|S^{(w)}|}\exp(\mathbf{w}_c^{\prime}\cdot\mathbf{\hat{s}}_k^{(w)})},$

回想一下，SAC只使用一个目标词作为注意来选择上下文词的意义，而SAT同时使用几个上下文词作为注意来选择目标词的适当意义。因此，SAT有望进行更可靠的WSD，并产生更准确的单词表示，这在实验中得到了探索。

### 6.3 Applications

在上一节中，我们将介绍HowNet和半音素表示。事实上，像HowNet这样的语言知识图包含了丰富的信息，可以有效地帮助下游的应用程序。因此，在本节中，我们将介绍语素表示的主要应用，包括基于语素的单词表示、语言知识图的构建和语言建模。

#### 6.3.1 Sememe-Guided Word Representation

词素导向词表示是通过引入源语言中基于语素的语言kb的信息来改进用于语素预测的词嵌入。Qi等人，[56]提出了两种半音素引导的词表示方法。

##### 6.3.1.1  Relation-Based Word Representation

一种简单而直观的方法是让具有相似语素注释的单词倾向于具有相似的单词嵌入，这被称为基于单词关系的方法。首先，一个同义词列表是由基于语素的语言KBs构建的，其中共享一定数量语素的词被视为同义词。接下来，同义词被迫有更紧密的单词嵌入。

形式上，让wi是wi的原始词嵌入，ˆwi是它调整后的词嵌入。并设Syn（wi）表示单词wi的同义词集。则将损失函数定义为

$\mathscr{L}_{sememe}=\sum_{w_i\in V}\bigg[\alpha_i\|\mathbf{w}_i-\hat{\mathbf{w}}_i\|^2+\sum_{w_j\in\mathrm{Syn}(w_i)}\beta_{ij}\|\mathbf{\hat{w}}_i-\hat{\mathbf{w}}_j\|^2\bigg],$

其中，α和β控制了这两项的相对强度。应该注意的是，强迫相似的单词有紧密的单词嵌入的想法类似于最先进的改造方法[19]。然而，重构方法不能在这里应用，因为像HowNet这样的基于符号的语言kb不能直接提供其需要的同义词列表。

##### 6.3.1.2 Sememe Embedding-Based Word Representation

基于词关系的方法虽然简单而有效，但由于它忽略了语义与词之间的复杂关系以及不同语素之间的关系，因此不能充分利用基于语义的语言kb的信息。为了解决这一局限性，提出了基于半素嵌入的方法，即联合学习半素嵌入和单词嵌入。

在这种方法中，半语素也用分布式向量表示，并将它们放在与单词相同的语义空间中。SPSE [66]通过分解词-半素矩阵和半半素矩阵来学习半素嵌入，该方法利用半素嵌入作为正则化器来学习更好的词嵌入。与SPSE不同，[56]中描述的模型不使用预先训练好的单词嵌入。相反，它同时学习单词嵌入和半素嵌入。更具体地说，可以从HowNet中提取一个单词半素矩阵M，其中Mi j = 1表示单词wi用半素xj注释，否则表示Mi j = 0。因此，通过因式分解M，损失函数可以定义为

$\mathscr{L}_{sememe}=\sum_{w_i\in V,x_j\in X}(\mathbf{w}_i\cdot\mathbf{x}_j+b_s+b'_j-\mathbf{M}_{ij})^2,$

该方法在统一的语义空间中获得词和语素嵌入。半素嵌入包含所有关于单词和半素之间关系的信息，并将这些信息注入到单词嵌入中。因此，我们期望单词嵌入更适合用于半音素的预测。

#### 6.3.2 Sememe-Guided Semantic Compositionality Modeling

语义组合性（SC）是一种语言现象，即语法复杂单元的意义是复杂单元的组成部分及其组合规则[50]的意义的函数。一些语言学家认为SC是语义学上的[51]的基本真理。在自然语言处理领域，SC在语言建模、[47]、情感分析、[42,61]、语法解析、[59]等许多任务中都被证明是有效的。

参考文献[55]提出了一种新的基于语义语素的语义组合建模方法。他们认为半符号有利于SC建模。为了验证这一点，他们首先设计了一个简单的SC度（SCD）测量实验，发现由简单的基于半素的公式计算的MWEs的SCD与人类判断高度相关。这一结果表明，半语素可以很好地描述信息信息及其组成部分的意义，并捕捉双方之间的语义关系。此外，他们还提出了两种包含半信素的学习SC嵌入模型，即具有聚合半信素（SCAS）模型的语义组合性和具有相互半信素注意（SCMSA）模型的语义组合性。当学习MWE的嵌入时，SCAS模型将MWE的组成及其半素的嵌入连接起来，而SCMSA模型考虑一个组成的半素和其他组成之间的相互注意。最后，他们整合了组合规则，即在等式中的R（6.10），进入两种模型。他们的模型在MWE相似度计算任务和半音素预测任务上取得了显著的性能

在本节中，我们将重点关注[55]所进行的工作。我们将首先引入基于半符号的SC度（SCD）计算公式，然后扩展它们的半符号合并的SC模型。

##### 6.3.2.1 Sememe-Based SCD Computation Formulae

虽然SC广泛存在于MWEs中，但并不是每个MWE都是完全语义组成的。事实上，不同的管理组织表现出不同程度的SC。参考[55]认为，半聚糖可以方便地测量SCD。为此，基于一个单词的所有半音素都能准确地描述这个单词的含义的假设，他们直观地设计了一组与SCD原理相一致的SCD计算公式。计算公式见表6.2。它们定义了4个scd，分别用数字3、2、1和0表示，其中数字越大意味着较高的scd越高。Sp、Sw1和Sw2分别表示MWE的第一和第二组成部分的半音素集。下面是对他们的SCD计算公式的一个简要解释：

##### 6.3.2.3 Sememe-Incorporated SC Models

在本节中，我们首先详细介绍两个基本的半素合并SC模型，即具有聚合半素的语义组合性（SCAS）和具有相互半素注意的语义组合性（SCMSA）。SCAS模型简单地连接了MWE的成分及其半素的嵌入，而SCMSA模型则考虑了一个成分的半素和其他成分之间的相互注意。然后，我们描述了如何将组合规则集成到这两个基本模型中。

![image-20230807154415419](E:/typora/picture/刘志远文档/image-20230807154415419.png)

(1) SCAS模型我们引入的第一个模型是SCAS模型，如图6.4所示。SCAS模型的想法很简单，也就是说，也很简单连接一个组成部分的词嵌入及其半语素嵌入的集合。正式来说，我们有

$\mathbf{w}_1^{\prime}=\sum_{x_i\in X_{w_1}}\mathbf{x_i},\quad\mathbf{w}_2^{\prime}=\sum_{x_j\in X_{w_2}}\mathbf{x_j},$

$\mathbf{p}=\tanh(\mathbf{W}_c[\mathbf{w}_1+\mathbf{w}_2;\mathbf{w}_1^{\prime}+\mathbf{w}_2^{\prime}]+\mathbf{b}_c),$





![image-20230807154554491](E:/typora/picture/刘志远文档/image-20230807154554491.png)

(2) **SCMSA Model**

SCAS模型简单地使用一个成分的所有半聚体嵌入的总和作为外部信息。但是，一个组成的含义可能随另一个组成而不同，因此，当一个组成与不同的组成结合时，一个组成的半素应该有不同的权重（在案例研究中有一个例子）。

相应地，我们引入了SCMSA模型（图6.5），该模型采用相互注意机制，动态地赋予半音素权值。正式来说，我们有

$\begin{aligned}
&\mathbf{e}_1&& =\mathrm{tanh}(\mathbf{W}_{a}\mathbf{w}_{1}+\mathbf{b}_{a}),  \\
&a_{2,i}&& =\frac{\exp\left(\mathbf{s}_i\cdot\mathbf{e}_1\right)}{\sum_{x_j\in X_{w_2}}\exp\left(\mathbf{x}_j\cdot\mathbf{e}_1\right)},  \\
&\mathbf{w}_{2}^{\prime}&& =\sum_{x_i\in X_{w_2}}a_{2,i}\mathbf{x}_i, 
\end{aligned}$

![image-20230807155046835](E:/typora/picture/刘志远文档/image-20230807155046835.png)

但是，有许多不同的组合规则，有些规则有稀疏的实例，不足以训练相应的组合矩阵使用d×2d参数。此外，我们认为除了组合规则特定的组合性信息外，组合矩阵应该包含共同的组合性信息。因此，它们让组合矩阵Wc为包含组合规则信息的低秩矩阵和包含公共组合性信息的矩阵的和：

$\begin{aligned}\mathbf{W}_c&=\mathbf{U}_1^r\mathbf{U}_2^r+\mathbf{W}_c^c,&(6.17)\\\text{where }\mathbf{U}_1^r&\in\mathbb{R}^{d\times d_r},\mathbf{U}_2^r\in\mathbb{R}^{d_r\times2d},\text{ and }d_r\in\mathbb{N}_+\text{ is a hyperparameter and may vary}\\\text{with the combination rule, and }\mathbf{W}_c^c&\in\mathbb{R}^{d\times2d}.\end{aligned}$

#### 6.3.3  Sememe-Guided Language Modeling

语言建模（LM）的目的是衡量一个单词序列的概率，反映其流畅性和作为一个在人类语言中可行的句子的可能性。语言建模是广泛的自然语言中的一个重要组成部分处理（NLP）任务，如机器翻译[9,10]、语音识别[34]、信息检索[5,24,45,54]和文档摘要[2,57]。

一个概率语言模型计算下一个单词的上下文单词的条件概率，这通常是从大规模的文本语料库中学习到的。以最简单的语言模型为例，n-gram根据文本语料库[31]上的最大似然来估计条件概率。近年来，递归神经网络（RNNs）作为最先进的语言建模方法，将上下文表示为一个低维隐藏状态来预测下一个单词（图6.6）。

![image-20230807155651063](E:/typora/picture/刘志远文档/image-20230807155651063.png)

 (1)首先，SDLM根据上下文估计半聚体的分布。(2)SDLM将这些半聚体作为专家，采用专家方法的稀疏乘积来选择最可能的感官。(3)最后，SDLM通过边缘化感官的分布来计算单词的分布。

SDLM串联由三个模块组成：符号预测器、感觉预测器和Word预测器（图6.6）。半素预测器首先将上下文向量作为输入，并为每个半素分配一个权重。然后将每个半音素视为一个专家，并对意义预测器中的一组意义上的概率分布进行预测。最后，在word预测器中得到每个单词的概率。

半信素预测器以上下文向量g∈RH1作为输入，并为每个半信素分配一个权重。假设给定上下文w1，w2，...，wt−1，单词wt包含信素xk（k∈{1,2，...，K}）的事件是独立的，因为信素是最小的语义单位，并且在任何两个不同的语素之间不存在语义重叠。为简单起见，上标t将被忽略。半素预测器被设计为具有s型激活函数的线性解码器。因此，将下一个单词包含半音素xk的概率表示为

$p_k=P(x_k|\mathbf{g})=\text{Sigmoid}(\mathbf{g}\cdot\mathbf{v}_k+b_k),$

感觉预测器和Word预测器。感觉预测器的架构是由专家的产品（PoE）[25]驱动的。每个半素都被认为是专家，只对与之相关的感官进行预测。设S（xk）表示包含第k个专家符号xk的感官集合。与传统的神经语言模型直接使用上下文向量g∈RH1和输出嵌入w∈RH2来生成每个单词的分数不同，感觉预测器使用φ(k)（g，w）来计算专家xk给出的分数。并选择一个由矩阵Uk∈RH1×H2参数化的双线性函数作为φ(k)（·，·）的直接实现：

$\phi^{(k)}(\mathbf{g},\mathbf{w})=\mathbf{g}^{\top}\mathbf{U}_{k}\mathbf{w}.$

#### 6.3.4 Sememe Prediction

HowNet的手工构建实际上是耗时和劳动密集型的，例如，HowNet已经由几位语言专家建立了10多年。然而，随着交流和技术的发展，新词和短语的出现，现有词的语义也在动态发展。在这种情况下，持续的手动注释和更新正变得更加不堪重负。此外，由于语素本体和当专家协同注释词汇语义时，保持注释的一致性也具有挑战性。针对解决人工标注的不灵活性和不一致问题，提出了自动词汇语素预测任务，有望有助于专家标注，减少人工工作量。请注意，为了简单起见，本部分介绍的大多数作品都没有考虑词半素的复杂层次，而是简单地将每个单词的所有带注释的词素分组为用于学习和预测的半素集。

语素预测的基本思想是，那些语义相似的词可能具有重叠的语素。因此，语素预测的关键挑战是如何表示单词和语素的语义意义，以建模它们之间的语义相关性。在这部分中，我们将重点介绍由Xie等人的[66]完成的半音素预测词。在他们的工作中，他们提出使用分布式表示学习[26]来建模单词和语义的语义。分布式表示学习的目标是将对象编码到一个低维的语义空间中，这显示出了其强大的人类语言语义建模能力，例如，单词嵌入[43]在自然语言处理的各种任务中得到了广泛的研究和应用。

### 6.4 Summary

在本章中，我们首先介绍了最著名的语素知识库HowNet，它使用了大约2,000个预定义的语素来注释超过100,000个中文和英语单词和短语。与WordNet等其他语言知识库不同，HowNet基于最小语义单位（语素），并捕获语素和单词之间的组合关系。为了学习半素知识的表示，我们详细阐述了三个模型，即简单半素聚集模型（SSA）、半素注意超过上下文模型（SAC）和半素注意超过目标模型（SAT）。这些模型不仅学习符号的表示，而且有助于改进单词的表示。接下来，我们描述了语素知识的一些应用，包括单词表示法、语义组合和语言建模。我们还详细介绍了如何自动预测单语和跨语言无注释单词的半音素。

## 7 World Knowledge Representation

世界知识表示旨在在低维语义空间中表示知识图中的实体和关系，已广泛应用于大型知识驱动任务中。在本章中，我们首先介绍了知识图的概念。接下来，我们将介绍其动机，并概述了现有的知识图表示方法。此外，我们将讨论几种旨在处理知识图表示的当前挑战的高级方法。我们还回顾了知识图表示的现实世界中的应用，如语言建模、问题回答、信息检索和推荐系统。

### 7.1 Introduction

为了将知识编码到现实应用中，知识图表示表示分布式表示知识图中的实体和关系，并应用于各种现实世界的人工智能领域，包括问题回答、信息检索和对话系统。也就是说，知识图表示学习作为知识图和知识驱动任务之间的桥梁起着至关重要的作用。

在本节中，我们将介绍知识图的概念，几个典型的知识图，知识图的表示学习，以及几个典型的知识驱动任务。

#### 7.1.1 World Knowledge Graphs

目前在信息检索和问题回答的应用中有几种广泛的应用。在本小节中，我们将介绍一些著名的KGs，如免费基础、DBpedia、Yago和WordNet。事实上，在垂直搜索的特定知识领域中，也有许多相对较小的kg。

 **Freebase**是世界上最流行的知识图之一。这是一个大型的由社区管理的数据库，由知名人士、地点和事物组成，

DBpedia是一项众包社区活动，旨在从维基百科中提取结构化内容，并使这些信息可以在网络上访问。它是由柏林自由大学、莱比锡大学和OpenLink软件公司的研究人员发起的，

YAGO是另一个伟大本体论的简称，是由萨尔布鲁克普朗克计算机科学研究所开发的高质量的KG，最初于2008年发布。YAGO中的知识自动从维基百科、WordNet和地理名称中提取，其准确性经过人工评估，证明其准确率为95%。YAGO之所以特殊，不仅是因为每个事实的置信值取决于人工评估，还因为YAGO锚定在空间和时间上，这可以为其部分实体提供空间维度或时间维度。

### 7.2 Knowledge Graph Representation

然而，随着KG规模的增加，我们面临着两个主要的挑战：数据稀疏性和计算效率低下。数据稀疏性是社交网络分析或兴趣挖掘等许多领域的一个普遍问题。这是因为在一个大的图中有太多的节点（例如，用户、产品或实体），而这些节点之间的边（例如，关系）太少，因为在现实世界中，一个节点的关系的数量是有限的。随着知识图规模的增加，计算效率是我们需要克服的另一个挑战。

为了解决这些问题，我们将表示法学习引入到知识表示法中。KGs中的表征学习旨在将实体和关系投射到一个低维的连续向量空间中，从而得到它们的分布式表征，其表现已经在单词表征和社会表征中得到了证实。与传统的单热表示相比，分布式表示的维数更少，从而降低了计算复杂度。此外，分布式表示可以通过低维嵌入计算出的一定距离来明确地显示实体之间的相似性，而单热表示中的所有嵌入都是正交的，这使得很难区分实体之间的潜在关系。

基于上述优点，知识图表示学习在知识应用中蓬勃发展，显著提高了kg在知识完成、知识融合和推理等任务上的能力。它被认为是知识构建、知识图和知识驱动应用之间的桥梁。到目前为止，已经提出了大量使用分布式表示建模知识图的方法，学习知识表示广泛应用于问题回答、信息检索和对话系统等各种信息驱动任务中。

综上所述，知识图表示学习（KRL）的目标是构建实体和关系的分布式知识表示，将知识投影到低维语义向量空间中。近年来，知识图表示学习取得了重大进步，大量的KRL方法提出构建知识表示，其中基于翻译的方法在许多公斤任务中取得了最先进的性能，在有效性和效率上都有正确的平衡。

在本节中，我们将首先描述我们将在KRL中使用的符号。然后，我们将介绍TransE，这是基于翻译的方法的基本版本。接下来，我们将详细探讨TransE的各种扩展方法。最后，我们将简要介绍一下在知识图建模中使用的其他表示学习方法。

#### 7.2.1 Notations

First, we introduce the general notations used in the rest of this section. We use
$G=(E,R,T)$to denote the whole KG, in which $E=\{e_{1},e_{2},\ldots,e$ 1Ei} stands for the entity set, R = {r1, T2,...,r|R|} stands for the relation set, and ${\boldsymbol{T}}$ stands for the triple set.$\left|E\right|$and |R| are the corresponding entity and relation numbers in their overall sets. As stated above, we represent knowledge in the form of triple fact
(h, r,t), where $h\in E$ means the head entity, t ∈ E means the tail entity, and $r\in R$means the relation between h and t

#### 7.2.2 TransE

TransE [7]是一个基于翻译的模型，用于学习实体和关系的低维嵌入。它将实体和关系投射到相同的语义嵌入空间中，然后将关系视为嵌入空间中的平移。首先，我们将从该方法的动机开始，然后讨论在TransE下如何训练知识表示的细节。最后，我们将探讨TransE的优缺点，以便更深入地了解。

#####7.2.2.1 Motivation

基于翻译的知识图表示学习方法背后主要有三个动机。其主要动机是，将实体之间的关系视为转换操作是很自然的。通过分布式表示，实体被投影到一个低维的向量空间中。直观上，我们同意一个合理的投影应该将具有相似语义含义的实体映射到同一字段。

TransE的次要动机源于Word2vec [49]在单词表示方面的突破。Word2vec提出了两个简单的模型，Skip-gram和CBOW，从大规模语料库中学习单词嵌入，显著提高了单词相似性和类比的性能。Word2vec学习到的单词嵌入有一些有趣的现象：如果两个词对共享相同的语义或语法关系，那么它们在每个单词对中的减法嵌入将是相似的。例如，我们有

**w***(*king*)* − **w***(**man**)* ≈ **w***(*queen*)* − **w***(*woman*),*

这表明王与人之间的潜在语义关系，与王后与女人之间的关系相似，被成功地嵌入到单词表征中。这种近似关系不仅可以与语义关系有关，也可以与句法关系有关。我们有

**w***(*bigger*)* − **w***(*big*)* ≈ **w***(*smaller*)* − **w***(*small*)*

在单词表示中发现的现象强烈暗示，可能存在一种显式的方法来将实体之间的关系表示为向量空间中的翻译操作。

最后一个动机来自于对计算复杂性的考虑。一方面，模型复杂性的大幅增加将导致高计算成本和模糊的模型可解释性。此外，一个复杂的模型可能会导致过拟合。另一方面，关于模型复杂性的实验结果表明，在大多数KG应用程序中，在存在大量的多关系数据集和相对较多的关系的条件下，更简单的模型几乎与表现性更强的模型一样好。随着KG规模的增加，计算复杂度成为知识图表示的主要挑战。翻译的直观假设导致了在准确性和效率之间更好的权衡。

##### 7.2.2.2 Methodology

如图7.3所示，TransE将实体和关系投射到相同的低维空间中。所有的嵌入都在Rd中取值，其中d是一个表示嵌入的维数的超参数。根据平移假设，对于T中的每三个h，r，t，我们希望嵌入的求和+是尾部嵌入+的最近邻。然后将TransE的评分函数定义如下：

![image-20230809164102596](E:/typora/picture/刘志远文档/image-20230809164102596.png)

更具体地说，为了学习这种实体和关系的嵌入，TransE形式化了一个基于边际的损失函数，以负抽样作为训练的目标。成对函数的定义如下：

$\mathscr{L}=\sum_{\langle h,r,t\rangle\in T}\sum_{\langle h^{\prime},r^{\prime},t^{\prime}\rangle\in T^{-}}\max(\gamma+\mathscr{E}(h,r,t))-\mathscr{E}(h^{\prime},r^{\prime},t^{\prime}),0),$

其中E（h，r，t）是正三倍（即T中的三倍）的能量函数的分数，而E（h，r，t）是负三重的能量函数的分数。能量函数E可以用L1或L2的距离来测量。γ > 0是边际的超参数，γ越大，表示正分数和相应的负分数之间的差距越大。T−是关于T的负三重集。

由于在知识图中没有显式的负三元组，所以我们定义T−如下

$T^-=\{\langle h^{\prime},r,t\rangle|h^{\prime}\in E\}\cup\{\langle h,r^{\prime},t\rangle|r^{\prime}\in R\}\cup\{\langle h,r,t^{\prime}\rangle|t^{\prime}\in E\},\quad\langle h,r,t\rangle\in T,$

这意味着负三重集T−由正三重h、r、t组成，头实体、关系或尾实体随机被KG中的任何其他实体或关系所取代。请注意，替换后生成的新三倍体如果已经在T中存在，则不会被视为阴性样本。

TransE采用小批随机梯度下降（SGD）进行优化，并对实体和关系进行随机初始化。知识完成是一个链接预测任务，目的是预测与给定的其余两个元素中的第三个元素（可以是实体或关系），旨在评估学习到的知识表示。

##### 7.2.2.3  Disadvantages and Challenges

TransE是有效的，在链路预测方面的能力。然而，它仍有一些缺点和挑战有待进一步探索。

这些书在作者身上共享相同的信息，但在许多其他领域也有所不同，如主题、背景和书中的著名角色。然而，在TransE中的翻译假设下，每个实体在所有三元组中只有一个嵌入，这大大限制了TransE在知识图表示中的能力。在[7]中，作者根据他们的头和尾论点的基数，将所有的关系分为四类，1对1、1对多、多对1、多对多。如果大多数头有一条尾巴，则为1对1，如果一个头有多尾，则为多，如果一个尾巴有多头，则为多对1，如果有多个头，则为多对多。统计数据表明，1对多、多对1、多对多的关系占很大比例。TransE在1比1方面表现良好，但在处理一对多、多对1、多对多的关系时却存在问题。同样，TransE也可能难以处理自身关系。

其次，翻译操作直观、有效，只考虑简单的一步翻译，这可能会限制对kg建模的能力。以实体为节点，以关系为边，我们可以构造一个具有三重事实的巨大知识图。然而，TransE关注的是最小化能量函数E（h，r，t）=h+r−t，，它只利用知识图中的一步关系信息，而不考虑位于长距离路径中的潜在关系。例如，如果我们知道禁城，位于，北京和北京，首都，中国，，我们可以推断禁城位于中国。TransE可以进一步增强。

第三，为了考虑效率，TransE中的表示和不同函数过于简单。因此，TransE可能无法在知识图中建模那些复杂的实体和关系。对于如何平衡有效性和效率，避免过拟合和过拟合仍然存在挑战。

除了上述的缺点和挑战外，文本信息和层次类型/标签信息等多源信息也具有重要意义，我们将在下面进一步讨论。

#### 7.2.3 Extensions of TransE

在TransE之后有很多扩展方法来解决上述挑战。具体来说，解决建模多、多、多对多关系和多对多关系的挑战，提出编码多步路径的长距离信息位于多步路径，CTransR，TransA，传输和KG2E进一步扩展传输的过度简化模型。我们将详细讨论这些扩展方法。

**TransH**、**TransR/CTransR**、**KG2E**

#### 7.2.4 Other Models

基于翻译的方法，如TransE，简单而有效，其能力在知识图补全和三重分类等各种任务上得到了一致的验证，实现了最先进的性能。然而，也有其他一些表示学习方法在知识图表示上表现良好。在这部分中，我们将简要介绍一下这些方法作为灵感。

##### 7.2.4.1 Structured Embeddings

结构化嵌入（SE）[8]是一种经典的[8]表示学习方法。在SE中，每个实体都被投影到一个d维的向量空间中。SE为每个关系r设计两个关系特定矩阵Mr，1，Mr，2∈Rd×d，在计算相似性时，用这些关系特定矩阵投影头和尾实体。SE的评分函数定义如下

$\mathscr{E}(h,r,t)=\|\mathbf{M}_{r,1}\mathbf{h}-\mathbf{M}_{r,2}\mathbf{t}\|_1,$

其中h和t都与这些投影矩阵转换为关系特定的向量空间。SE的假设是，根据损失函数，投影的头部和尾部嵌入应该尽可能相似。与基于翻译的方法不同，SE将实体建模为嵌入和关系作为投影矩阵。在训练中，SE考虑了训练集中的所有三元组，并最小化了总体损失函数。

##### 7.2.4.2 Semantic Matching Energy

语义匹配能量（SME）[5,6]提出了一种更为复杂的表示学习方法。与SE不同的是，SME认为实体和关系都是低维向量。对于三重h，r，t，，和，用投影函数g组合得到一个新的嵌入lh，r，用t和，得到lt，r。接下来，对两个组合嵌入的lh，r和lt，r使用一个点向乘法函数来得到这个三重组合的分数。SME在第二步中提出了两种不同的投影函数，其中的线性形式为:

$\mathscr{E}(h,r,t)=(\mathbf{M}_1\mathbf{h}+\mathbf{M}_2\mathbf{r}+\mathbf{b}_1)^\top(\mathbf{M}_3\mathbf{t}+\mathbf{M}_4\mathbf{r}+\mathbf{b}_2),$

且双线性形式为：

$\mathscr{E}(h,r,t)=((\mathbf{M}_1\mathbf{h}\odot\mathbf{M}_2\mathbf{r})+\mathbf{b}_1)^\top((\mathbf{M}_3\mathbf{t}\odot\mathbf{M}_4\mathbf{r})+\mathbf{b}_2),$

其中，是元素级（阿达玛）乘积。m1、m2、m3、m4是投影函数中的权值矩阵，b1和b2是偏差。Bordes等人。[6]是基于SME的，它用三路张量代替矩阵改进了双线性形式。

##### 7.2.4.3  Latent Factor Model

提出了一种用于大型多关系数据集建模的潜在因素模型（LFM）。LFM基于双线性结构，它将实体建模为嵌入，将关系建模为矩阵。它可以在不同关系之间共享稀疏潜在因素，显著降低模型和计算复杂度。LFM的评分函数定义如下:

$\mathscr{E}(h,r,t)=\mathbf{h}^\top\mathbf{M}_r\mathbf{t},$

其中，先生是关系r的代表。此外，[92]提出了距离模型，将Mr限制为对角矩阵。该增强后的模型不仅减少了LFM的参数数，从而降低了模型的计算复杂度，而且还获得了更好的性能。

##### 7.2.4.4 RESCAL

![image-20230811163314915](E:/typora/picture/刘志远文档/image-20230811163314915.png)

为了捕获所有三元组的固有结构，提出了一个名为RESCAL的张量分解模型。假设−→X={X1，...，Xk}，对于每个切片Xn，我们有以下的秩-r因式分解：

$\mathbf{X}_n\approx\mathbf{A}\mathbf{R}_n\mathbf{A}^\top,$

其中，A∈Rd×r表示r维实体表示，Rn∈Rr×r表示第n个关系的r个潜在分量的相互作用。这个因子分解中的假设类似于LFM，而RESCAL也优化了不存在的三元组，其中−→Xijm=0，而不是只考虑正实例。

根据这个张量分解假设，RESCAL的损失函数定义如下：

$\mathscr{L}=\frac{1}{2}\left(\sum_n\|\mathbf{X}_n-\mathbf{A}\mathbf{R}_n\mathbf{A}^{\top}\|_F^2\right)+\frac{1}{2}\lambda\left(\|\mathbf{A}\|_F^2+\sum_n\|\mathbf{R}_n\|_F^2\right),$

其中第二项是一个正则化项，λ是一个超参数。

##### 7.2.4.5 HOLE

RESCAL可以很好地处理多关系数据，但计算复杂度很高。为了利用有效性和效率，全息嵌入（HOLE）被提出作为RESCAL [53]的增强版本。

HOLE使用一种名为循环相关的操作来生成组合表示，这类似于那些联想记忆的全息模型。循环相关操作：两个实体h和t之间的Rd×Rd→Rd如下

$\mathbf{h}\star\mathbf{t}t_k=\sum_{i=0}^{d-1}h_it_{(k+i)mod~d}.$

图7.10a还演示了此操作的一个简单实例。一个三重h，r，t的概率被定义为

![image-20230811164308578](E:/typora/picture/刘志远文档/image-20230811164308578.png)

![image-20230811164430836](E:/typora/picture/刘志远文档/image-20230811164430836.png)

$P(\phi_r(h,t)=1)=\text{Sigmoid}(\mathbf{r}^\top(\mathbf{h}\star\mathbf{t})).$

考虑到循环相关带来了很多优点： (1)与乘法或卷积等其他操作不同，循环相关是不可交换的（即h t = t h），它能够在知识图中建模非对称关系。(2)与RESCAL中的张量积相比，循环相关的计算复杂度较低。此外，在快速傅里叶变换（FFT）的帮助下，循环相关性可以进一步加速.

### 7.3 Multisource Knowledge Graph Representation

我们生活在一个复杂的多元现实世界中，在这个世界中，我们不仅可以通过所有感官获取信息，从结构化的知识图，还可以从纯文本、类别、图像和视频中学习知识。这种跨模态信息被认为是多源信息。除了在以前的KRL方法中被广泛使用的结构化知识图外，我们还将介绍其他一些利用多源信息的KRL方法：

1.纯文本是我们每天传递、接收和分析的最常见的信息之一。我们还有大量的纯文本有待检测，其中包含了结构化知识图可能不包含位置的重要知识。实体描述是一种特殊的文本信息，它在几句句子或一个短段落内描述相应的实体。通常，实体描述是由一些知识图（即自由库）来维护的，或者可以从像维基百科这样的大型数据库中自动提取出来。

2.实体类型是构建知识表示的另一个重要的结构化信息。为了在我们先前的知识系统中学习新的对象，人类倾向于将这些对象系统化为现有的类别。实体类型通常用层次结构表示，它由不同的实体子类型的粒度组成。在现实世界中，实体通常有多种实体类型。大多数现有的著名知识图都有自己定制的实体类型的层次结构。

3.图像提供了直观的视觉信息来描述实体的样子，这被确认为是我们每天接收和处理的最重要的信息。位于图像中的潜在信息有很大帮助，特别是在处理具体实体时。例如，我们可以找出潜在的关系在樱桃和梅子之间（有两种植物都属于蔷薇科）从他们的外观。图像可以从网站上下载，还有大量的图像数据集，比如ImageNet。

多源信息学习提供了一种新的方法，不仅从结构化知识图的内部信息中学习知识表示，而且从纯文本、层次类型和图像的外部信息中学习知识表示。此外，对多源信息学习的探索有助于进一步理解人类在现实世界中的所有感官的认知。基于知识图学习到的跨模态表示也将提供不同类型信息之间的可能的关系。

#### 7.3.1 Knowledge Graph Representation with Texts

文本信息是当今最常见和最广泛使用的信息之一。网上每天都有大量的纯文本生成，而且很容易被提取出来。词语是我们思想的压缩符号，可以提供实体之间的联系，这在KRL中具有重要的意义。

#####7.3.1.1 Knowledge Graph and Text Joint Embedding

Wang et al. [76] attempt to utilize textual information by jointly embedding entities,relations, and words into the same low-dimensional continuous vector space. Their joint model contains three parts, namely, the knowledge model, the text model, and the alignment model. More specifically, the knowledge model is learned based on the triple facts in KGs by translation-based models, while the text model is learned based on the concurrences of words in the large corpus by Skip-gram. As for the alignment model, two methods are proposed utilizing Wikipedia anchors and entity names. The main idea of alignment by Wikipedia anchors is replacing the word-word pair $(w,\nu)$with the word-entity pair （w，e,) according to the anchors in Wiki pages, while the main idea of alignment by entity names is replacing the entities in original triple
$\langle h,r,t\rangle $with the corresponding entity names $\langle w_{h},r,t\rangle,\langle h,r,w_{t}\rangle,$ and （Wh,T, w:〉-

##### 7.3.1.2 Description-Embodied Knowledge Graph Representation

Another way of utilizing textual information is directly constructing knowledge rep resentations from entity descriptions instead of merely considering the alignments Xie et al. [82] proposes Description-embodied Knowledge Graph Representation Learning (DKRL) that provides two kinds of knowledge representations: the first is the structure-based representation hs and ts, which can directly represent entities widely used in previous methods, and the second is the description-based represen tation hp and tp which derives from entity descriptions. The energy function derives from translation-based framework:

$\mathscr{E}(h,r,t)=\|\mathbf{h}_S+\mathbf{r}-\mathbf{t}_S\|+\|\mathbf{h}_S+\mathbf{r}-\mathbf{t}_D\|+\|\mathbf{h}_D+\mathbf{r}-\mathbf{t}_S\|+\|\mathbf{h}_D+\mathbf{r}-\mathbf{t}_D\|.$

基于描述的表示是通过CBOW或CNN编码器构建的，这些编码器将纯文本中丰富的文本信息编码为知识表示。DKRL的体系结构如图7.12所示。

与传统的基于翻译的方法相比，DKRL中的两种实体表示同时使用结构信息和文本信息进行构建，从而在知识图补全和类型分类方面获得更好的性能。此外，DKRL可以代表一个实体，即使它不在训练集中，只要有几句句子来描述这个实体。随着他们每天数以百万计的新实体的出现，DKRL就能够处理零射击学习。

#### 7.3.2 Knowledge Graph Representation with Types

实体类型作为实体的一种类别信息，通常以层次结构进行排列，可以提供结构化信息，从而更好地理解KRL中的实体。

##### 7.3.2.1 Type-Constraint Knowledge Graph Representation

Krompaß等人[36]将类型信息作为类型约束，并通过类型约束改进了现有的RESCAL和TransE等方法。很直观的是，在一个特定的关系中，头部或尾部的实体应该属于一些特定的类型。例如，关系wroite_books的主体实体应该是人（或者更准确地说是作者），而尾部实体应该是一本书。

Specifically, in RESCAL,the original factorization X, ~ AR,A is modified to

$$
\mathbf{X}^{\prime}_r\approx\mathbf{A}_{[head_r,:]}\mathbf{R}_r\mathbf{A}_{[tail_r,:]}^{\top},
$$

(7.51)

in which head,,tail, are the set of entities fitting the type constraints of head or tail and $\mathbf{X}_{r}^{\prime}$ is a sparse adjacency matrix of shape |heady|x |tail-|. In the enhanced ver-sion, only the entities that fit type constraints will be considered during factorization

In TransE, type constraints are utilized in negative sampling. The margin-based

score functions of translation-based methods need negative instances,which are generated through randomly replacing head or tail entities with another entity in triples. With type constraints, the negative samples are chosen by

$$
h^{\prime}\in E_{[head_{r}]}\subseteq E\:,\quad t^{\prime}\in E_{[tail_{r}]}\subseteq E\:,
$$

(7.52)

where $E_{[head_{r}]}$ is the subset of entities following type constraints for head in relation r, and $E_{[t\:ail_{r}]}$ is that for tail.

##### 7.3.2.2 Type-Embodied Knowledge Graph Representation

将类型信息作为约束条件来考虑是简单而有效的，但性能仍然有限。Xie等人[83]不仅仅将类型信息视为类型约束，而是提出了类型体现的知识图表示学习（TKRL），利用层次类型结构来指导投影矩阵的构造。受TransR的启发，每个实体在不同的场景中都应该有多个表示，TKRL的能量函数定义如下

$\mathscr{E}(h,r,t)=\|\mathbf{M}_{rh}\mathbf{h}+\mathbf{r}-\mathbf{M}_{rt}\mathbf{t}\|,$

![image-20230811171609830](E:/typora/picture/刘志远文档/image-20230811171609830.png)

where $\mathbf{M}_{C^{(i)}}$ stands for the projection matrix of the ith subtype of the hierarchical type c,$\boldsymbol{\beta}_{i}$is the corresponding weight of the subtype. Figure7.13 demonstrates a simple illustration of TKRL. Taking RHE, for instance, given an entity William Shakespeare, it is first projected to a rather general sub-type space like human and then sequentially projected to a more precise subtype like author or English author. Moreover, TKRL also proposes an enhanced soft-type constraint to alle-viate the problems caused by type information incompleteness.

#### 7.3.3  Knowledge Graph Representation with Images

图像可以提供相应实体前景的直观的视觉信息，从而从某些方面提供暗示实体的某些潜在属性的重要提示。例如，图7.14展示了装甲和装备的实体图像。左边显示了装甲装备，Armet，，令人惊讶的是，我们可以直接从图像中推断出这些知识。

#####7.3.3.1 Image-Embodied Knowledge Graph Representation

Xie等人[81]提出了图像体现知识图表示学习（IKRL），在构建知识表示时考虑了视觉信息。受[82]中多实体表示的启发，IKRL除了提出基于结构的表示外，还提出了基于图像的表示hI和tI，并在基于翻译的框架内同时学习两种类型的实体表示。

$\mathscr{E}(h,r,t)=\|\mathbf{h}_S+\mathbf{r}-\mathbf{t}_S\|+\|\mathbf{h}_S+\mathbf{r}-\mathbf{t}_I\|+\|\mathbf{h}_I+\mathbf{r}-\mathbf{t}_S\|+\|\mathbf{h}_I+\mathbf{r}-\mathbf{t}_I\|.$

更具体地说，IKRL首先用神经网络构造所有实体图像的图像表示，然后通过投影矩阵将这些图像表示从图像空间投影到实体空间。由于大多数实体可能具有多个不同质量的图像，IKRL通过基于注意力的方法选择信息更丰富、更有区别的图像。IKRL的评价结果不仅证实了视觉信息对理解的意义

#### 7.3.4  Knowledge Graph Representation with Logic Rules

Typical knowledge graphs store knowledge in the form oftriple facts with one relation linking twoentities. Mostexisting KRL methods only consider the information within triple facts separately, ignoring the possible interactions and correlations between dif-ferent triples. Logic rules, which are certain kinds of summaries deriving from human beings' prior knowledge, could help us with knowledge inference and reasoning. For instance, if we know the triple fact that〈Beijing,iS_capital_of, China〉we can easily infer with high confidence that 〈Beiing, 1ocated_in, China)since we know the logic rule that the relation iS_Capital_of三1ocated in

Some works are focusing on introducing logic rules to knowledge acquisition and

inference, among which Markov Logic Networks are intuitively utilized to address this challenge [3,58,751]. The path-based TransE「38] stated above also implicitly considers the latent logic rules between different relations via relation paths

##### 7.3.4.1  KALE

KALE是一种基于翻译的KRL方法，它与逻辑规则[24]联合学习知识表示。联合学习由三重建模和规则建模两部分组成。对于三重建模，KALE遵循翻译假设，对评分函数的微小改变如下

$\mathscr{E}(h,r,t)=1-\frac{1}{3\sqrt{d}}\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|,$

其中，d表示知识嵌入的维度。E（h，r，t）在[0,1]中取值，便于联合学习。

对于新添加的规则建模，KALE使用了[25]中提出的t-范数模糊逻辑，该逻辑表示一个复杂公式的真值和其组成部分的真值。特别是，KALE关注两种典型的逻辑规则。第一个是∀，t：h，r1，t⇒，r2，t（例如，给定北京，是中国，的资本，我们可以推断北京，位于，中国）。KALE通过特定的基于t范数的逻辑连接词表示该逻辑规则f1的评分函数，如下：

$\mathscr{E}(f_1)=\mathscr{E}(h,r_1,t)\mathscr{E}(h,r_2,t)-\mathscr{E}(h,r_1,t)+1.$

第二个是∀，e，t：h，r1，e∧，r2，t⇒，r3，t（例如，鉴于清华位于北京）和北京位于中国，，我们可以推断清华位于中国)。而KALE将第二个评分功能定义为

$\mathscr{E}(f_2)=\mathscr{E}(h,r_1,e)\mathscr{E}(e,r_2,t)\mathscr{E}(h,r_3,t)-\mathscr{E}(h,r_1,e)\mathscr{E}(e,r_2,t)+1.$

联合训练包含了所有的正公式，包括三重事实和逻辑规则。请注意，为了考虑逻辑规则的质量，KALE使用预先训练好的TransE根据其真值对所有可能的逻辑规则进行排序，并手动过滤出排在最前面的一些规则。

### 7.4 Applications

近年来，知识驱动的人工智能，如质量保证系统和聊天机器人的蓬勃发展。人工智能代理能够准确、深入地了解用户需求，然后适当、灵活地给出响应和解决方案。如果没有某种形式的知识，这种工作是无法完成的。

为了将知识引入人工智能代理，研究人员首先从纯文本、图像和结构化知识库等异构信息中提取知识。然后，将这些各种异构信息与知识图等某些结构融合并存储。接下来，根据一些KRL方法将知识投影到低维语义空间。最后，这些学习到的知识表示被应用于信息检索和对话系统等各种知识应用中。图7.16演示了一个从头开始的知识驱动应用程序的简短管道

![image-20230811175656000](E:/typora/picture/刘志远文档/image-20230811175656000.png)

从图中，我们可以观察到，知识图表示学习是整个知识驱动应用程序管道中的关键组成部分。它弥合了存储知识和知识的知识图之间的差距使用知识的应用程序。与符号方法相比，分布式方法的知识表示能够解决数据的稀疏性，并对实体和关系之间的相似性进行建模。此外，基于嵌入的方法易于与深度学习方法一起使用，并且自然适合与异构信息的结合。

在本节中，我们将主要从两个方面介绍知识表示的可能应用。首先，我们将介绍知识表示在知识驱动的应用程序中的使用，然后我们将展示知识表示在知识提取和构建方面的能力。

#### 7.4.1  Knowledge Graph Completion

知识图完成的目的是通过从纯文本、现有知识库和图像等异构源中提取知识来构建结构化的知识库。知识构建由关系提取和信息提取等几个子任务组成，是整个知识驱动框架的基本步骤。近年来，自动知识建设由于处理大量的现有和新信息非常耗时和劳动密集型而引起了人们的广泛关注。在下一节中，我们将介绍一些关于神经关系提取的探索，并集中讨论知识表示的组合。

##### 7.4.1.1 Knowledge Representations for Relation Extraction

关系提取侧重于预测两个实体之间的正确关系，给定一个包含两个实体的简短纯文本。一般来说，所有要预测的关系都是预定义的，这与开放信息提取不同。实体通常用命名实体识别系统标记，或根据锚定文本提取，或通过距离监督[50]自动生成。

传统的关系提取和分类方法主要基于统计机器学习，这很大程度上依赖于提取特征的质量。Zeng等人[96]首先将CNN引入关系分类，取得了很大的改进。Lin等人[40]进一步改进了基于注意模型的神经关系提取模型。

Han等人[27,28]提出了一种新的知识获取的联合表示学习框架。其关键思想是，联合模型通过kg-文本对齐，在一个统一的语义空间中学习知识和文本表示。图7.17显示了KG-text联合模型的简要框架。在文本部分，将具有Mark Twain和佛罗里达两个实体的句子作为CNN编码器的输入，CNN的输出被认为是该句子诞生的潜在关系位置。而对于KG部分，实体和关系表示是通过基于翻译的方法来学习的。学习到的KG和文本部分的表示在训练过程中被对齐。这项工作是第一次尝试将知识表示从现有的知识表示编码到知识构建任务，并在知识完成和关系提取方面实现改进。

![image-20230811181623793](E:/typora/picture/刘志远文档/image-20230811181623793.png)

![image-20230811181806171](E:/typora/picture/刘志远文档/image-20230811181806171.png)

#### 7.4.2 Knowledge-Guided Entity Typing

实体类型是检测纯文本中已命名实体（或实体提及）的语义类型的任务。例如，给定乔丹在NBA打了15个赛季，实体类型的目的是推断乔丹在这个句子中的是一个人，一个运动员，甚至是一个篮球运动员。实体类型对于命名实体消除歧义很重要，因为它可以缩小提到[10]的实体的候选范围。此外，实体类型还有利于大量的自然语言处理（NLP）任务，如关系提取[46]、问题回答[90]和知识库总体[9]。

传统的命名实体识别模型[69,73]通常将实体提及分类为一组粗糙的标签（例如，人、组织、位置和其他）。由于这些实体类型对于许多NLP任务来说过于粗粒度，因此[15,41,94,95]已经提出了一些工作来引入更大的细粒度类型集，这些类型通常是这些粗粒度类型的子类型。以前的细粒度实体类型方法通常使用NOS标记和解析等NLP工具派生特征，不可避免地会遭受错误传播的影响。Dong等人[18]首次尝试探索实体类型中的深度学习。该方法仅采用词向量作为特征，抛弃了复杂的特征工程。岛冈等人[63]进一步将注意力方案引入到细粒度实体类型的神经模型中。

神经模型在细粒度实体类型方面已经取得了最先进的性能。然而，这些方法面临着以下重要的挑战：

(1)实体上下文分离。现有的方法通常编码上下文词，而不利用实体和上下文之间的关键相关性。然而，直观地是，单词在上下文中的重要性对实体类型事物会受到我们所关心的实体提及的显著影响。例如，在1975年的一句话中，盖茨和保罗·艾伦共同创立了微软，微软成为了世界上最大的个人电脑软件公司，公司这个词在决定微软的类型时要比盖茨的类型重要得多。

(2)实体知识分离。现有的方法只考虑实体类型时实体提及的文本信息。事实上，知识图（KGs）为确定实体类型提供了丰富而有效的附加信息。例如，在1975年的句子中，盖茨。微软……即使我们没有KG中的微软的类型信息，类似于微软的实体（如IBM）也将提供补充信息。

为了解决实体-上下文分离和实体-知识分离的问题，我们提出了知识引导注意（KNET）神经实体分型问题。如图7.18所示，KNET主要由两部分组成。首先，KNET构建一个神经网络，包括一个长短期记忆（LSTM）和一个全连接层，以生成上下文和命名实体表示。其次，KNET引入知识关注，强调这些关键词，提高上下文表征的质量。这里我们详细介绍了对知识的关注。

知识图以三元组h、r、t，的形式提供了关于实体的丰富信息，其中h和t是实体，r是它们之间的关系。许多KRL的工作都致力于基于kg中的三重信息将实体和关系编码到实值语义向量空间中。KRL为我们提供了一种利用KG信息进行实体类型化的有效方法。

KNET采用最广泛使用的KRL方法TransE为每个实体e的实体嵌入e。在训练场景中，已知实体提及m表示嵌入e的KGs中相应的e，因此，KNET可以直接计算知识注意力如下

$\alpha_i^\mathrm{KA}=f\left(\mathbf{eW}_\mathrm{KA}\left[\begin{array}{c}\overrightarrow{\mathbf{h}_i}\\\mathbf{h}_i\end{array}\right]\right),$

其中，WKA为双线性参数矩阵，ai KA为第i个单词的注意权重。

测试中的知识关注。挑战在于，在测试场景中，我们不知道某个实体所提到的KG中相应的实体。一个解决方案是执行实体链接，但它会引入链接错误。此外，在许多情况下，kg可能不包含许多实体提及的相应实体。

为了解决这个挑战，我们在培训期间为kg中的实体构建了一个额外的基于文本的表示。具体地说，对于一个实体e及其上下文句子s，我们使用单向LSTM将其左右上下文编码为cl和cr，并进一步学习基于文本的表示ˆe如下：

$\hat{\mathbf{e}}=\tanh\left(\mathbf{W}\left[{\begin{array}{c}\mathbf{m}\\\mathbf{c}_l\\\mathbf{c}_r\end{array}}\right]\right),$

其中，W为参数矩阵，m为提及表示。请注意，这里使用的LSTM与上下文表示中的不同，以防止干扰。为了连接基于文本的表示和基于kg的表示，在训练场景中，我们通过在目标函数中添加一个额外的组件来同时学习ˆe：

$\mathscr{O}_\mathrm{KG}(\theta)=-\sum_e\|\mathbf{e}-\hat{\mathbf{e}}\|^2.$

这样，在测试场景中，我们就可以直接使用等式了7.61使用等式获得相应的实体表示并计算知识关注7.60.

#### 7.4.3 Knowledge-Guided Information Retrieval

大规模知识图的出现推动了面向实体的搜索技术的发展，它利用知识图来改进搜索引擎。面向实体的搜索的最新进展包括使用实体注释[61,85]进行更好的文本表示，更丰富的排名特征[14]，查询和文档[45,84]之间基于实体的连接，以及通过知识图关系或嵌入[19,88]进行的软匹配查询和文档。这些方法从知识图中引入了实体和语义，并大大提高了基于特征的搜索系统的有效性。

信息检索的另一个前沿领域是神经排序模型（神经ir）的发展。深度学习技术已被用于学习查询和文档的分布式表示，这些表示捕获它们的相关性关系（基于表示的）[62]，或者直接从它们的单词级交互（基于交互的）[13,23,87]中建模查询-文档的相关性。神经红外方法，特别是基于交互的方法，当有大规模训练数据时，大大提高了排名精度。

面向实体的搜索和神经红外从两个不同的方面推动了搜索引擎的边界。面向实体的搜索结合了来自实体的人类知识和知识图语义。它在基于特征的排名系统上显示出了良好的结果。另一方面，神经红外利用分布式表示和神经网络来学习更复杂的排序模型，形成大规模的训练数据。实体-二重奏神经排序模型（EDRM），如图7.19所示，在基于交互的神经排序模型中加入了实体。EDRM首先使用知识图中的语义来学习实体的分布式表示：描述和类型。然后，它遵循最近最先进的面向实体的搜索框架，即单词-实体二重唱[86]，并使用单词袋和实体袋将文档与查询进行匹配。而不是手动功能，EDRM使用基于交互的神经模型[13]来将查询和文档与单词-实体二重唱表示相匹配。因此，EDRM结合了面向实体的搜索和基于交互的神经红外；它将知识图语义引入神经红外，并用神经网络增强了面向实体的搜索。

![image-20230813094823465](E:/typora/picture/刘志远文档/image-20230813094823465.png)

##### 7.4.3.1 Interaction-Based Ranking Models

给定一个查询q和一个文档d，基于交互的模型首先建立q和d之间的字级转换矩阵。翻译矩阵使用单词相关性来描述单词对相似性，通过基于交互的模型中的单词嵌入相似性来捕获。

通常，基于交互的排序模型首先将q和d中的每个单词w映射到一个L维嵌入$v_w$

$\mathbf{v}_w=\mathrm{Emb}_w(w).$

然后基于查询和文档嵌入构造交互矩阵M。矩阵中的每个元素Mi j，比较q中的第i个单词和d中的第j个单词，例如，使用单词嵌入的余弦相似性

$\mathbf{M}_{ij}=\cos(\mathbf{v}_{w_{i}^{q}},\mathbf{v}_{w_{j}^{d}}).$

通过描述查询和文档之间的术语级别匹配的翻译矩阵，下一步是从该矩阵中计算出最终的排名得分。在基于交互的神经排名模型中已经开发了许多方法，但一般来说，将在M上包括一个特征提取器，然后是一个或几个排名层，将特征结合到排名分数中。

##### 7.4.3.2 Semantic Entity Representation

EDRM将知识图中关于一个实体的语义信息合并到其表示中。该表示包括三种嵌入：实体嵌入、描述嵌入和类型嵌入，均在L维中，并合并生成实体的语义表示。

**实体嵌入**使用一个L维嵌入层嵌入来得到e的实体嵌入e:$\mathbf{v}_e=\mathrm{Emb}_e(e).$

**描述嵌入**编码一个包含m个单词的实体描述，并解释该实体。EDRM首先使用单词嵌入层Embv将描述词v嵌入到v中。然后它将文本中的所有嵌入组合到嵌入矩阵v中。接下来，它利用卷积滤波器滑动文本，并将l长度n-gram组合为$g_e^j$

$\mathbf{g}_{e}^{j}=\mathrm{ReLU}(\mathbf{W}_{\mathrm{CNN}}\cdot\mathbf{V}_{w}^{j:j+h}+\mathbf{b}_{\mathrm{CNN}}),$

其中，WCNN和bCNN是卷积滤波器的两个参数。

然后在卷积层后使用最大池化来生成描述嵌入的$v_e^{des}$

$\mathbf{v}_e^{des}=\max(\mathbf{g}_e^1,...,\mathbf{g}_e^j,...,\mathbf{g}_e^m).$

![image-20230813100114070](E:/typora/picture/刘志远文档/image-20230813100114070.png)

然后，EDRM利用一种注意机制，将实体类型组合到类型嵌入的$v_e^{type}$中：$\mathbf{v}_e^{type}=\sum_j^n\alpha_j\mathbf{v}_{f_j},$

其中，αj为注意力得分，计算结果为:

$\begin{aligned}\alpha_j&=\frac{\exp(y_j)}{\sum_l^n\exp(y_l)},&&(7.70)\\\\y_j&=\left(\sum_i\mathbf{W}_{bow}\mathbf{v}_{t_i}\right)\cdot\mathbf{v}_{f_j},&&(7.71)\end{aligned}$

其中$y_j$是查询或文档表示和类型嵌入$f_j$的点积。我们利用单词袋来进行查询或文档编码。$W_{bow}$是一个参数矩阵。

组合。这三个嵌入由一个线性层组合起来，以生成实体的语义表示

$\mathbf{v}_e^{sem}=\mathbf{v}_e^{emb}+\mathbf{W}_e[\mathbf{v}_e^{des};\mathbf{v}_e^{type}]^\top+\mathbf{b}_e,$

in which $W_e$ is an *L* × 2*L* matrix and$b_e$is an *L*-dimensional vector.

##### 7.4.3.3 Neural Entity-Duet Framework

词-实体二重唱[86]是最近开发的一个面向实体的搜索框架。它利用单词袋和实体袋的二重唱表示来匹配问题q和文档d与手工制作的特性。这项工作将其引入到神经红外技术中。

他们首先用实体注释构造实体包$q_e$和$d_e$，以及针对q和d的词包$q_w$和$d_w$。二重唱采用了四路交互：查询单词到文档单词（$q_w$-$d_w$）、查询单词到文档实体（$q_w$-$d_e$）、查询实体到文档单词（qe-dw），以及查询实体到文档实体（qe-de）。

EDRM不使用翻译层，而是使用翻译层来计算一对查询文档术语：（vi wq或vi eq）和（v j wd或v j ed）之间的相似性。构造了交互矩阵M = {Mww、Mwe、Mew、Mee}。Mww、Mwe、Mew、Mee分别表示qw-dw、qw-de、dw、de的相互作用。其中的元素是对应项的余弦相似性:

$\begin{aligned}\mathbf{M}_{ww}^{ij}&=\cos(\mathbf{v}_{w^q}^i,\mathbf{v}_{w^d}^j);\mathbf{M}_{ee}^{ij}=\cos(\mathbf{v}_{e^q}^i,\mathbf{v}_{e^d}^j)\\\mathbf{M}_{ew}^{ij}&=\cos(\mathbf{v}_{e^q}^i,\mathbf{v}_{w^d}^j);\mathbf{M}_{we}^{ij}=\cos(\mathbf{v}_{w^q}^i,\mathbf{v}_{e^d}^j).\end{aligned}$

最终的排名功能(M)是四个交叉匹配（φ(M)）的串联体：

$\Phi(\mathbf{M})=[\phi(\mathbf{M}_{ww});\phi(\mathbf{M}_{we});\phi(\mathbf{M}_{ew});\phi(\mathbf{M}_{ee})],$

其中，φ可以是基于交互的神经排名模型中使用的任何函数。

实体-二重唱是一种在实体和单词空间中交叉匹配查询和文档的有效方法。在EDRM中，它将知识图的语义表示引入到神经-红外模型中。

EDRM提供的二重唱翻译矩阵可以插入任何标准的基于交互的神经排序模型，如K-NRM [87]和Conv-KNRM [13]。在足够的训练数据下，对整个模型进行反向传播端到端优化。在此过程中，与排序神经网络共同学习了知识图的语义、实体嵌入、描述嵌入、类型嵌入以及与实体的匹配的集成。

#### 7.4.4 Knowledge-Guided Language Models

知识是语言建模的一个重要的外部信息。这是因为统计上的共现不能指示各种知识的生成，特别是对于那些具有低频率的命名实体。研究人员试图将外部知识纳入语言模型，以更好地在生成和表示方面的表现。

##### 7.4.4.1 NKLM

语言模型旨在学习单词序列上的概率分布，这是一项经典的、必要的自然语言处理任务。近年来，序列到序列神经模型（seq2seq）被广泛发展，并广泛应用于序列生成任务，如机器翻译[68]和图像标题生成[72]。然而，大多数seq2seq模型在建模和使用背景知识时都有显著的局限性。

为了解决这个问题，Ahn等人[1]提出了一种神经知识语言模型（NKLM），该模型在使用RNN语言模型生成自然语言序列时考虑了知识图所提供的知识。关键思想是NKLM有两种生成单词的方法。第一种方法与传统的seq2seq模型相同，即根据softmax的概率生成“词汇词”，第二种方法是根据外部知识图生成“知识词”。

Specifically,the NKLM model takes LSTM as the framework of generating

“vocabulary word’. For external knowledge graph information, NKLM denotes the topic knowledge as Y三 {a1,...a|,x」},in which a; represents the entities (i.e.,named as“topic’in [1]) that appear in the same triple of a certain entity. Ateach step t,NKLM takes both “vocabulary word”$w_{t-1}^{\nu}$and “knowledge word”w;-1 as well as the fact at-1 predicted at step t-l as the inputs of LSTM. Next, the hidden state of LSTM $h_{t}$ is combined with the knowledge context e to get the fact key $k_{t}$ via an MLP module. The knowledge context ek derives from the mean embeddings of all related facts of fact k. The fact key k, is then used to extract the most appropriate fact a, from the corresponding topic knowledge. And finally, the selected fact a, is combined with hidden state h, to predict(1) both “vocabulary word”wy and “knowledge word’ w,and(2) which word to generate at this step. The architecture of NKLM is shown in Fig. 7.20.

NKLM模型探索了一种新的神经模型，它将外部知识图中的符号知识信息与seq2seq语言模型相结合。然而，在生成自然语言时给出了知识的主题，这使得NKLM更不实用，对于更一般的自由演讲也更不可扩展。然而，我们仍然相信用这种方法将知识编码为语言模型是很有希望的。

![image-20230813164143369](E:/typora/picture/刘志远文档/image-20230813164143369.png)

##### 7.4.4.2 ERNIE

像BERT [17]这样的预先训练过的语言模型具有很强的表示来自文本中的语言信息的能力。通过丰富的语言表示，预训练的模型在各种NLP应用程序上获得了最先进的结果。然而，现有的预训练语言模型很少考虑结合外部知识来提供相关的背景信息，以便更好地理解语言。例如，鉴于鲍勃·迪伦在《风与编年史》中写的一句话，不知道《风与编年史》分别是歌曲和书，很难认出鲍勃·迪伦的两个职业，即词曲作者和作家。

为了增强具有外部知识的语言表示模型，Zhang等人[100]提出了一种具有信息实体的增强型语言表示模型（ERNIE）。知识图（KGs）是重要的外部知识资源，他们认为KGs中的信息实体可以作为用知识增强语言表示的桥梁。厄尼考虑克服整合外部知识的两个主要挑战：结构化知识编码和异构信息融合。

为了提取和编码知识信息，ERNIE首先识别文本中的命名实体提及，然后将这些提及与KGs中对应的实体对齐。ERNIE没有直接使用KGs中基于图的事实，而是用TransE [7]等知识嵌入算法对KGs的图结构进行编码，然后将信息实体嵌入作为输入。基于文本和kg之间的对齐，ERNIE将知识模块中的实体表示集成到语义模块的底层层中。

与BERT类似，ERNIE采用了掩蔽语言模型和下一句话预测作为训练前的目标。此外，为了更好地融合文本和知识特征，ERNIE使用了一个新的预训练目标（去噪实体自动编码器），通过在输入文本中随机屏蔽一些已命名的实体对齐，并训练从kg中选择合适的实体来完成对齐。与现有的仅利用局部上下文来预测标记的预训练语言表示模型不同，这些目标需要ERNIE聚合上下文和知识事实来预测标记和实体，并产生一个知识渊博的语言表示模型。

图7.21是整个体系结构。左图显示ERNIE由两个编码器（T编码器和K编码器）组成，其中T编码器由几个经典的变压器层堆叠，K编码器由为知识集成而设计的新的聚合器层堆叠。右边的部分是聚合器层的细节。在聚合器层中，来自前一个聚合器的输入令牌嵌入和实体嵌入分别被输入到两个多头自注意中。然后，聚合器采用信息融合层对令牌和实体序列进行相互集成，并计算每个令牌和实体的输出嵌入。

![image-20230813164614002](E:/typora/picture/刘志远文档/image-20230813164614002.png)

厄尼探讨了如何将知识信息整合到语言表示模型中。实验结果表明，ERNIE具有对远距离监督数据的去噪能力和对有限数据的微调能力。

### 7.5 Summary

在本章中，我们首先介绍了知识图的概念。知识图以三重事实的形式包含了实体和实体之间的关系，为人类学习和理解现实世界提供了一种有效的途径。接下来，我们介绍了知识图表示的动机，它被认为是一种对大量数据有用而方便的方法，在多个知识基任务中被广泛地探索和应用，并显著提高了性能。并描述了现有的知识图表示方法。此外，我们将讨论几种旨在处理知识图表示的当前挑战的高级方法。我们还回顾了知识图表示的现实世界中的应用，如语言建模、问题回答、信息检索和推荐系统。

## 8 Network Representation

网络表示学习的目的是将网络中的顶点嵌入到低维密集表示中，其中网络中相似的顶点应该具有“接近”表示（通常通过其表示的余弦相似度或欧氏距离来衡量）。这些表示法可以作为顶点的特征，并应用于许多网络研究任务。在本章中，我们将介绍过去十年来的网络表示学习算法。然后，我们将讨论它们在应用于各种现实世界的网络时的扩展。最后，我们将介绍一些常见的网络表示学习评估任务和相关数据集。

作为一种表示对象及其关系的自然方式，该网络在我们的日常生活中无处不在。Facebook和推特等社交网络的快速发展鼓励研究人员在网络结构上设计有效的算法。网络信息的正确表示是网络研究的一个关键问题。传统的网络表示通常是高维和稀疏的，这在人们将统计学习应用于网络时成为了一个弱点。随着机器学习的发展，网络中顶点的特征学习正成为一项新兴的任务。因此，网络表示学习算法将网络信息转化为低维密集的实值向量，可以作为现有机器学习算法的输入。例如，顶点的表示可以被提供给一个分类器，如支持向量机（SVM），用于顶点分类任务。此外，通过将这些表示作为欧几里得空间中的点，这些表示也可以用于可视化。在本节中，我们将形式化网络表示学习问题。

Denote a network as $G=(V,E)$ where V is the vertex set and E' is the edge set

An edge $e=(\nu_{i},\nu_{j})\in E$ where $v_{i},v_{j}\in V$ is a directed edge from vertex V; to Vj The outdegree of vertex v; is defined as $\deg_{O}(v_{i})=|\{v_{j}|(v_{i},v_{j})\in E\}|$ . Similarly the indegree of vertex $v_{i}$ is deg, (v;）= |{v;|(Vj,V;）∈ E}|. For undirected network we have deg(v:） = dego(v;)= deg (V;). Taking social network as an example, a vertex represents a user and an edge represents the friendship between two users The indegree and outdegree represent the number of followers and followees of a user, respectively.

因此，人们提出了学习网络中顶点的低维密集表示的想法。形式上，网络表示学习的目标是学习顶点v∈V的实值向量v∈Rd，其中维数d远小于顶点|和|的数量|。其思想是，相似的顶点应该具有如图8.1所示的密切表示。网络表示学习可以是无监督的或半监督的。这些表示在没有特征工程的情况下自动学习，一旦学习，可以进一步用于特定的任务，如分类。这些表示是低维的，这使得有效的算法可以设计在表示上而不考虑网络结构本身。我们将在本章稍后讨论关于网络表示评估的更多细节。

### 8.2 Network Representation

在本节中，我们将详细介绍几种网络表示学习算法。

#### 8.2.1 Spectral Clustering Based Methods

基于谱聚类的方法是一组计算亲和矩阵的前k个特征向量或奇异向量的算法，如网络的邻接矩阵或拉普拉斯矩阵。这些方法在很大程度上依赖于亲和矩阵的构造。不同亲和矩阵的评价结果差异很大。一般来说，基于谱聚类的方法具有较高的复杂度，因为特征向量和奇异向量的计算具有非线性的时间复杂度。另一方面，基于光谱聚类的方法在计算过程中需要在内存中保存一个亲和矩阵。因此，空间的复杂性也不能被忽视。这些缺点限制了这些方法的大规模和在线泛化。现在，我们将提出几种基于光谱聚类的算法。

局部线性嵌入（LLE）[98]假设顶点的表示是从一个流形中采样的。更具体地说，LLE假设一个顶点及其邻居的表示位于流形的局部线性块中。也就是说，一个顶点的表示可以用它的邻居的表示的线性组合来近似。LLE利用邻居的线性组合来重建中心顶点。在形式上，所有顶点的重构误差都可以表示为
$$
\mathscr{L}(\mathbf{W},\mathbf{V})=\sum_{i=1}^{|V|}\left\|\mathbf{v}_i-\sum_{j=1}^{|V|}\mathbf{W}_{ij}\mathbf{v}_j\right\|^2,
$$

(8.1)

where $\mathbf{V}\in\mathbb{R}^{|V|\times d}$ is the vertex embedding matrix and $\mathbf{W}_{ij}$ is the contribution coef-ficient of vertex V；to V;. LLE enforces $\mathbf{W}_{ij}=0$ if $V_{i}$ and vjare not connected,i.e.,$(\nu_{i},\nu_{j})\notin E$. Further, the summation of a row of matrix W is set to 1, i.e.
$$
\sum_{j=1}^{|V|}\mathbf{W}_{ij}=1.
$$
方程8.1通过交替优化权重矩阵W和表示v来求解，对W的优化可以作为一个最小二乘问题来求解。对表示V的优化导致了以下优化问题：

$\mathscr{L}(\mathbf{W},\mathbf{V})=\sum_{i=1}^{|V|}\left\|\mathbf{v}_i-\sum_{j=1}^{|V|}\mathbf{W}_{ij}\mathbf{v}_j\right\|^2,\quad(8.2)$

$s.t.\sum_{i=1}^{|V|}\mathbf{v}_i=\mathbf{0},\quad(8.3)$

$\text{and}|V|^{-1}\sum_{i=1}^{|V|}\mathbf{v}_i^{\top}\mathbf{v}_i=\mathbf{I}_d,$

其中Id表示d×d单位矩阵条件方程式8.3和8.4确保了解决方案的唯一性。第一个条件强制将所有顶点嵌入的中心设为零点，第二个条件保证不同的坐标具有相同的尺度，即对重构误差的贡献相等。

The optimization problem can be formulated as the computation of eigenvectors

of matrix $(\mathbf{I}_{|V|}-\mathbf{W}^{\top})(\mathbf{I}_{|V|}-\mathbf{W})$ , which is an easily solvable eigenvalue problem More details can be found in the note [22]

拉普拉斯特征映射[8]算法简单地遵循两个连通顶点的表示应该接近的思想。具体来说，“接近度”是用欧几里得距离的平方来衡量的。我们用D表示对角度矩阵，其中D是|V|×|V|对角矩阵，对角条目Dii是顶点vi的度。图的拉普拉斯矩阵L定义为对角矩阵D与邻接矩阵a的差值，即L = D−A。

拉普拉斯特征映射算法希望最小化以下代价函数：

$\begin{aligned}
{\mathcal{L}}(\mathbf{V})& =\sum_{\{i,j|(\nu_{i},\nu_{j})\in E\}}\|\mathbf{v}_{i}-\mathbf{v}_{j}\|^{2}, && (8.5)  \\
&{}_{s.t.}\mathbf{V}^{\top}D\mathbf{V}=\mathbf{I}_{d}.&& (8.6) 
\end{aligned}$

代价函数是所有连通顶点对的平方损失的和，该条件防止了任意尺度引起的平凡全零解。公式8.5可用矩阵形式重新表示为

$\mathbf{V}^{*}=\arg\min_{\mathbf{V}^{\top}D\mathbf{V}=\mathbf{I}_{d}}\operatorname{tr}(\mathbf{V}^{\top}L\mathbf{V}).\quad(8.7)$

代数知识告诉我们，等式的最优解V为∗请注意，拉普拉斯特征映射算法可以很容易地推广到加权图中。

LLE和拉普拉斯特征映射都有一个对称的代价函数，这表明这两种算法都不能应用于有向图。提出了有向图嵌入（DGE）[17]来推广拉普拉斯特征映射。

#### 8.2.2 DeepWalk

DeepWalk [93]提出了一种新的方法，首次将深度学习技术引入网络表示学习。建模截断的随机游动而不是邻接矩阵的好处是双重的：首先，随机游动只需要局部信息，从而实现离散和在线算法，而邻接矩阵建模可能需要存储在内存中，从而消耗空间；其次，建模随机游动可以减轻建模原始二进制邻接矩阵的方差和不确定性。我们将在下一小节中深入了解深度行走技术。

无监督表示学习算法在自然语言处理领域得到了广泛的研究和应用。作者表明，短随机游动中的顶点频率也像文献中的单词一样遵循幂律。展示了顶点与单词之间的联系，作者采用了一个著名的单词表示学习算法word2vec[80]到顶点表示学习。现在，我们将详细介绍深度行走的算法。

基于独立假设，损失函数可以重写为 $\min_{\mathbf{v}}\sum_{k=-w,k\neq0}^{w}-\log P(v_{i+k}|\mathbf{v}_{i}).$

总体损失函数可以通过对每个随机游走中的每个顶点进行相加得到。

现在我们讨论如何预测一个单个顶点vj给定的中心顶点vi。在DeepWalk中，每个顶点vi都有两个具有相同维数的表示：顶点表示vi∈Rd和上下文表示ci∈Rd。预测P的概率（v j|vi）由所有顶点上的softmax函数定义：$P(\nu_j|\mathbf{v}_i)=\frac{\exp(\mathbf{v}_i\mathbf{c}_j^\top)}{\sum_{k=1}^{|V|}\exp(\mathbf{v}_i\mathbf{c}_k^\top)}.\quad\quad\quad\quad(8.21)$

![image-20230813175538837](E:/typora/picture/刘志远文档/image-20230813175538837.png)

where RandomWalk(G,$\nu_{i},l)$generates a random walk rooted at v; with lengthl and Skip-gram(V,$\ell_{\nu_{i}},$w) function is defined in Algorithm 8.2, where α is the learning rate of stochastic gradient descent

Note that the parameter updating rule $\mathbf{V}=\mathbf{V}-\alpha_{l}\frac{\partial J}{\partial\mathbf{V}}$ in Skip-gram has a com-

plexity of O(|V|) because in the computation of the gradient of P(vk|v;)(as shown in Eq.8.21), the denominator has $\left|V\right|$ terms to compute. This complexity is unac ceptable for large-scale networks

![image-20230813175719938](E:/typora/picture/刘志远文档/image-20230813175719938.png)

![image-20230813175800330](E:/typora/picture/刘志远文档/image-20230813175800330.png)

为了解决这个问题，人们提出了分层的Softmax作为原始Softmax函数的变体。其核心思想是将顶点映射到一个平衡的二叉树，其中每个顶点对应于树的一个叶子。然后对一个顶点的预测转向对从根到相应叶子的路径的预测。假设从根到顶点vk的路径用一个树节点b1，b2...，blog|V|的序列表示，然后我们有逻辑函数可以很容易地在树节点上实现二进制决策。因此，时间复杂度从O（|V|）降低到O（log |V|）。我们可以通过使用霍夫曼编码将频繁的顶点映射到靠近根的树节点来加速算法。我们也可以使用负抽样，在word2vec中使用来代替层次softmax加速。

到目前为止，我们已经完成了深度行走算法的介绍。DeepWalk将有效的深度学习技术引入到网络嵌入学习中。表8.2给出了深度行走和Word2vec之间的类比。DeepWalk在网络分类任务上优于传统的网络表示学习方法，在大规模网络中也很有效。此外，随机游动的产生也可以推广到非随机游动，如信息传播流。

#### 8.2.3 Matrix Factorization Based Methods

##### 8.2.3.1 LINE

Tang等人[111]提出了一种名为LINE的网络嵌入模型。LINE算法可以处理具有任意类型的大规模网络：（un）有向的或加权的。为了建模顶点之间的相互作用，LINE建模了一阶接近，它由观察到的链接表示，二阶接近，它由共享的邻居决定，而不是顶点之间的链接。

在我们介绍算法的细节之前，我们可以后退一步，看看这个想法是如何工作的。一阶接近度的建模，即观察到的链接，是邻接矩阵的建模。正如我们在最后一节中所说的，邻接矩阵通常过于稀疏。因此，二阶接近度的建模，即具有共享邻居的顶点，可以作为补充信息，以丰富邻接矩阵，使其更密集。对所有具有共同邻居的顶点对进行枚举是非常耗时的。因此，有必要设计一个采样阶段来处理大规模的网络。采样阶段就像蒙特卡罗模拟来近似理想矩阵。

现在我们只有两个问题：如何定义一阶和二阶接近性，以及如何定义损失函数。换句话说，它等于如何定义M和损失函数。

顶点u和v之间的一阶接近度定义为边缘上的权重$w_{uv}$。如果顶点u和v之间没有边，那么它们之间的一阶接近度为0。

顶点u和v之间的二阶接近度定义为它们的邻域网络之间的相似性。设pu =（wu，1，...，wu，|V |）表示顶点u与所有其他顶点之间的一阶接近度。然后将u和v之间的二阶接近度定义为pu和pv的相似性。如果它们没有共享的邻居，那么二阶接近度为零。

然后我们可以更具体地引入LINE模型。vi和vj之间的联合概率为:$p_1(v_i,v_j)=\frac{1}{1+\exp(-\mathbf{v}_i\cdot\mathbf{v}_j)},\quad\quad\quad(8.34)$

其中vi和vj是d维行向量，表示顶点vi和vj的表示。

To supervise the prbies emirical pobility dfined a $\hat{p}_{1}(i,j)=\frac{w_{ij}}{W}$

where $W=\sum_{(\nu_{i},\nu_{j})\in E}W_{ij}$ .Thsourgal isto find vertex embeddings toapproximate
$\frac{w_{ij}}{W}$win $\frac1{1+\exp(-\mathbf{v}_i\cdot\mathbf{v}_j)}.$ roiwu neiea nscscui is uonoay
$$
\mathbf{v}_{i}\cdot\mathbf{v}_{j}=\mathbf{M}_{ij}=-\log(\frac{W}{w_{ij}}-1).
$$
The loss function between joint probability $p_{1}$ and its empirical

p100abllity P1 1S

$$
\mathcal{L}_{1}=D_{\mathrm{KL}}(\hat{p}_{1}\mid\mid p_{1}),
$$

(8.35)

where Dy(.|·）is KL-divergence of two probability distributions

我们分别训练了一阶和二阶的接近嵌入，并在训练阶段结束后将这些嵌入连接在一起作为顶点表示。

#### 8.2.4 Structural Deep Network Methods

与以往使用浅层神经网络模型来表征网络表示的方法不同，结构深度网络嵌入（SDNE）[125]采用了更深层次的神经模型来建模顶点嵌入之间的非线性。如图8.2所示，整个模型可分为两部分： (1)第一部分由拉普拉斯特征映射监督，建模一阶接近；(2)第二部分是无监督深度神经自编码器，表征二阶接近。最后，该算法以有监督部分的中间层作为网络表示。

首先，我们将简要介绍深度神经自动编码器。神经自动编码器要求输出向量应该与输入向量相似。一般来说，输出不能与输入向量相同，因为自动编码器的中间层的维数比输入层和输出层的维数要小得多。也就是说，一个深度自动编码器首先将输入压缩成一个低维的中间向量，然后尝试从一个低维的中间向量中重建原始的输入向量。一旦深度自动编码器被训练好，我们就可以说中间层是原始输入的一个极好的低维表示，因为我们可以从它中恢复输入向量。

更正式地说，我们假设输入向量是xi。然后将每一层的隐藏表示定义为

$\begin{aligned}
&\mathbf{y}_{i}^{(1)} =\mathrm{Sigmoid}(\mathbf{W}^{(1)}\mathbf{x}_{i}+\mathbf{b}^{(1)}),  \\
&&(8.38) \\
&\mathbf{y}_{i}^{(k)} =\mathrm{Sigmoid}(\mathbf{W}^{(k)}\mathbf{y}_{i}^{(k-1)}+\mathbf{b}^{(k)}),k=2,3\ldots, 
\end{aligned}$

![image-20230813205121324](E:/typora/picture/刘志远文档/image-20230813205121324.png)

式中，W (k)和b (k)为第k层的加权矩阵和偏差向量。我们假设Kth层的隐藏表示具有最小的维数。在得到yi (K)后，我们可以通过反转计算过程得到输出xˆi。那么自动编码器的优化目标是使输入向量xi和输出向量xˆi之间的差值最小：

$\mathscr{L}(\mathbf{W},\mathbf{b})=\sum_{i=1}^n\|\hat{\mathbf{x}}_i-\mathbf{x}_i\|^2,\quad(8.39)$

回到网络表示问题上，SDNE将自动编码器应用于每个顶点。每个顶点vi的输入向量xi定义如下：如果顶点vi和vj连接，则第j个条目xi j > 0，否则为xi j = 0。对于未称重的图，如果顶点（vi，v j）∈E，xi j = 1。然后中间层y (K) i可以看作是顶点vi的低维表示。还要注意的是，由于真实网络的稀疏性，输入向量中的零项比正项多得多。因此，应该强调正条目的丢失。因此，二次接近建模的最终优化目标可以写为

我们介绍了由深度自动编码器建模的无监督部分。现在我们转到监督部分。监督部分只是要求连通顶点的表示应该彼此接近。因此，这部分的损失函数为$\mathscr{L}_{1st}=\sum_{i,j=1}^{|V|}\mathbf{x}_{ij}\|\mathbf{y}_i^{(K)}-\mathbf{y}_j^{(K)}\|^2.$

最后，总体损失函数包括正则化项为$\mathscr{L}=\mathscr{L}_{2nd}+\alpha\mathscr{L}_{1st}+\lambda\mathscr{L}_{reg},$

#### 8.2.6 Applications

##### 8.2.6.1 Multi-label Classification

多标签分类任务是应用最广泛的网络表示学习评估任务。顶点的表示被认为是顶点特征，并应用于分类器来预测顶点标签。更正式地说，我们假设总共有K个标签。顶点-标签关系可以表示为二进制矩阵M∈{0,1}|V|×K，其中Mi j = 1表示顶点vi有第j个标签，否则表示Mi j = 0。具体来说，对于多类分类问题，每个顶点都有一个标签，这意味着矩阵m的每一行只有一个“1”。对于评估任务，我们设置一个训练比率，表示顶点观察标签的百分比。然后，我们的目标是预测测试集中的顶点的标签。

对于无监督网络表示学习算法，训练集的标签不用于嵌入学习。网络表示被输入到分类器，如SVM或逻辑回归。每个标签都将有它的分类标识符。对于半监督学习方法，他们在表示学习期间考虑到观察到的顶点标签。这些算法将有其特定的分类器来进行标签预测。

##### 8.2.6.2 Link Prediction

为了给定顶点表示进行链接预测，我们首先需要评估一对顶点的强度。通过计算两个顶点表示之间的相似性，来计算两个顶点之间的强度。这种相似度通常由余弦相似度、内积或平方损失来计算，这取决于算法。例如，如果一个算法在其目标函数中使用了Vi−Cj2 2，那么应该使用平方损失来度量顶点表示之间的相似性。然后，在我们得到所有未观察到的链接的相似性后，我们可以对它们进行排序，以进行链接预测。对于链路预测有两个重要的指标：接收机工作特征曲线下的面积（AUC）和精度。

AUC。AUC值是一个随机选择的缺失链接比一个随机选择的不存在的链接得分更高的概率。为了实现，我们随机选择一个缺失的链接和一个不存在的链接，并比较它们的相似性得分。假设在n个独立比较中有n个1倍的缺失环节有更高的分数，n个2个他们有相同的分数。那么AUC值为$\text{AUC}=\frac{n_1+0.5n_2}n.\quad\quad\quad\quad\quad\quad(8.71)$

注意，对于随机网络表示，AUC值应该为0.5。准确率给定所有未观察到的链接的排名，我们预测具有顶部-l得分最高的链接作为预测的链接。假设存在缺少Lr的链接，则精度定义为Lr /L。

##### 8.2.6.3 Information Diffusion Prediction

信息扩散预测是研究信息项目如何在用户之间传播的重要任务。信息扩散的预测，也称为级联预测，已经被广泛应用，如产品采用[67]、流行病学[124]、社交网络[63]、新闻和观点的传播[68]。

如图8.9所示，微观扩散预测的目的是猜测下一个被感染的用户，而宏观扩散预测的目的是估计扩散过程中被感染用户的总数。此外，当信息扩散发生在社交网络服务上时，用户之间的潜在社交图谱将可用。社会图将被认为是扩散预测的额外结构输入。

森林[139]是第一个同时解决微观和宏观预测的工作。如图8.10所示，森林提出了一个结构性的上下文最初引入的加速图卷积网络[41]的结构算法，以建立基于RNN的微观级联模型。对于每个用户v，我们首先从v及其邻居N (v)中抽取Z用户{u1，u2 ...，u Z }。然后通过聚合邻域特征来更新其特征向量。更新后的用户特征向量通过聚合来自v的一阶邻居的特征来编码结构信息。该操作也可以递归处理，以探索用户v的更大邻域。根据经验，两步邻域探索是时间有效的，并且足以给出有希望的结果。

![image-20230813210514860](E:/typora/picture/刘志远文档/image-20230813210514860.png)

森林进一步整合了宏观预测的能力，即通过强化学习来估计级联的最终大小到模型中。该方法可分为四个步骤： (a)通过微观级联模型对观察到的K个用户进行编码；(b)使微观级联模型能够通过级联模拟预测级联的大小；(c)使用均方对数变换误差（MSLE）作为宏观预测的监督信号；(d)采用强化学习框架，通过策略梯度算法更新参数。总体工作流程如图8.11所示。

### 8.3 Graph Neural Networks

在本节中，我们将介绍另一种用于网络表示学习的方法，它被称为图神经网络（GNNs）[101]。这些方法的目的是利用神经网络来建模图数据，并在许多应用中显示了其强大的能力。

#### 8.3.1 Motivations

近年来，CNNs [65]在各个机器学习领域都取得了突破，特别是在计算机视觉领域，并开启了深度学习[64]的革命。cnn能够提取多尺度局部特征，这些特征用于生成更具表现性的表示。随着我们更深入研究cnn和图表，我们发现了cnn的关键字：局部连接、共享权值和多层[64]的使用。这些也是非常重要的解决图域的问题，因为(1)图是最典型的局部连接结构，(2)共享权重减少计算成本与传统的光谱图理论[23]相比，和(3)多层结构是处理层次模式的关键，捕捉各种大小的特点。然而，cnn只能操作常规的欧几里得数据，如图像（二维网格）和文本（一维序列），而这些数据结构可以被视为图的实例。因此，认为要找到cnn对图的推广是很简单的。如图8.12所示，很难定义局部卷积滤波器和池化算子，这阻碍了CNN从欧几里得域到非欧几里得域的转换。

![image-20230813210858148](E:/typora/picture/刘志远文档/image-20230813210858148.png)

另一个动机来自于网络嵌入的[12,24,37,42,149]。在图分析领域，传统的机器学习方法通常依赖于手工设计的特征，并受到其缺乏灵活性和高成本的限制。遵循表示学习的思想和单词嵌入[81]的成功，DeepWalk [93]被认为是第一个基于图嵌入的方法表示学习，对生成的随机游动应用跳格模型[81]。类似的方法，如node2vec [38]、LINE [111]和TADW [136]也取得了突破。然而，这些方法有两个严重的缺点，[42]。首先，编码器中的节点之间不共享参数，这导致了计算效率低下，因为这意味着参数的数量随节点的数量呈线性增长。其次，直接嵌入方法缺乏泛化的能力，这意味着它们不能处理动态图，也不能推广到新的图。基于网络神经网络和网络嵌入，提出了图神经网络（GNNs）来集体聚合图结构中的信息。因此，它们可以对由元素及其依赖性组成的输入和/或输出进行建模。此外，图神经网络可以同时用RNN核建模图上的扩散过程。在本节的其余部分中，我们将首先介绍几种典型的图神经网络的变体，如图卷积网络（GCNs）、图注意网络（GATs）和图递归网络（GRNs）。然后，我们将介绍对原始模型的几个扩展，最后，我们将给出一些利用图神经网络的应用程序的例子。

#### 8.3.2 Graph Convolutional Networks

图卷积网络（GCNs）的目的是将卷积推广到图域。在这个方向上的进展通常被归类为光谱方法和空间（非光谱）方法。

##### 8.3.2.1 Spectral Approaches

##### 8.3.2.2 Spatial Approaches

在上面提到的所有光谱方法中，学习到的滤波器都依赖于拉普拉斯特征基，该特征基依赖于图的结构，即在特定结构上训练的模型不能直接应用于具有不同结构的图。

空间方法直接在图上定义卷积，在空间近邻上操作。空间方法的主要挑战是定义不同大小邻域的卷积操作和保持cnn的局部不变性。

神经FPs。Duvenaud等人，[31]对不同程度的节点使用不同的权重矩阵

$\begin{aligned}
&\mathbf{x}^{(t)} =\mathbf{h}_{\nu}^{(t-1)}+\sum_{i=1}^{|N_{\nu}|}\mathbf{h}_{i}^{(t-1)}, & (8.83)  \\
&\mathbf{h}_{\nu}^{(t)} =f(\mathbf{W}_{|N_{\nu}|}^{(t)}\mathbf{x}^{(t)}), 
\end{aligned}$

式中，W (t) |Nv |为t层|Nv |度节点的权重矩阵。而该方法的主要缺点是不能应用于节点度较多的大尺度图。

在下面对其他模型的描述中，我们使用h（v t）来表示节点v在t层的隐藏状态。Nv表示节点v的邻域集，|Nv|表示集合的大小。

**DCNN**Atwood和Towsley [4]提出了扩散-卷积神经网络（DCNNs）。转移矩阵用于定义DCNN中节点的邻域。对于节点分类，它有

$\mathbf{H}=f\left(\mathbf{W}^c\odot\vec{\mathbf{P}}\mathbf{X}\right),\quad(8.84)$

![image-20230813211650506](E:/typora/picture/刘志远文档/image-20230813211650506.png)

**DGCN**。Zhuang和Ma [158]提出了对偶图卷积网络（DGCN）来共同考虑图的局部一致性和全局一致性。它使用两个卷积网络来捕获局部/全局一致性，并采用无监督损失来对它们进行集成。第一个卷积网络与等式相同8.80.第二个网络用正点间互信息（PPMI）矩阵代替邻接矩阵：
$$
\mathbf{H}^{(t)}=f(D_{P}^{-\frac{1}{2}}X_{P}D_{P}^{-\frac{1}{2}}H^{(t-1)}\mathbf{W}),
$$

(8.85)

where $X_{P}$ is the PPMI matrix and $D_{P}$ is the diagonal degree matrix of $X_{P}$

图形。汉密尔顿等人的[41]提出了GraphSAGE，一个通用的归纳框架。该框架通过从节点的局部邻域中采样和聚合特征来生成嵌入。

$\begin{aligned}
&\mathbf{h}_{N_{\nu}}^{(t)} =\mathrm{AGGREGATE}^{(t)}(\{\mathbf{h}_{u}^{(t-1)},\forall u\in N_{\nu}\}),  \\
&&(8.86) \\
&\mathbf{h}_{\nu}^{(t)} =f(\mathbf{W}^{(t)}[\mathbf{h}_{\nu}^{(t-1)};\mathbf{h}_{N_{\nu}}^{(t)}]). 
\end{aligned}$

然而，[41]并没有利用等式中的全部邻居集8.86，但通过均匀抽样的固定大小的邻居集合。[41]提出了三个聚合器函数。

平均聚合器。它可以看作是来自转换GCN框架[59]的卷积操作的近似值，因此GCN变体的归纳版本可以通过$\mathbf{h}_{\nu}^{(t)}=f\left(\mathbf{W}\cdot\mathbf{M}\text{EAN}\left(\{\mathbf{h}_{\nu}^{(t-1)}\}\cup\{\mathbf{h}_{u}^{(t-1)}|\forall u\in N_{\nu}\}\right)\right).\quad(8.87)$

平均聚合器不同于其他聚合器，因为它不执行在等式中连接ht−1v和ht Nv的连接操作8.86.它可以看作是“跳过连接”[46]的一种形式，可以获得更好的性能。

LSTM聚合器。汉密尔顿等人的[41]也使用了一个基于LSTM的聚合器，它具有更大的表达能力。然而，lstm以顺序的方式处理输入，因此它们不是排列不变的。汉密尔顿等人[41]通过排列节点的邻居来适应lstm操作无序集。

池聚合器。在池化聚合器中，每个邻居的隐藏状态通过一个完全连接的层被输入，然后对节点的邻居集应用最大池化操作。

$\mathbf{h}_{N_{v}}^{(t)}=\max(\{f\left(\mathbf{W}_{pood}\mathbf{h}_{u}^{(t-1)}+\mathbf{b}\right),\forall u\in N_{\nu}\}).\quad(8.88)$

请注意，这里可以使用任何对称函数来代替最大池化操作。

#### 8.3.3 Graph Attention Networks

注意机制已成功地应用于许多基于序列的任务中，如机器翻译[5,36,121]、机器阅读[19]等。许多工作都集中于将注意机制推广到图域。

气体浓度。[122]等人提出了一种图注意网络（GAT），它将注意机制纳入到传播步骤中。具体来说，它使用了自注意策略，每个节点的隐藏状态都是通过关注其相邻节点来计算的。

[122]等人定义了单个图的注意层，并通过叠加该层构建了任意的图的注意网络。该层通过以下方法计算节点对（i、j）的注意机制中的系数：

$\alpha_{ij}=\frac{\exp\left(\text{LeakyReLU}\left(\mathbf{a}^\top[\mathbf{Wh}_i^{(t-1)};\mathbf{Wh}_j^{(t-1)}]\right)\right)}{\sum_{k\in N_i}\exp\left(\text{LeakyReLU}\left(\mathbf{a}^\top[\mathbf{Wh}_i^{(t-1)};\mathbf{Wh}_k^{(t-1)}]\right)\right)},\quad\quad(8.89)$

式中，αi j为节点j对节点i的注意系数。W∈RF×F是一个应用于每个节点的共享线性变换的权重矩阵，一个a∈R2F是权重向量。它通过一个softmax函数进行归一化，并应用了LeakyReLU非线性（负输入斜度为0.2）。

然后通过（应用非线性f后）得到每个节点的最终输出特征：$\mathbf{h}_i^{(t)}=f\left(\sum_{j\in N_i}\alpha_{ij}\mathbf{W}\mathbf{h}_j^{(t-1)}\right).\quad\quad\quad\quad\quad(8.90)$

此外，该层利用类似于[121]的多头注意来稳定学习过程。它应用K个独立的注意机制来计算隐藏状态，然后连接它们的特征（或计算平均值），从而产生以下两种输出表示：$\begin{aligned}\mathbf{h}_i^{(t)}&=\|_{k=1}^Kf\left(\sum_{j\in N_i}\alpha_{ij}^k\mathbf{W}^k\mathbf{h}_j^{(t-1)}\right),\\\\\mathbf{h}_i^{(t)}&=f\left(\frac{1}{K}\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^k\mathbf{W}^k\mathbf{h}_j^{(t-1)}\right),\end{aligned}$

[122]中的注意结构有几个特性： (1)节点-邻居对的计算是并行的，因此操作是有效的；(2)通过给相邻节点分配合理的权值来处理不同程度的节点；(3)可以很容易地应用于归纳学习问题。

加安。除GAT外，门控注意网络（GAAN）[150]也采用了多头注意机制。然而，它使用一种自我注意机制，从不同的头部收集信息，以取代GAT的平均操作。

#### 8.3.4 Graph Recurrent Networks

一些工作正在尝试在传播步骤中使用GRU [20]或LSTM [48]等门机制，以释放由普通GNN架构所引起的限制，并提高信息在图中长期传播的有效性。我们称这些方法为图递归网络（GRNs），我们将在本小节中介绍grn的一些变体。

**GGNN**Li等人[72]提出了门控图神经网络（GGNN），它在传播步骤中使用了门控循环单元（GRU）。它遵循递归神经网络对固定数量的L步长的计算步长，然后通过时间反向传播来计算梯度。具体来说，传播模型的基本递归性是

$\begin{aligned}
&\mathbf{a}_{\nu}^{(t)}=&& A_{\nu}^{\top}[\mathbf{h}_{1}^{(t-1)}\ldots\mathbf{h}_{N}^{(t-1)}]^{\top}+\mathbf{b},  \\
&\text{z} _{\nu}^{(t)}= && =\mathrm{Sigmoid}\left(\mathbf{W}^{z}\mathbf{a}_{\nu}^{(t)}+\mathbf{U}^{z}\mathbf{h}_{\nu}^{(t-1)}\right),  \\
&\mathbf{r}_{v}^{(t)}&& =\mathrm{Sigmoid}\left(\mathbf{W}^r\mathbf{a}_\nu^{(t)}+\mathbf{U}^r\mathbf{h}_\nu^{(t-1)}\right),  \\
&\widetilde{\mathbf{h}}_{\nu}^{(t)}&& =\tanh\left(\mathbf{W}\mathbf{a}_\nu^{(t)}+\mathbf{U}\left(\mathbf{r}_\nu^{(t)}\odot\mathbf{h}_\nu^{(t-1)}\right)\right),  \\
&\mathbf{h}_{\nu}^{(t)}&& =\left(1-\mathbf{z}_{\nu}^{(t)}\right)\odot\mathbf{h}_{\nu}^{(t-1)}+\mathbf{z}_{\nu}^{(t)}\odot\mathbf{\widetilde{h}}_{\nu}^{(t)}. 
\end{aligned}$

节点v首先聚合来自其邻居的消息，其中Av是图邻接矩阵A的子矩阵，表示节点v与其相邻节点的连接。然后，在类gru函数的作用下，利用节点邻居的信息和前一个时间步的隐藏状态来更新节点的隐藏状态。a收集节点v、z和r的邻域信息，分别为更新门和重置门。

通过基于树或图的传播过程，lstm也类似于GRU被使用。

**图LSTM**。这两种类型的树-lstm可以很容易地适应于图。[148]中的图结构的LSTM是N-ary Tree-LSTM应用的一个例子到图。然而，它是一个简化的版本，因为图中的每个节点最多有2条传入的边（来自它的父节点和兄弟姐妹的前身）。Peng等人[92]提出了基于关系提取任务的图LSTM的另一种变体。图和树之间的主要区别是图的边都有它们的标签，而[92]利用不同的权重矩阵来表示不同的标签。

$\begin{aligned}
\mathbf{i}_{1}^{t}& \text{,}  =\mathrm{Sigmoid}\left(\mathbf{W}^i\mathbf{x}_{\nu}^t+\sum_{k\in N_{\nu}}\mathbf{U}_{m(\nu,k)}^i\mathbf{h}_{k}^{t-1}+\mathbf{b}^i\right),  \\
\mathbf{f}_{\nu k}^{t}& =\mathrm{Sigmoid}\left(\mathbf{W}^{f}\mathbf{x}_{\nu}^{t}+\mathbf{U}_{m(\nu,k)}^{f}\mathbf{h}_{k}^{t-1}+\mathbf{b}^{f}\right),  \\
\mathbf{o}_{\nu}^{t}& =\mathrm{Sigmoid}\left(\mathbf{W}^o\mathbf{x}_{\nu}^t+\sum_{k\in N_{\nu}}\mathbf{U}_{m(\nu,k)}^o\mathbf{h}_{k}^{t-1}+\mathbf{b}^o\right),  \\
\mathbf{u}_{\nu}^{t}& =\tanh\left(\mathbf{W}^u\mathbf{x}_{\nu}^t+\sum_{k\in N_{\nu}}\mathbf{U}_{m(\nu,k)}^u\mathbf{h}_{k}^{t-1}+\mathbf{b}^u\right),  \\
c_{\nu}^{t}& =\mathbf{i}_{\nu}^{t}\odot\mathbf{u}_{\nu}^{t}+\sum_{k\in N_{\nu}}\mathbf{f}_{\nu k}^{t}\odot\mathbf{c}_{k}^{t-1},  \\
\mathbf{h}_{\nu}^{t}& =\mathbf{o}_{\nu}^{t}\odot\tanh(\mathbf{c}_{\nu}^{t}), 
\end{aligned}$

其中，m（v、k）表示节点v和k之间的边标签。此外，[74]还提出了一个Graph LSTM网络来处理语义对象解析任务。采用置信驱动方案自适应地选择起始节点并确定节点更新序列。它遵循了将现有的lstm推广到图结构数据的相同想法，但有一个特定的更新序列，而我们上面提到的方法对节点的顺序是不可知的。

句子LSTM。Zhang等人[152]提出了句子LSTM（S-LSTM）来改进文本编码。它将文本转换为一个图，并利用图LSTM来学习表示。S-LSTM在许多自然语言处理问题中表现出了很强的表示能力。

####8.3.5 Extensions

##### 8.3.5.1 Skip Connection

许多应用程序展开或堆叠图神经网络层，目的是获得更好的结果，因为更多的层（即k个层）使每个节点从相邻的k个跳点中聚合更多的信息。然而，在许多实验中已经观察到，更深的模型不能提高性能，而更深的模型甚至可以执行更糟糕的[59]。这主要是因为更多的层也可以传播来自呈指数增长的扩展邻域成员的噪声信息。

一种直接的方法，残差网络[45]，可以从计算机视觉社区找到。然而，即使有剩余的连接，具有更多层的GCN在许多数据集[59]上的性能也不如2层GCN。

公路网络。Rahimi等人[96]借鉴了高速公路网络[159]的想法，并使用分层闸门建造了高速公路GCN。每一层的输入值乘以门控权值，然后用输出值求和：

$\begin{aligned}T(\mathbf{h}^{(t)})&=\text{Sigmoid}\left(\mathbf{W}^{(t)}\mathbf{h}^{(t)}+\mathbf{b}^{(t)}\right),\\\mathbf{h}^{(t+1)}&=\mathbf{h}^{(t+1)}\odot T(\mathbf{h}^{(t)})+\mathbf{h}^{(t)}\odot(1-T(\mathbf{h}^{(t)})).\end{aligned}$

通过添加高速公路闸门，在[96]中讨论的一个特定问题中，性能在四层达到峰值。在[94]中提出的列网（CLN）也利用了高速公路网。然而，它的计算门控权值的函数却有所不同。

跳跃知识网络。Xu等人的[134]研究了邻域聚合方案的性质和由此产生的局限性。提出了一种可以学习自适应、结构感知表示的跳跃知识网络。跳转知识网络从最后一层的每个节点的所有中间表示（其中“跳转”到最后一层）中进行选择，这使模型能够为每个节点选择有效的邻域信息。Xu等人的[134]在实验中使用了三种连接、最大池化和LSTM-注意的方法来聚合信息。跳跃知识网络在社会、生物信息学和引文网络的实验中表现良好。它还可以与图卷积网络、GraphSAGE和图注意网络等模型相结合，以提高它们的性能。

##### 8.3.5.2 Hierarchical Pooling

在计算机视觉领域，一个卷积层之后通常是一个池化层，以获得更一般的特征。与这些池化层类似，许多工作都集中在设计图上的分层池化层上。复杂和大规模的图通常具有丰富的层次结构，这对于节点级和图级的分类任务具有重要意义。

为了探索这些内部特征，边缘条件卷积（ECC）[106]设计了其具有递归降采样操作的池化模块。降采样方法是基于用拉普拉斯算子的最大特征向量的符号将图分解为两个分量。

扩散池[144]提出了一个可学习的层次聚类模块，通过在每一层训练一个分配矩阵：
$$
\mathbf{S}^{(l)}=\mathrm{Softmax}(\mathrm{GNN}_{l,pool}(A^{(l)},\mathbf{V}^{(l)})),
$$

(8.98)

where $\mathbf{V}(l)$ is node features and $A^{(l)}$ is coarsened adjacency matrix of layer l.

##### 8.3.5.3 Neighborhood Sampling

原始的图卷积神经网络有几个缺点。具体来说，GCN需要完整的图拉普拉斯量，这对于大型图来说是计算消耗的。此外，一个节点在L层的嵌入是通过其所有邻居在L−1层的嵌入递归计算的。因此，单个节点的接受域相对于层数呈指数级增长，因此计算单个节点的梯度代价很大。最后，GCN独立训练一个固定图，缺乏归纳学习能力。

GraphSAGE [41]是对原始GCN的全面改进。为了解决上述问题，GraphSAGE用可学习的聚合函数替换了全图拉普拉斯函数，这些函数对于执行消息传递和推广到不可见的节点至关重要。如等式中所示8.86，它们首先聚合邻域嵌入，与目标节点的嵌入连接起来，然后传播到下一层。通过学习到的聚合和传播函数，GraphSAGE可以为不可见的节点生成嵌入。此外，GraphSAGE使用邻居采样来缓解感受野的扩张。

PinSage [143]提出了基于重要性的抽样方法。通过模拟从目标节点开始的随机游走，选择归一化访问数最高的顶部T节点。

FastGCN [16]进一步改进了采样算法。FastGCN直接对每个节点的邻居进行采样，而不是对每个节点的接受域进行采样。FastGCN采用重要抽样

适应。与上述固定采样方法相比，[51]引入了一个参数化的可训练采样器，以前一层为条件进行分层采样。此外，该自适应采样器可以找到最优的采样重要性，并同时减少方差

##### 8.3.5.4 Various Graph Types

在原始的GNN [101]中，输入图由具有标签信息的节点和无向边组成，这是最简单的图格式。然而，世界上有许多图的变体。在下面，我们将介绍一些设计用于建模不同类型的图的方法。

定向图形。该图的第一个变体是有向图。无向边可以被视为两条有向边，这表明两个节点之间存在着一种关系。然而，有向边比无向边能带来更多的信息。例如，在一个知识图中，边从头实体开始，到尾实体结束，头实体是尾实体的父类，这表明我们应该以不同的方式对待来自父类和子类的信息传播过程。DGP [55]使用两种权重矩阵，Wp和Wc，来合并更精确的结构信息。传播规则如下：

异构图形。图的第二种变体是异构图，其中有几种节点。处理异构图的最简单的方法是将每个节点的类型转换为一个与原始特征连接的一个热的特征向量。更重要的是，图的初始空间[151]将元路径的概念引入到异构图上的传播中。使用元路径，我们可以根据邻居的节点类型和距离对其进行分组。对于每个邻组，图初始化将其视为齐次图中的子图来进行传播，并将来自不同齐次图的传播结果连接起来，以进行集体节点表示。最近，[128]提出了一种利用节点级和语义级注意的异构图注意网络（HAN）。该模型能够同时考虑节点的重要性和元路径

带边缘信息的图形。在图的另一种变体中，每条边都有附加的信息，比如边的权重或类型。我们列出了两种处理这类图的方法：

首先，我们可以将该图转换为一个二部图，其中原始边也成为节点，一条原始边被分割成两条新边，这意味着在边节点和开始/结束节点之间有两条新边。G2S [7]的编码器对邻居使用以下聚合函数：
$$
\mathbf{h}_{\nu}^{(t)}=f\left(\frac{1}{|N_{\nu}|}\sum_{u\in N_{\nu}}\mathbf{W}_{r}\left(\mathbf{r}_{\nu}^{(t)}\odot\mathbf{h}_{u}^{(t-1)}\right)+\mathbf{b}_{r}\right),
$$

(8.101)

where W, and b, are the propagation parameters for different types of edges
(relations).

其次，我们可以适应不同的权值矩阵来适应不同类型的边的传播。当关系的数量很大时，r-GCN [102]引入两种正则化来减少建模关系数量的参数数量：基对角分解和块对角分解。通过基分解，每个Wr的定义如下：$\mathbf{W}_r=\sum\limits_{b=1}^B\alpha_{rb}\mathbf{M}_b.\quad\quad\quad\quad\quad(8.102)$

这里每个Wr是基变换的线性组合Mb∈Rdin×，系数αr b。在块对角分解中，r-GCN通过对一组低维矩阵的直接和来定义每个Wr，这需要比第一个矩阵更多的参数。

动态图形。该图的另一种变体是动态图，它具有静态图结构和动态输入信号。为了捕获这两种信息，DCRNN [71]和STGCN [147]首先通过gnn收集空间信息，然后将输出输入序列模型，如序列到序列模型或cnn。不同的是，Ctapil-RNN[53]和ST-GCN [135]同时收集时空信息。它们扩展了具有时间连接的静态图结构，从而可以将传统的gnn应用到扩展的图上。

###8.4 Summary

在本章中，我们介绍了网络表示学习，它将网络结构信息转化为连续的向量空间，并使深度学习技术在网络数据上成为可能。无监督网络表示学习是NRL发展过程中的首要任务。光谱聚类、DeepWalk、LINE、GraRep等方法都利用网络结构进行顶点嵌入学习。然后，TADW在矩阵分解的框架下，将文本信息整合到NRL中。然后，NEU算法向前推进了一步，提出了一种提高任何学习网络嵌入质量的通用方法。其他无监督方法也考虑保留网络拓扑的特定属性，例如，社区和不对称。

近年来，半监督NRL算法引起了广泛的关注。这种方法侧重于一个特定的任务，如分类和使用训练集的标签来提高网络嵌入的质量。为此，我们提出了节点2vec、MMDW和许多其他方法，包括图神经网络（GNNs）家族。半监督算法可以获得更好的结果，因为它们可以利用特定任务中的更多信息。

## 9 Cross-Modal Representation

跨模态表示学习是表示学习的重要组成部分，旨在学习文本、音频、图像、视频等模态的潜在语义表征。在本章中，我们首先介绍了典型的跨模态表示模型。在此之后，我们回顾了几个与跨模态表示学习相关的真实应用，包括图像字幕、视觉关系检测和视觉问题回答。

### 9.1 Introduction

正如维基百科所介绍的，模态是对计算机和人类之间单一独立的感官输入/输出通道进行分类。更一般地说，模式是人类和现实世界之间信息交换的不同手段。分类通常是基于信息呈现给人类的形式。现实世界中的典型模式包括文本、音频、图像、视频等。

跨模态表示学习是表征学习的重要组成部分。事实上，人工智能本质上是一种多模态任务[30]。人类每天都要接触到多模式的信息，整合不同模式的信息，做出综合判断是正常的。此外，不同的模式并不是独立的，但它们有或多或少的相关性。例如，对一个音节的判断不仅是由我们所听到的声音，还包括我们所看到的说话者的嘴唇和舌头的运动。在[48]上的一个实验表明，一个声音/ba/与视觉/ga/被大多数人视为a /da/。另一个例子是人类的能力考虑二维图像和3d扫描相同的对象在一起，重建其结构：图像和扫描之间的相关性可以找到基于这一事实的不连续深度扫描通常表明图像的直线[52]。受此启发，我们很自然地考虑在人工智能系统中组合来自多模态的输入并生成跨模态表示的可能性。

Ngiam等人。[52]探索了将多种模式合并为一个学习任务的概率。作者将一个典型的机器学习任务分为三个阶段：特征学习、监督学习和预测。他们进一步提出了四种多模态的学习设置： (1)单模态学习：所有阶段都是在一种模态上完成的。(2)多模态融合：所有的阶段都是用所有可用的模式来完成的。(3)跨模态学习：在特征学习阶段，所有的模态都是可用的，但在监督学习和预测中，只使用了一种模态。(4)共享表示学习：在特征学习阶段，所有的模式可用。在监督学习中，只使用了一种模态，而在预测中，使用了一种不同的模态。

实验结果表明，这些多模态任务具有良好的效果。当提供更多的模式（如多模态融合、跨模态学习和共享表示学习）时，系统的性能通常会更好。在本章的下一部分中，我们将首先介绍跨模态表示模型，这是自然语言处理中跨模态表示学习的基本部分。然后，我们将介绍几个关键的应用程序，如图像字幕、视觉关系检测和视觉问题回答。

### 9.2 Cross-Modal Representation

跨模态表示学习旨在利用来自多种模态的信息来构建嵌入。现有的涉及文本模态的跨模态表示模型一般可以分为两类： (1) [30,77]试图将来自不同模态的信息融合成统一的嵌入（例如，视觉基础的词表示）。(2)研究人员还试图在一个共同的语义空间中建立不同模式的嵌入，这允许模型计算跨模态相似性。这种跨模态相似性可以进一步用于下游任务，如零镜头识别[5,14,18,53,65]和跨媒体检索[23,55]。在本节中，我们将分别介绍这两种跨模态表示模型。

#### 9.2.1 Visual Word2vec

计算单词嵌入是自然语言处理中表示学习的一项基本任务。典型的单词嵌入模型（如Word2vec [49]）是在一个文本语料库上进行训练的。这些模型虽然非常成功，但并不能发现可以在其他模式中表达的单词之间的隐式语义相关性。Kottur等人[30]提供了一个例子：尽管吃东西和盯着看似乎与文字无关，但图片可能显示，当人们吃东西时，他们也会倾向于盯着它看。这意味着在构建单词嵌入时考虑其他模式可能有助于捕获更多的隐式语义相关性。

视觉是最关键的表达方式之一，它已经吸引了那些寻求改善词汇表现的研究人员的注意。已经提出了几种结合视觉信息和改进视觉单词嵌入的模型。下面我们将介绍两个包含视觉信息的典型单词表示模型。

#####9.2.1.1 Word Embedding with Global Visual Context

Xu等人[77]提出了一个模型，可以自然地尝试结合视觉特征。它声称，在大多数单词表示模型中，只考虑局部上下文信息（例如，试图使用邻近的单词和短语来预测一个单词）。另一方面，全局文本信息（例如，文章的主题）经常被忽视。该模型利用可视化信息作为全局特征，扩展了一个简单的局部上下文模型（见图9.1）。

![image-20230814094554156](E:/typora/picture/刘知远文档/image-20230814094554156.png)

该模型的输入是一个图像I和一个描述它的序列。它基于一个简单的局部上下文语言模型：当我们考虑一个序列中的某个单词wt时，它的局部特征是一个窗口中单词嵌入的平均值，即{wt−k，...，wt−1，wt+1，...，wt+k}。视觉特征使用CNN直接从图像I中计算出来，然后作为全局特征。然后将局部特征和全局特征连接成一个向量f。一个单词wt（在这个空白部分）的预测概率是f和单词嵌入wt的softmax归一化乘积

$\begin{aligned}o_{w_t}&=\mathbf{w}_t^T\mathbf{f},&(9.1)\\P(w_t|w_{t-k},\ldots,w_{t-1},w_{t+1},\ldots,w_{t+k};I)&=\frac{\exp(o_{w_i})}{\sum_i\exp(o_{w_i})}.&(9.2)\end{aligned}$

该模型通过最大化对数概率的平均值来进行优化：

$\mathscr{L}=\frac{1}{T}\sum_{t=k}^{T-k}\log P(w_t|w_{t-k},\ldots,w_{t-1},w_{t+1},\ldots,w_{t+k};I).\quad(9.3)$

分类错误将被反向传播到局部文本向量（即单词嵌入）、视觉向量和所有模型参数。这就完成了对一组单词嵌入、一个语言模型和用于视觉编码的模型的联合学习。

##### 9.2.1.2 Word Embedding with Abstract Visual Scene

Kottur等人[30]还提出了一种神经模型来从视觉信息中捕获细粒度的语义。我们不关注真实的像素，而是考虑视觉背后的抽象场景。该模型以一对视觉场景和一个相关的单词序列（I，w）作为输入。在每个训练步骤中，在单词序列w上使用一个窗口，形成一个子序列Sw。Sw中的所有单词将使用单热编码输入到输入层，因此输入层的维度是|V |，这也是词汇表的大小。然后将单词转换为它们的嵌入，而隐藏层是所有这些嵌入的平均值。隐层的大小为NH，这也是单词嵌入的维数。隐层和输出层由一个维数为NH∗NK的全连接矩阵和一个softmax函数连接。输出层可以看作是在视觉场景I的离散值函数g（·）上的概率分布（细节将在以下段落中给出）。通过最小化目标函数，对整个模型进行了优化$\mathscr{L}=-\log P(g(w)|S_w).\quad(9.4)$

该模型中最重要的部分是函数g（·）。它将视觉场景I映射到集合{1,2，...，NK }中，这表明它是什么样的抽象场景。在实践中，使用K-means聚类离线学习，每个聚类表示一种视觉场景的语义，从而设计为与场景相关的单词序列w。

#### 9.2.2 Cross-Modal Representation for Zero-Shot Recognition

大规模的数据集部分地支持了深度学习方法的成功。尽管数据集的规模继续扩大，并且涉及到更多的类别，但数据集的注释是昂贵和耗时的。对于许多类别，有非常有限的实例，甚至没有实例，这限制了识别系统的可伸缩性。

提出了零射击识别来解决上述问题，目的是对在训练中没有看到的类别实例进行分类。许多工作提出利用跨模态表示进行零镜头图像分类[5,14,18,53,65]。具体来说，图像表示和类别表示被嵌入到一个公共的语义空间中，其中图像表示和类别表示之间的相似性可以用于进一步的分类。例如，在这样一个共同的语义空间中，猫的图像的嵌入比类别卡车的嵌入更接近于类别猫的嵌入。

#####9.2.2.1 Deep Visual-Semantic Embedding

零射击学习的挑战在于缺乏不可见类别的实例，这使得获得表现良好的不可见类别的分类器具有挑战性。Frome等人[18]提出了一个模型，利用标记图像和来自大规模纯文本的信息进行零镜头图像分类。他们试图利用单词嵌入中的语义信息，并将其转移到图像分类系统中。

他们的模型的动机是，单词嵌入包含了概念或类别的语义信息，这些信息可以潜在地用作相应类别的分类符。相似的类别在语义空间中可以很好地聚类。例如，在单词嵌入空间中，虎鲨这个词的最近邻是类似种类的鲨鱼，如牛鲨、黑尖鲨、沙洲鲨和海洋白尖鲨。此外，不同集群之间的边界是明确的。上述特性表明，词嵌入可以进一步用作识别系统的分类器。

具体来说，该模型首先在大规模的维基百科文章中使用Skip-gram文本模型来预训练单词嵌入。对于视觉特征提取，该模型在ImageNet上对1000个对象类别的深度卷积神经网络进行了预训练。利用预先训练好的单词嵌入和卷积神经网络对所提出的深度视觉-语义嵌入模型（DeViSE）进行了初始化。

为了训练所提出的模型，他们将预先训练好的卷积神经网络的最大软层替换为线性投影层。该模型被训练来预测使用铰链排名损失的图像类别的单词嵌入：$\mathscr{L}(I,y)=\sum_{j\neq y}\max[0,\gamma-\mathbf{w}_y\mathbf{MI}+\mathbf{w}_j\mathbf{MI}],\quad\quad\quad(9.5)$

wy和wj的学习单词嵌入正标签和采样的负标签，分别表示图像的特征从卷积神经网络，M是线性投影层的可训练参数，和γ是铰链超参数排名损失。给定一个图像，目标要求模型对正确的标签产生比随机选择的标签更高的分数，其中分数被定义为投影图像特征和单词嵌入术语的点积。

在测试时，给定一个测试图像，在训练过程中使用相同的方法获得每个可能类别的得分。请注意，在测试时的一个关键区别是，分类器（单词嵌入）被扩展到所有可能的类别，包括看不见的类别。因此，该模型能够预测不可见的类别。

实验结果表明，DeViSE可以做出更合理的零射击预测，这意味着即使预测不完全正确，它在语义上也与地面真实类相关。但缺点是，虽然模型可以利用单词嵌入中的语义信息进行零镜头图像分类，但使用单词嵌入作为分类器限制了模型的灵活性，导致原始的1000个类别的性能低于原始的softmax分类器。

##### 9.2.2.2 Convex Combination of Semantic Embeddings

受DeViSE的启发，[53]提出了一个模型ConSE，该模型试图利用来自单词嵌入的语义信息进行零射击分类。与DeViSE的一个重要区别是，它们使用已看到类别的词嵌入的凸组合来获得测试图像的语义嵌入。相应类别的分数决定了组合词嵌入的权重。

具体来说，他们对已看到的类别训练一个深度卷积神经网络。在测试时，给定一个测试图像I（可能来自不可见的类别），他们获得已看到类别的最高T自信预测，其中T是一个超参数。然后，I的语义嵌入f (I)由顶级T自信类别的语义嵌入的凸组合确定，可以正式定义如下：

$\begin{aligned}f(I)=\frac{1}{Z}\sum_{t=1}^{T}P(\hat{y}_{0}(I,t)|I)\cdot\mathbf{w}(\hat{y}_{0}(I,t)),&&(9.6)\end{aligned}$

where $\hat{y}_{0}(I,t)$ is the tth mostconfident training label for I, w(yo(I,t))is the semantic embedding(word embedding) of $\mathbf{\hat{y}}_{0}(I,t)$ , and Z is a normalization factor given by

$$
Z=\sum_{t=1}^{T}P(\hat{y}_{0}(I,t)|I).
$$

(9.7)

在获得语义嵌入f (I)后，类别m的得分由f (I)和w (m)的余弦相似度给出。

虽然ConSE和DeViSE有许多相似之处，但也有一些关键的不同之处。DeViSE用一个投影层取代了预先训练过的视觉模型的softmax层，而ConSE则保留了softmax层。ConSE不需要进行进一步的训练，并在测试时使用语义嵌入的凸组合来执行零射击分类。实验结果表明，ConSE在看不见类别上的性能优于DeViSE，具有更好的泛化能力。然而，ConSE在可见类别上的性能不如DeViSE和原始的softmax分类器具有竞争力。

####9.2.3 Cross-Modal Representation for Cross-Media Retrieval

在公共语义空间中从不同模态学习跨模态表示可以轻松计算跨模态相似性，这可以促进许多重要的跨模态任务，如跨媒体检索。随着互联网上的文本、图像、视频和音频等多媒体数据的快速增长，跨不同方式检索信息的需求变得越来越强烈。交叉媒体检索是多媒体领域的一项重要任务，其目的是跨文本和图像等不同模式进行检索。例如，用户可以提交一匹白马的图像，并从不同的模式中检索相关信息，如对马的文本描述，反之亦然。跨模式检索的一个重大挑战是不同模式之间的域差异。此外，对于一个特定的感兴趣的领域，跨模态数据可能是不够的，这限制了现有的跨模态检索方法的性能。许多工作都集中在上述交叉模态检索[23,24]中提到的挑战上。

#####9.2.3.1 Cross-Modal Hybrid Transfer Network

Huang等人[24]提出了一个框架，试图通过迁移学习来缓解跨模态数据稀疏性问题。他们建议利用来自大规模单模态数据集的知识来促进在小规模数据集上的模型训练。大量的辅助数据集表示为源域，感兴趣的小规模数据集表示为目标域。在他们的工作中，他们采用了一个大型图像数据库ImageNet [12]作为源域。

他们的模型由一个模态共享转移子网和一个层共享相关子网组成。在模态共享传输子网络中，他们采用AlexNet [32]的卷积层来提取源域和目标域的图像特征，并利用词向量来获取文本特征。图像和文本特征通过两个完全连接的层，其中进行单模态和跨模态的知识转移。

单模态知识转移的目的是将知识从源域的图像转移到目标域的图像。主要的挑战是两个图像数据集之间的域差异。他们提出通过最小化源域和目标域之间的图像模态的最大平均差异（MMD）来解决域差异问题。MMD在全连接层中以层的方式计算。

通过最小化再现核希尔伯特空间中的MMD，得到了来自源域和目标域的图像表示鼓励有相同的分布，因此来自源域图像的知识有望转移到目标域的图像。此外，还通过优化已标记图像实例的软最大损耗，对源域中的图像编码器进行了微调。

跨模态知识转移的目的是在目标领域的图像和文本之间进行知识转移。通过最小化它们的欧氏距离，鼓励来自目标域中的注释对的文本和图像表示彼此接近。在全连接的层中，图像和文本表示的跨模态传输损失也被分层计算。图像和文本模式之间的领域差异有望在高层层中减少。

在层共享相关子网中，将目标域中的模态共享转移子网的表示输入到共享的全连接层中，以获得图像和文本的最终公共表示。由于参数在两种模态之间共享，最后两个完全连接的层被期望捕获跨模态相关。他们的模型还利用目标域的标签信息，最小化标记图像/文本对的软大损失。在获得最终的公共表示后，可以通过简单地计算语义空间中的最近邻来实现跨媒体检索。

#####9.2.3.2 Deep Cross-Media Knowledge Transfer

作为一个扩展[23,24]也专注于处理领域差异和跨媒体检索在特定领域，黄和彭[23]提出一个框架，转移知识从大规模跨媒体数据集（源域）提高模型性能在另一个小规模跨媒体数据集（目标域）。

与[24]的一个关键区别是，源域中的数据集也由带有标签注释的图像/文本对组成，而不是[24]中的单模态设置。由于这两个领域都包含图像和文本媒体类型，因此领域的差异来自同一媒体类型的媒体水平差异，以及不同领域之间的图像/文本相关模式的相关级差异。他们提出通过联合减少媒体层面和相关层面的领域差异来转移媒体内部语义和媒体间的相关知识。

为了提取不同媒体类型的分布式特征，图像编码器采用VGG19 [63]，文本编码器采用Word CNN [29]。这两个域具有相同的架构，但不共享参数。提取的图像/文本特征分别通过两个完全连接的层，在其中进行媒体级传输。与[24]类似，它们通过最小化源域和目标域之间的最大平均差异（MMD）来减少相同模式内的域差异。MMD以分层的方式计算，以在相同的模式中传输知识。它们还最小化了源域和目标域中图像/文本表示对之间的欧氏距离，以保持跨模态的语义信息。

相关级转移的目的是减少不同领域中图像/文本相关模式的域差异。在两个域中，图像表示和文本表示共享最后两个完全连接的层，以获得每个域的公共表示。它们优化了不同域中共享的全连接层之间的层级MMD损失，以进行相关级的知识转移，这鼓励了源域和目标域具有相同的图像/文本相关模式。最后，用图像/文本对的标签信息对这两个领域进行训练。请注意，源域和目标域并不一定共享相同的标签集。

此外，他们还提出了一种渐进式迁移机制，这是一种旨在提高模型训练的鲁棒性的课程学习方法。这是通过在早期选择简单的样本进行模型训练来实现的，并逐渐增加了训练的难度。根据双向跨媒体检索的一致性来衡量训练样本的难度。

###9.3 Image Captioning

图像字幕是自动生成图像的自然语言描述的任务。它是连接自然语言处理和计算机视觉的人工智能领域的一项基本任务。与图像分类和目标检测等其他计算机视觉任务相比，图像字幕难度较大，原因有二：一是不仅要检测物体，而且要检测它们之间的关系；其次，除了基本的判断和分类外，还必须生成自然语言句子。

传统的图像字幕方法通常使用检索模型或生成模型，与新的深度神经网络模型相比，这些模型的推广能力相对较弱。在本节中，我们将在下面介绍这两种类型的几个典型模型。

####9.3.1 Retrieval Models for Image Captioning

检索模型的主要管道是(1)使用特殊特征表示图像和/或句子；(2)对于新的图像和/或句子，根据特征的相似性搜索可能的候选对象。

将单词与图像进行链接有着丰富的历史，而[50]（一种检索模型）是第一个图像标注系统。本文试图建立一个基于标记数据的图像关键字分配系统。管道的内容如下：

(1)图像分割。每个图像被分成几个部分，使用最简单的矩形划分。这样做的原因是，一个图像通常用多个标签进行注释，每个标签通常只对应于它的一部分。分割将有助于减少标签中的噪声。

(2)特征提取。提取了图像各部分的特征。

(3)聚类。将图像片段的特征向量分为几个簇。每个集群累积单词的频率，从而计算单词的可能性。具体来说，$P(w_i|c_j)=\frac{P(c_j|w_i)P(w_i)}{\sum_kP(c_j|w_k)P(w_k)}=\frac{n_{ji}}{N_j},\quad\quad(9.10)$

其中，n ji为wordwi在集群j中出现的次数，nj为所有单词在集群j中出现的次数。该计算是基于使用频率作为概率的。

(4)推理。对于一幅新图像，模型将其分割成分段，提取每个部分的特征，最后聚合分配给每个部分的关键字，得到最终的预测。

该模型的关键思想是图像分割。以一幅风景图片为例，有两个部分：山和天空，这两个部分都要标注这两个标签。然而，如果另一张图片有山和河流两个部分，这两个山的部分将有希望在同一个集群中，并发现它们共享同一个标签山。这样，就可以给图像的正确部分分配标签，也可以减轻噪声。

[17]提出了另一个典型的检索模型，它可以在图像和句子之间分配一个链接分数。一个有意义的中间空间会计算出这个链接的分数。意义空间的表示是一个三重形式的对象，动作，场景.三重体的每个槽都有一个有限的离散候选集。将图像和句子映射到意义空间的问题涉及到求解一个马尔可夫随机场。

与之前的模型不同，该系统不仅可以做图像标题，还可以做逆标题，即给定一个句子，该模型提供了某些可能的关联图像。在推理阶段，首先将图像（句子）映射到中间意义空间，然后我们在池中搜索匹配分数最好的句子（图像）。

之后，研究人员还提出了许多考虑了图像不同种类特征的检索模型，如[21,28,34]。

#### 9.3.2 Generation Models for Image Captioning

与基于检索的模型不同，生成模型的基本管道是(1)使用计算机视觉技术提取图像特征，(2)使用语言模型或句子模板等方法从这些特征中生成句子。

Kulkarni等人[33]提出了一个系统，使特定的图像和句子生成过程之间建立紧密的联系。该模型使用视觉检测器来检测特定的对象，以及单个对象的属性和多个对象之间的关系。然后构造一个条件随机场，合并一元图像势和高阶文本势，从而预测标签为图像。由条件随机场（CRF）预测的标签被安排为一个三重体，例如，白色，云，，，蓝色，天空.

然后根据标签生成句子。基于三重骨架构建句子有两种方法。(1)首先是使用n-gram语言模型。例如，当试图决定是否在一对有意义的单词（这意味着它们在三重内部）a和b之间放置一个胶水单词x时，将比较ˆp（axb）和ˆp（ab）的概率。ˆp是n-gram语言模型的标准长度归一化概率。(2)第二种方法是使用一组描述性语言模板，从而缓解了语言模型中的语法错误问题。

此外，[16]提出了一个新的框架来明确地表示图像结构与其标题句子结构之间的关系。该方法，视觉依赖表示，检测图像中的对象，并基于所提出的视觉依赖语法检测这些对象之间的关系，其中包括8个典型的关系。然后，图像可以排列为一个依赖图，其中节点是对象，边是关系。这个图像依赖图可以与标题句子的语法依赖表示对齐。本文进一步提供了四个模板，从提取的依赖关系表示中生成描述性句子

除了这两个典型的作品之外，还有大量用于图像字幕的生成模型，比如[15,35,78]。

####9.3.3 Neural Models for Image Captioning

在[33]中，2011年有人声称，在图像字幕任务中：自然语言生成仍然是一个开放的研究问题。以前的大多数工作都是基于检索和总结的。从2015年开始，受神经语言模型和神经机器翻译研究进展的启发，提出了一些基于编码器-解码器系统的端到端神经图像字幕模型。这些新模型极大地提高了生成自然语言描述的能力。

#####9.3.3.1 The Basic Model

传统的机器翻译模型通常会将许多子任务拼接在一起，比如单个单词的翻译和重新排序，以执行句子和段落的翻译。最近的神经机器翻译模型，如[8]，使用单一的编码-解码器模型，可以方便地通过随机梯度下降进行优化。图像字幕的任务本质上类似于机器翻译，因为它也可以被视为一个翻译任务，其中源“语言”是一个图像。用于机器翻译的编码器和解码器通常是rnn，这是对单词序列的自然选择。对于图像字幕，选择CNN作为编码器，同时仍使用RNN作为解码器。

Vinyals等人，[70]是使用编码器-解码器进行图像字幕的最典型的模型（见图9.2）。具体地说，使用CNN模型将图像编码为一个固定长度的向量，该向量被认为包含了进行字幕所需的信息。使用这个向量，使用一个RNN语言模型来生成自然语言描述，这就是解码器。在这里，解码器类似于用于机器翻译的LSTM。第一个单元以图像向量作为输入向量，其余单元以前一个字嵌入作为输入。每个单元输出一个向量o，并将一个向量传递给下一个单元。O被进一步输入到一个softmax层，其输出p是词汇表中每个单词的概率。在训练和测试中，处理这些计算概率的方法是不同的：

![image-20230814104404106](E:/typora/picture/刘知远文档/image-20230814104404106.png)

训练这些概率p用于计算所提供的描述句子的可能性。考虑到rnn的性质，很容易将联合概率建模为条件概率。$\log P(s|I)=\sum_{t=0}^N\log P(w_t|I,w_0,\ldots,w_{t-1}),\quad\quad(9.11)$

其中，s = {w标记，和1，w2，...，wN }是句子和它的单词，w0是一个特殊的开始，I是图像。因此，可以进行随机梯度下降来优化模型。

测试。有多种方法可以生成给定一个图像的句子。第一个叫做抽样。对于每一步，选择p中概率最高的单个单词，并作为下一个单元的输入，直到生成END令牌或达到最大长度。第二个叫做光束搜索。对于每一步（现在句子的长度为t），保留k个最好的句子。每一个都会产生几个长度为t + 1的新句子，同样，他们只知道句子是保持。光束搜索提供了一个更好的近似值

$s^*=\arg\max_s\log P(s|I).$

###9.4 Visual Relationship Detection

视觉关系检测是检测图像中的对象并理解它们之间的关系的任务。虽然目标检测总是基于语义分割或目标检测方法，如R-CNN，但理解这种关系是这项任务的关键挑战。虽然检测与图像信息的视觉关系是直观而有效的[25,62,84]，但利用语言信息可以进一步提高模型性能[37,41,82]。

####9.4.1 Visual Relationship Detection with Language Priors

Lu等人[41]提出了一个模型，该模型使用语言先验来提高罕见关系的性能，而仅从图像中很难获得足够的训练实例。整体架构如图9.5所示。他们首先训练一个CNN来计算从视觉输入中获得的非标准化关系的概率

![image-20230814110637714](E:/typora/picture/刘知远文档/image-20230814110637714.png)

####9.4.2 isual Translation Embedding Network

受最近知识表示学习进展的启发，[82]提出了一种视觉翻译嵌入网络VTransE。对象和对象之间的关系被建模为TransE [7]类似的向量转换。VTransE首先将主体和对象投射到与关系转换向量r∈rr相同的空间中。主体和对象可以在特征空间中表示为xs，∈RM，其中M∈。与TransE关系类似，VTransE建立的关系为

![image-20230814112610652](E:/typora/picture/刘知远文档/image-20230814112610652.png)

###9.6 Summary

在本章中，我们首先介绍了跨模态表示学习的概念。跨模态学习是必要的，因为许多现实世界的任务需要能够理解来自不同模态的信息，如文本和图像。接下来，我们介绍了跨模态表示学习的概念，它旨在利用链接，并更好地利用来自不同模态的信息绑结我们概述了现有的跨模态任务的跨模态表示学习方法，包括零镜头识别、跨媒体检索、图像字幕和视觉问题回答。这些跨模态学习方法要么尝试将来自不同模态的信息融合成统一的嵌入，要么尝试在公共语义空间中为不同模态构建嵌入，从而允许模型计算跨模态相似性。跨模态表示学习正引起越来越多的关注，可以作为不同研究领域之间的一个有前途的联系。
